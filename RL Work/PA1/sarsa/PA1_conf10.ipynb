{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon greedy\n",
    "def choose_action_epsilon(env,Q, state, hyper=0.1, rg=rg):\n",
    "    if not Q[int(state/(env.num_cols)), state%(env.num_cols)].any() or rg.rand() < hyper:\n",
    "        return rg.choice(Q.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(Q[int(state/(env.num_cols)), state%(env.num_cols)])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(env,Q, state,hyper=1, rg=rg):\n",
    "    return rg.choice(Q.shape[-1], p = softmax(Q[int(state/(env.num_cols)), state%(env.num_cols)] / hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 10\n",
    "wind = True\n",
    "start_state = [0,4]\n",
    "p = 1.0\n",
    "chosenAction = choose_action_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = 0.4\n",
    "opt_gamma = 0.7\n",
    "tol_alpha = 0.2\n",
    "tol_gamma = 0.2\n",
    "opt_epsilon = 1.\n",
    "tol_epsilon = 0.\n",
    "\n",
    "# hyper parameter set\n",
    "alphas = np.linspace(opt_alpha-tol_alpha,opt_alpha+tol_alpha,5)\n",
    "gammas = np.linspace(opt_gamma-tol_gamma,opt_gamma+tol_gamma,5)\n",
    "epsilons = np.linspace(opt_epsilon-tol_epsilon,opt_epsilon+tol_epsilon,1)\n",
    "episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SpXfJ6XXLtTe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BqE09JUiL1B8"
   },
   "outputs": [],
   "source": [
    "# # specify world parameters\n",
    "# num_cols = 10\n",
    "# num_rows = 10\n",
    "# obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "#                          [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "#                          [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "#                          [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "# bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "# restart_states = np.array([[3,7],[8,2]])\n",
    "# start_state = np.array([[3,6]])\n",
    "# goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# # create model\n",
    "# gw = GridWorld(num_rows=num_rows,\n",
    "#                num_cols=num_cols,\n",
    "#                start_state=start_state,\n",
    "#                goal_states=goal_states, wind = False)\n",
    "# gw.add_obstructions(obstructed_states=obstructions,\n",
    "#                     bad_states=bad_states,\n",
    "#                     restart_states=restart_states)\n",
    "# gw.add_rewards(step_reward=-1,\n",
    "#                goal_reward=10,\n",
    "#                bad_state_reward=-6,\n",
    "#                restart_state_reward=-100)\n",
    "# gw.add_transition_probability(p_good_transition=0.7,\n",
    "#                               bias=0.5)\n",
    "# env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UdRce8oMZNb",
    "outputId": "ee3858e1-e109-42e6-80eb-336702f708e3"
   },
   "outputs": [],
   "source": [
    "# print(\"Number of actions\", env.num_actions) #0 -> UP, 1-> DOWN, 2 -> LEFT, 3-> RIGHT\n",
    "# print(\"Number of states\", env.num_states)\n",
    "# print(\"start state\", env.start_state_seq)\n",
    "# print(\"goal state(s)\", env.goal_states_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 1\n",
    "DOWN = 0\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "def plot_Q(Q, message = \"Q plot\", save_file=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(message)\n",
    "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
    "    plt.colorbar()\n",
    "    def x_direct(a):\n",
    "        if a in [UP, DOWN]:\n",
    "            return 0\n",
    "        return 1 if a == RIGHT else -1\n",
    "    def y_direct(a):\n",
    "        if a in [RIGHT, LEFT]:\n",
    "            return 0\n",
    "        return 1 if a == UP else -1\n",
    "    policy = Q.argmax(-1)\n",
    "    policyx = np.vectorize(x_direct)(policy)\n",
    "    policyy = np.vectorize(y_direct)(policy)\n",
    "    idx = np.indices(policy.shape)\n",
    "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
    "    if(save_file != None):\n",
    "        plt.savefig(save_file)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    # return fig\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sarsa(env,Q,gamma,alpha,epsilon,choose_action,plot_heat = False,max_timesteps=100) :\n",
    "  episode_rewards = []\n",
    "  state_visit_count = np.zeros((env.num_rows, env.num_cols))\n",
    "  steps_to_completion = []\n",
    "  steps = 0\n",
    "  for steps in tqdm(range(episodes)):\n",
    "    timesteps=0\n",
    "    env.reset()\n",
    "    current_state = env.start_state_seq[0]\n",
    "    current_action = choose_action(env,Q,current_state,hyper=epsilon)\n",
    "    rewards = []\n",
    "    tot_reward = 0 \n",
    "    while timesteps<max_timesteps :\n",
    "      timesteps+=1\n",
    "      next_state,reward = env.step(current_state,current_action)\n",
    "      next_action = choose_action(env,Q,next_state, hyper=epsilon)\n",
    "      # best next action\n",
    "#       best_next_action = np.argmax(Q[next_state//env.num_rows,next_state%env.num_cols])\n",
    "      Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action] += alpha*(reward[0] + gamma*Q[int(next_state/(env.num_cols)), next_state%(env.num_cols), next_action] - Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action])\n",
    "      rewards.append(reward[0])\n",
    "      tot_reward = tot_reward + reward[0]\n",
    "      # print(reward)\n",
    "      if reward == env.r_goal :\n",
    "        break\n",
    "      # print(current_state)\n",
    "      current_state = next_state\n",
    "      current_action = next_action\n",
    "      state_visit_count[int(current_state/(env.num_cols)), current_state%(env.num_cols)]+=1\n",
    "    episode_rewards.append(tot_reward)\n",
    "    steps_to_completion.append(timesteps)\n",
    "\n",
    "    if (steps+1)%10 == 0 and plot_heat:\n",
    "      clear_output(wait=True)\n",
    "      plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(steps+1, np.mean(episode_rewards[steps-10+1:steps]),\n",
    "                                                                           np.mean(steps_to_completion[steps-10+1:steps]),\n",
    "                                                                           Q.max(), Q.min()))\n",
    "  return Q, episode_rewards, steps_to_completion,state_visit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(env,Q, max_timesteps=100) :\n",
    "    episode_rewards = []\n",
    "    steps_to_completion = []\n",
    "    steps = 0\n",
    "    episodes = 100\n",
    "    for steps in tqdm(range(episodes)):\n",
    "        timesteps=0\n",
    "        env.reset()\n",
    "        current_state = env.start_state_seq[0]\n",
    "        current_action = np.argmax(Q[int(current_state/(env.num_cols)), current_state%(env.num_cols)])\n",
    "        rewards = []\n",
    "        tot_reward = 0 \n",
    "        while timesteps<max_timesteps :\n",
    "            timesteps+=1\n",
    "            next_state,reward = env.step(current_state,current_action)\n",
    "            next_action = np.argmax(Q[int(next_state/(env.num_cols)), next_state%(env.num_cols)])\n",
    "\n",
    "            rewards.append(reward[0])\n",
    "            tot_reward = tot_reward + reward[0]\n",
    "            # print(reward)\n",
    "            if reward == env.r_goal :\n",
    "                break\n",
    "            # print(current_state)\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "        episode_rewards.append(tot_reward)\n",
    "        steps_to_completion.append(timesteps)\n",
    "\n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([start_state])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = wind)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=p,\n",
    "                              bias=0.5)\n",
    "env = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [03:38<00:00, 91.73it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 224.22it/s]\n",
      "100%|██████████| 20000/20000 [04:33<00:00, 73.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 398.41it/s]\n",
      "100%|██████████| 20000/20000 [04:26<00:00, 74.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 847.43it/s]\n",
      "100%|██████████| 20000/20000 [04:29<00:00, 74.22it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 346.02it/s]\n",
      "100%|██████████| 20000/20000 [04:01<00:00, 82.93it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 2499.99it/s]\n",
      "100%|██████████| 20000/20000 [04:33<00:00, 73.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 294.99it/s]\n",
      "100%|██████████| 20000/20000 [04:31<00:00, 73.77it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 223.22it/s]\n",
      "100%|██████████| 20000/20000 [04:33<00:00, 73.22it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1219.47it/s]\n",
      "100%|██████████| 20000/20000 [04:31<00:00, 73.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2325.63it/s]\n",
      "100%|██████████| 20000/20000 [04:03<00:00, 82.10it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.60it/s]\n",
      "100%|██████████| 20000/20000 [04:31<00:00, 73.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 357.14it/s]\n",
      "100%|██████████| 20000/20000 [04:54<00:00, 67.80it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 350.88it/s]\n",
      "100%|██████████| 20000/20000 [04:11<00:00, 79.58it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 2857.17it/s]\n",
      "100%|██████████| 20000/20000 [04:26<00:00, 75.01it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1219.53it/s]\n",
      "100%|██████████| 20000/20000 [03:19<00:00, 100.40it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.64it/s]\n",
      "100%|██████████| 20000/20000 [03:40<00:00, 90.90it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 555.56it/s]\n",
      "100%|██████████| 20000/20000 [03:55<00:00, 84.79it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 271.00it/s]\n",
      "100%|██████████| 20000/20000 [03:46<00:00, 88.23it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 476.19it/s]\n",
      "100%|██████████| 20000/20000 [03:36<00:00, 92.43it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 2857.06it/s]\n",
      "100%|██████████| 20000/20000 [03:08<00:00, 106.26it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1754.43it/s]\n",
      "100%|██████████| 20000/20000 [04:02<00:00, 82.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 287.36it/s]\n",
      "100%|██████████| 20000/20000 [03:56<00:00, 84.42it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 485.44it/s]\n",
      "100%|██████████| 20000/20000 [03:34<00:00, 93.21it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 476.19it/s]\n",
      "100%|██████████| 20000/20000 [03:36<00:00, 92.34it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 568.18it/s]\n",
      "100%|██████████| 20000/20000 [02:41<00:00, 124.02it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 571.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004 0.7999999999999999 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tune hyperparameters\n",
    "from math import inf\n",
    "seed = 42\n",
    "max_valuation = -inf\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "            rg = np.random.RandomState(seed)\n",
    "            if(conf % 2 == 0):\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)\n",
    "            else:\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "            valuation = eval_Q(env,Q)\n",
    "            if(valuation >= max_valuation):\n",
    "                opt_alpha = alpha\n",
    "                opt_gamma = gamma\n",
    "                opt_epsilon = epsilon\n",
    "                max_valuation = valuation\n",
    "            \n",
    "print(opt_alpha,opt_gamma,opt_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:52<00:00, 116.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [03:22<00:00, 98.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [03:19<00:00, 100.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence of rewards approx.  -100.0\n",
      "Convergence of time steps approx.  100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_avgs, reward_avgs, steps_avgs = [], [], []\n",
    "num_expts = 3\n",
    "\n",
    "alpha = opt_alpha = 0.3\n",
    "gamma = opt_gamma = 0.8\n",
    "# epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "    Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "    rg = np.random.RandomState(i)\n",
    "    Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,1,choose_action = choose_action_softmax)\n",
    "    Q_avgs.append(Q.copy())\n",
    "    reward_avgs.append(rewards)\n",
    "    steps_avgs.append(steps)\n",
    "    \n",
    "conv_rewards = np.mean(np.average(reward_avgs,axis=0)[episodes-10:episodes])\n",
    "conv_steps = np.mean(np.average(steps_avgs,axis=0)[episodes-10:episodes])\n",
    "print(\"Convergence of rewards approx. \", conv_rewards)\n",
    "print(\"Convergence of time steps approx. \", conv_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    os.mkdir('conf'+str(conf))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps to Goal')\n",
    "plt.plot(np.arange(episodes),np.average(steps_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/steps-vs-episodes.jpeg')\n",
    "plt.close()\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(np.arange(episodes),np.average(reward_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/rewards-vs-episodes.jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:59<00:00, 111.39it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "rg = np.random.RandomState(i)\n",
    "Q, rewards, steps, state_visit_count = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 921.6x633.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plot = sns.heatmap(state_visit_count)\n",
    "plot.invert_yaxis()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('conf'+str(conf)+'/heatmap-state-visit-count.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Q(Q, save_file='conf'+str(conf)+'/heatmap-policy.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2702.72it/s]\n"
     ]
    }
   ],
   "source": [
    "valuation = eval_Q(env,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = 17\n",
    "\n",
    "with open('conf'+str(conf)+'/hyperparameter-tuning-results.txt', 'w') as f:\n",
    "    f.write('conf '+ str(conf) + '\\n')\n",
    "    f.write('wind = ' + str(wind) + '\\n')\n",
    "    f.write('start state = ' + str(start_state) + '\\n')\n",
    "    f.write('p = ' + str(p) + '\\n')\n",
    "    f.write('strategy = e_greedy\\n\\n')\n",
    "    f.write('opt_alpha = ' + str(opt_alpha) + '\\n')\n",
    "    f.write('opt_gamma = ' + str(opt_gamma)+'\\n')\n",
    "    f.write('opt_epsilon = ' + str(opt_epsilon)+'\\n')\n",
    "    f.write('\\nconvergence: '+'\\n')\n",
    "    f.write('rewards = ' + str(conv_rewards)+'\\n')\n",
    "    f.write('steps = ' + str(conv_steps)+'\\n')\n",
    "    f.write('\\nevaluation: ' + str(valuation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
