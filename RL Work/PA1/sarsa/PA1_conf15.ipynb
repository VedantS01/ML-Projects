{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon greedy\n",
    "def choose_action_epsilon(env,Q, state, hyper=0.1, rg=rg):\n",
    "    if not Q[int(state/(env.num_cols)), state%(env.num_cols)].any() or rg.rand() < hyper:\n",
    "        return rg.choice(Q.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(Q[int(state/(env.num_cols)), state%(env.num_cols)])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(env,Q, state,hyper=1, rg=rg):\n",
    "    return rg.choice(Q.shape[-1], p = softmax(Q[int(state/(env.num_cols)), state%(env.num_cols)] / hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 15\n",
    "# os.mkdir(\"conf\"+str(conf))\n",
    "wind = True\n",
    "start_state = [3,6]\n",
    "p = 0.7\n",
    "chosenAction = choose_action_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = 0.4\n",
    "opt_gamma = 0.8\n",
    "tol_alpha = 0.2\n",
    "tol_gamma = 0.1\n",
    "opt_epsilon = 0.\n",
    "tol_epsilon = 0.\n",
    "\n",
    "# hyper parameter set\n",
    "alphas = np.linspace(opt_alpha-tol_alpha,opt_alpha+tol_alpha,5)\n",
    "gammas = np.linspace(opt_gamma-tol_gamma,opt_gamma+tol_gamma,3)\n",
    "epsilons = [0,0.01,0.1]\n",
    "episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SpXfJ6XXLtTe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BqE09JUiL1B8"
   },
   "outputs": [],
   "source": [
    "# # specify world parameters\n",
    "# num_cols = 10\n",
    "# num_rows = 10\n",
    "# obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "#                          [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "#                          [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "#                          [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "# bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "# restart_states = np.array([[3,7],[8,2]])\n",
    "# start_state = np.array([[3,6]])\n",
    "# goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# # create model\n",
    "# gw = GridWorld(num_rows=num_rows,\n",
    "#                num_cols=num_cols,\n",
    "#                start_state=start_state,\n",
    "#                goal_states=goal_states, wind = False)\n",
    "# gw.add_obstructions(obstructed_states=obstructions,\n",
    "#                     bad_states=bad_states,\n",
    "#                     restart_states=restart_states)\n",
    "# gw.add_rewards(step_reward=-1,\n",
    "#                goal_reward=10,\n",
    "#                bad_state_reward=-6,\n",
    "#                restart_state_reward=-100)\n",
    "# gw.add_transition_probability(p_good_transition=0.7,\n",
    "#                               bias=0.5)\n",
    "# env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UdRce8oMZNb",
    "outputId": "ee3858e1-e109-42e6-80eb-336702f708e3"
   },
   "outputs": [],
   "source": [
    "# print(\"Number of actions\", env.num_actions) #0 -> UP, 1-> DOWN, 2 -> LEFT, 3-> RIGHT\n",
    "# print(\"Number of states\", env.num_states)\n",
    "# print(\"start state\", env.start_state_seq)\n",
    "# print(\"goal state(s)\", env.goal_states_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 1\n",
    "DOWN = 0\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "def plot_Q(Q, message = \"Q plot\", save_file=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(message)\n",
    "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
    "    plt.colorbar()\n",
    "    def x_direct(a):\n",
    "        if a in [UP, DOWN]:\n",
    "            return 0\n",
    "        return 1 if a == RIGHT else -1\n",
    "    def y_direct(a):\n",
    "        if a in [RIGHT, LEFT]:\n",
    "            return 0\n",
    "        return 1 if a == UP else -1\n",
    "    policy = Q.argmax(-1)\n",
    "    policyx = np.vectorize(x_direct)(policy)\n",
    "    policyy = np.vectorize(y_direct)(policy)\n",
    "    idx = np.indices(policy.shape)\n",
    "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
    "    if(save_file != None):\n",
    "        plt.savefig(save_file)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    # return fig\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sarsa(env,Q,gamma,alpha,epsilon,choose_action,plot_heat = False,max_timesteps=100) :\n",
    "  episode_rewards = []\n",
    "  state_visit_count = np.zeros((env.num_rows, env.num_cols))\n",
    "  steps_to_completion = []\n",
    "  steps = 0\n",
    "  for steps in tqdm(range(episodes)):\n",
    "    timesteps=0\n",
    "    env.reset()\n",
    "    current_state = env.start_state_seq[0]\n",
    "    current_action = choose_action(env,Q,current_state,hyper=epsilon)\n",
    "    rewards = []\n",
    "    tot_reward = 0 \n",
    "    while timesteps<max_timesteps :\n",
    "      timesteps+=1\n",
    "      next_state,reward = env.step(current_state,current_action)\n",
    "      next_action = choose_action(env,Q,next_state, hyper=epsilon)\n",
    "      # best next action\n",
    "#       best_next_action = np.argmax(Q[next_state//env.num_rows,next_state%env.num_cols])\n",
    "      Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action] += alpha*(reward[0] + gamma*Q[int(next_state/(env.num_cols)), next_state%(env.num_cols), next_action] - Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action])\n",
    "      rewards.append(reward[0])\n",
    "      tot_reward = tot_reward + reward[0]\n",
    "      # print(reward)\n",
    "      if reward == env.r_goal :\n",
    "        break\n",
    "      # print(current_state)\n",
    "      current_state = next_state\n",
    "      current_action = next_action\n",
    "      state_visit_count[int(current_state/(env.num_cols)), current_state%(env.num_cols)]+=1\n",
    "    episode_rewards.append(tot_reward)\n",
    "    steps_to_completion.append(timesteps)\n",
    "\n",
    "    if (steps+1)%10 == 0 and plot_heat:\n",
    "      clear_output(wait=True)\n",
    "      plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(steps+1, np.mean(episode_rewards[steps-10+1:steps]),\n",
    "                                                                           np.mean(steps_to_completion[steps-10+1:steps]),\n",
    "                                                                           Q.max(), Q.min()))\n",
    "  return Q, episode_rewards, steps_to_completion,state_visit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(env,Q, max_timesteps=100) :\n",
    "    episode_rewards = []\n",
    "    steps_to_completion = []\n",
    "    steps = 0\n",
    "    episodes = 100\n",
    "    for steps in tqdm(range(episodes)):\n",
    "        timesteps=0\n",
    "        env.reset()\n",
    "        current_state = env.start_state_seq[0]\n",
    "        current_action = np.argmax(Q[int(current_state/(env.num_cols)), current_state%(env.num_cols)])\n",
    "        rewards = []\n",
    "        tot_reward = 0 \n",
    "        while timesteps<max_timesteps :\n",
    "            timesteps+=1\n",
    "            next_state,reward = env.step(current_state,current_action)\n",
    "            next_action = np.argmax(Q[int(next_state/(env.num_cols)), next_state%(env.num_cols)])\n",
    "\n",
    "            rewards.append(reward[0])\n",
    "            tot_reward = tot_reward + reward[0]\n",
    "            # print(reward)\n",
    "            if reward == env.r_goal :\n",
    "                break\n",
    "            # print(current_state)\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "        episode_rewards.append(tot_reward)\n",
    "        steps_to_completion.append(timesteps)\n",
    "\n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([start_state])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = wind)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=p,\n",
    "                              bias=0.5)\n",
    "env = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:44<00:00, 448.77it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 666.71it/s]\n",
      "100%|██████████| 20000/20000 [01:15<00:00, 264.85it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 418.41it/s]\n",
      "100%|██████████| 20000/20000 [01:31<00:00, 219.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 396.82it/s]\n",
      "100%|██████████| 20000/20000 [00:32<00:00, 619.00it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 277.78it/s]\n",
      "100%|██████████| 20000/20000 [00:51<00:00, 388.49it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1538.47it/s]\n",
      "100%|██████████| 20000/20000 [01:03<00:00, 313.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1470.67it/s]\n",
      "100%|██████████| 20000/20000 [00:29<00:00, 689.08it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1587.31it/s]\n",
      "100%|██████████| 20000/20000 [00:29<00:00, 684.37it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 917.42it/s]\n",
      "100%|██████████| 20000/20000 [00:39<00:00, 509.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1087.00it/s]\n",
      "100%|██████████| 20000/20000 [01:29<00:00, 224.50it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 276.25it/s]\n",
      "100%|██████████| 20000/20000 [01:17<00:00, 257.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 272.48it/s]\n",
      "100%|██████████| 20000/20000 [01:16<00:00, 260.17it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 490.20it/s]\n",
      "100%|██████████| 20000/20000 [00:44<00:00, 452.51it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1818.12it/s]\n",
      "100%|██████████| 20000/20000 [00:47<00:00, 420.49it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 392.16it/s]\n",
      "100%|██████████| 20000/20000 [01:03<00:00, 317.21it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 485.44it/s]\n",
      "100%|██████████| 20000/20000 [00:27<00:00, 718.67it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1020.42it/s]\n",
      "100%|██████████| 20000/20000 [00:25<00:00, 790.30it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 854.69it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 568.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1408.41it/s]\n",
      "100%|██████████| 20000/20000 [00:58<00:00, 342.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 321.54it/s]\n",
      "100%|██████████| 20000/20000 [00:54<00:00, 366.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 485.44it/s]\n",
      "100%|██████████| 20000/20000 [01:07<00:00, 295.39it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 292.40it/s]\n",
      "100%|██████████| 20000/20000 [00:50<00:00, 396.27it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 581.39it/s]\n",
      "100%|██████████| 20000/20000 [01:02<00:00, 318.64it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 312.50it/s]\n",
      "100%|██████████| 20000/20000 [00:55<00:00, 363.29it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 492.61it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 953.28it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 925.92it/s]\n",
      "100%|██████████| 20000/20000 [00:38<00:00, 524.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 833.31it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 472.22it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 847.45it/s]\n",
      "100%|██████████| 20000/20000 [00:56<00:00, 355.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 558.66it/s]\n",
      "100%|██████████| 20000/20000 [01:06<00:00, 301.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 301.21it/s]\n",
      "100%|██████████| 20000/20000 [01:07<00:00, 295.84it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 448.43it/s]\n",
      "100%|██████████| 20000/20000 [01:09<00:00, 286.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 327.87it/s]\n",
      "100%|██████████| 20000/20000 [00:47<00:00, 423.14it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 313.48it/s]\n",
      "100%|██████████| 20000/20000 [00:55<00:00, 358.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 602.42it/s]\n",
      "100%|██████████| 20000/20000 [01:02<00:00, 321.42it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 392.15it/s]\n",
      "100%|██████████| 20000/20000 [00:32<00:00, 610.41it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1063.86it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 472.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2380.73it/s]\n",
      "100%|██████████| 20000/20000 [01:13<00:00, 272.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 310.56it/s]\n",
      "100%|██████████| 20000/20000 [01:05<00:00, 307.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 271.74it/s]\n",
      "100%|██████████| 20000/20000 [01:04<00:00, 309.33it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 476.20it/s]\n",
      "100%|██████████| 20000/20000 [01:06<00:00, 298.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 349.65it/s]\n",
      "100%|██████████| 20000/20000 [00:56<00:00, 353.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 289.85it/s]\n",
      "100%|██████████| 20000/20000 [00:57<00:00, 348.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 341.30it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 556.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 787.41it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 462.76it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 900.92it/s]\n",
      "100%|██████████| 20000/20000 [00:48<00:00, 415.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 452.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.9 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tune hyperparameters\n",
    "from math import inf\n",
    "seed = 42\n",
    "max_valuation = -inf\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "            rg = np.random.RandomState(seed)\n",
    "            if(conf % 2 == 0):\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)\n",
    "            else:\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "            valuation = eval_Q(env,Q)\n",
    "            if(valuation >= max_valuation):\n",
    "                opt_alpha = alpha\n",
    "                opt_gamma = gamma\n",
    "                opt_epsilon = epsilon\n",
    "                max_valuation = valuation\n",
    "            \n",
    "print(opt_alpha,opt_gamma,opt_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:15<00:00, 1271.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:21<00:00, 914.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:15<00:00, 1251.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence of rewards approx.  -37.96666666666666\n",
      "Convergence of time steps approx.  26.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_avgs, reward_avgs, steps_avgs = [], [], []\n",
    "num_expts = 3\n",
    "\n",
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "    Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "    rg = np.random.RandomState(i)\n",
    "    Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "    Q_avgs.append(Q.copy())\n",
    "    reward_avgs.append(rewards)\n",
    "    steps_avgs.append(steps)\n",
    "    \n",
    "conv_rewards = np.mean(np.average(reward_avgs,axis=0)[episodes-10:episodes])\n",
    "conv_steps = np.mean(np.average(steps_avgs,axis=0)[episodes-10:episodes])\n",
    "print(\"Convergence of rewards approx. \", conv_rewards)\n",
    "print(\"Convergence of time steps approx. \", conv_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    os.mkdir('conf'+str(conf))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps to Goal')\n",
    "plt.plot(np.arange(episodes),np.average(steps_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/steps-vs-episodes.jpeg')\n",
    "plt.close()\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(np.arange(episodes),np.average(reward_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/rewards-vs-episodes.jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:25<00:00, 795.20it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "rg = np.random.RandomState(i)\n",
    "Q, rewards, steps, state_visit_count = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAIKCAYAAABr6lqXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3/0lEQVR4nO3deZxldX3n/9cbmgYJEsBdFEVxCQQ3dKITf2ExEeSnkBlEYtwiExIZBXXUJO2eAJEornGIJBpnAi6IGAXHBVHExICJgCit4GBAFlHEbkC2hqY+88c5Fa7X6qqCrlvf6jqvp4/zuHXP/dxzP8fqbj71qc/5nlQVkiRJktrYrHUCkiRJ0pBZkEuSJEkNWZBLkiRJDVmQS5IkSQ1ZkEuSJEkNWZBLkiRJDa1oncBMVqzc0bUYJUnSsrX+9qvTOodRd1z37xOtvba47yOW1PkuNXbIJUmSpIaWZIdckiRJi2jqztYZDJoFuSRJ0tDVVOsMBs2RFUmSJKkhO+SSJElDN2WHvCU75JIkSVJDdsglSZIGrpwhb8oOuSRJktSQHXJJkqShc4a8KTvkkiRJ2iQk2T/J15LclOTGJN9Mss/I69sn+WCS65LcnOTMJLvPcJytkrwjyTVJbk1yTpLfmiFusySrklye5LYkFyY5aAO5HZbk4iTrklyS5GXzPS8LckmSpKGrqcluCyDJHwOfAc4D/gtwMHAKsHX/eoDTgf2AI4CDgC2As5I8ZOxwHwIOA94MPBu4BvhikieMxR0FvBV4P/As4FzglCT7j+V2GHACcGr/+acAxyc5fF7nVlXziVtUK1buuPSSkiRJWiDrb786rXMYdfuVF0609lr50Mdv1PkmeTjwPWBVVb1nAzEHAp8G9qmqs/p9vwpcBpxUVUf2+x4PfAs4tKo+3O9bAawGLqmqA/p99weuBI6tqreMfM6XgftV1eNG3vsj4PNV9ZKRuL8HDgAeVFV3zHZ+dsglSZKGburOyW4b71BgCvjALDEHAD+aLsYBquoGuq75gWNxdwAnj8StBz4O7Jtky373vsBK4KSxzzkJ2D3Jzv3zpwH3myHuROA+wNPnOjkLckmSJE1UkvNGt3twiKcDFwO/l+QHSdYnuTTJy0didgMumuG9q4GdkmwzEndZVd0yQ9xKYJeRuHXApTPEAew6EscMnz0et0GusiJJkjR0S38d8gf32zuA1wM/oJshf3+SFVX1XmAH4PIZ3rumf9weuKmPWztL3A4jj9fXL893zxTHDMccj9sgC3JJkiRNVFXtsZGH2Ay4N/AHVfWpft9X+tnyVUnet5HHb8qRFUmSpKGbmprstvF+1j9+aWz/GcADgAfRdai3n+G94x3sueLWjMRt16/eMlccMxxzPG6DLMglSZIGrmpqotsCWD3H61N9zG4zvLYrcEVV3TRyrJ2TbD1D3O3cNTO+GtgSeOQMcQDfHctt/LPH4zbIglySJElL3T/2j/uO7d8PuKqqfgycBuyYZM/pF5NsCzynf23a6XTrkx88ErcCOAQ4o6rW9bu/QLcaywvGPvOFwEVVdVn//Bzgug3ErQG+PtfJOUMuSZI0dAszVjJJnwPOAk5Icl/g3+kK6mcCL+1jTqMrjk9K8jq6UZJVQIC3Tx+oqi5IcjLwniRb0K1TfjiwMyNFdVVdm+RddDPqPwfOpyva96FbOnE67o4kb6K7EdDVwJl9zKHAEVV1+1wnZ0EuSZKkJa2qKsnvAm8D/pxuXvti4AVV9dE+ZirJs4HjgOOBregK9L2r6sqxQ74UOAY4GtgOuBDYr6rOH4t7A93KLK8EHghcAjyvqj47lt8HkhTwGuB1wBXAK6rq+Pmc35x36ux/cvhvdLco/XW6AfUpuluM/jPwN1X1jfl82Hx5p05JkrScLbU7da77/j9PtPba8tFPX1Lnu9TMOkPe3zL0PLqfMp4EFN2C6VvQDbD/J+BfkvzlxiaykYvFS5IkSZukuUZWjgO2BZ5SVecBJHkY8A/ATVW1a5L9gE8nubiq/mGy6UqSJGnBLczt7XUPzTqykuRnwJFV9ZGx/Y+luz3oA6vquiRH083dPHkhknJkRZIkLWdLbmTl4rMnO7Ly2D2X1PkuNXMte3gv7lqIfdTP+vc+oH/+T8CvLWBekiRJWiw1NdlNs5qrID8PODzJeNyRwK10S85MW4ckSZKku2WuGfI3A18ELk7yJbq7Fz2V7mLOo6vq1j7uScx9ByVJkiQtRUt/HfJlbdaCvKrOSvIM4C3Ai4E76dZffNH0mo+9zwOfmViWkiRJ0jI1542BquqfgN+eI+ZbC5WQJEmSFplz3k15p05JkqShc2Slqbku6pQkSZI0QXbIJUmSBq7KGwO1ZIdckiRJasgOuSRJ0tB5UWdTdsglSZKkhuyQS5IkDZ2rrDRlh1ySJElqyA65JEnS0DlD3pQdckmSJKkhO+SSJElDN+U65C3ZIZckSZIaskMuSZI0dM6QN2WHXJIkSWrIDrkkSdLQuQ55UxbkkiRJQ+fISlOOrEiSJEkN2SGXJEkaOkdWmrJDLkmSJDVkh1ySJGno7JA3ZYdckiRJasgOuSRJ0sBV3dk6hUGzQy5JkiQ1ZIdckiRp6Jwhb8oOuSRJktSQHXJJkqSh806dTdkhlyRJkhqyQy5JkjR0zpA3ZYdckiRJasgOuSRJ0tA5Q96UBbkkSdLQObLSlCMrkiRJUkN2yCVJkobOkZWm7JBLkiRJDdkhlyRJGjpnyJuyQy5JkiQ1ZIdckiRp6OyQN2WHXJIkSWpoozrkSe4PrKmq9QuUjyRJkhabq6w0NWeHPMkfJ/l6knOSHNzve36Sa4FrgBuSvCNJNiaRJOdNbxtzHEmSJGlTMmuHPMlLgb8BzgWuB05Ksg1wAvAJ4F+BpwL/A7i03y9JkqRNiTPkTc01svIK4ISqOhwgyWF0BfrxVfWqPuZ9SdYAf8xGFORVtcd/JLVyx7qnx5EkSZI2JXONrDwK+OTI808AK4HPjMV9BnjkAuYlSZKkxVJTk900q7k65LcCW488n/56q7G4ewG3LVRSkiRJWkSOrDQ1V4f8W8Crktyrv2jz9cDVwBFJNgdIsgL478DqSSYqSZIkLUdzdcj/AvgSsBa4o9+3N3AqcHGSC4EnADsD+08oR0mSJE2SYyVNzdohr6qvA78BvAv4APDUqvom8Ay67vluwJXA86rqi5NNVZIkSVp+5rwxUFV9B/jO2L5LgYMnlZQkSZIWkTPkTc15YyBJkiRJkzNnh1ySJEnLnB3ypuyQS5IkSQ3ZIZckSRq68ibpLdkhlyRJkhqyQy5JkjR0zpA3ZYdckiRJasgOuSRJ0tDZIW/KDrkkSZKWtCR7JakZtuvH4rZP8sEk1yW5OcmZSXaf4XhbJXlHkmuS3JrknCS/NUPcZklWJbk8yW1JLkxy0AZyPCzJxUnWJbkkycvme352yCVJkoauNpkO+ZHAv408Xz/9RZIApwMPB44A1gKrgLOSPKGqrhp534eA/x94HfDvwMuBLyZ5WlV9ayTuKOC1wBuA84DfA05J8uyq+tzIZx8GnAC8DTgTeAZwfJJU1d/MdVKpJbjMzYqVOy69pCRJkhbI+tuvTuscRt36D6smWnvd68Vv26jzTbIXcBbwO1V15gZiDgQ+DexTVWf1+34VuAw4qaqO7Pc9HvgWcGhVfbjftwJYDVxSVQf0++4PXAkcW1VvGfmcLwP3q6rHjbz3R8Dnq+olI3F/DxwAPKiq7pjt/BxZkSRJ0nJwAPCj6WIcoKpuoOuaHzgWdwdw8kjceuDjwL5Jtux37wusBE4a+5yTgN2T7Nw/fxpwvxniTgTuAzx9rsQtyCVJkoauaqJbkvNGt43I9CNJ7kzysyQfTbLTyGu7ARfN8J7VwE5JthmJu6yqbpkhbiWwy0jcOuDSGeIAdh2JY4bPHo/bIGfIJUmStNTdALwTOBu4EXgi8HrgnCRPrKprgR2Ay2d475r+cXvgpj5u7SxxO4w8Xl+/PN89UxwzHHM8boMsyCVJkoZuwsseVtUeG/n+C4ALRnadneRrwL/SXej5xo05fmuOrEiSJGmTU1XnA98HntLvWkvXBR833sGeK27NSNx2/eotc8UxwzHH4zbIglySJGnopqYmu03W9EjJau6a5x61K3BFVd00Erdzkq1niLudu2bGVwNbAo+cIQ7guyNxzPDZ43EbZEEuSZKkTU6SJwOPoRtbATgN2DHJniMx2wLP6V+bdjqwBXDwSNwK4BDgjKpa1+/+At1qLC8Y++gXAhdV1WX983OA6zYQtwb4+lzn4gy5JEnS0C3xGwMl+QjdeuLnA9fTXdS5CrgaeF8fdhpdcXxSktdx142BArx9+lhVdUGSk4H3JNmiP+7hwM6MFNVVdW2SdwGrkvy8/+xDgH3olk6cjrsjyZvobgR0Nd2NgfYBDgWOqKrb5zo/C3JJkiQtdRcBz6e7A+fWwI+BTwFvqarrAKpqKsmzgeOA44Gt6Ar0vavqyrHjvRQ4Bjga2A64ENivn0sf9Qa6lVleCTwQuAR4XlV9djSoqj6QpIDX0N398wrgFVV1/HxOzjt1SpIkLbKldqfOW/721ROtvbb+o3cvqfNdapwhlyRJkhpyZEWSJGnoJr8SimZhh1ySJElqyA65JEnS0C3xVVaWOwtySZKkoZtyPY2WHFmRJEmSGrJDLkmSNHRe1NmUHXJJkiSpITvkkiRJQ2eHvCk75JIkSVJDdsglSZKGrlxlpSU75JIkSVJDdsglSZKGzhnypuyQS5IkSQ3ZIZckSRo679TZlB1ySZIkqSE75JIkSUNXzpC3ZEEuSZI0dI6sNDVrQZ7kc8BngJOr6vpJJpLkvOmvN9/iwZP8KEmSJGnJmGuGfD/geOCaJCcn2T+Jc+eSJEnLSE1NTXTT7OYzsvIaYHfguf12bZKTgH+oqu8sVCJVtcd/JLVyR39vIkmSpEGYT7f7X6rqvwEPBF4MfBt4NfCtJOcnOTLJfSeZpCRJkiZoqia7aVbzHj+pqlur6iNVtS/wUGAVsBJ4D3B1kk9PJENJkiRpGbtH8+BVdU1Vvb2qfh14CnAC8J8XNDNJkiQtjpqa7KZZbfQFmlV1XlUdCbg0iiRJknQ3zXVR59nAjfM5UFWt3/h0JEmStOic825q1oK8qvZerEQkSZKkIfJOnZIkSUPnWuFNeZMfSZIkqSE75JIkSUPnDHlTdsglSZKkhuyQS5IkDZ1rhTdlQS5JkjR0jqw05ciKJEmS1JAdckmSpIErlz1syg65JEmS1JAdckmSpKFzhrwpO+SSJElSQ3bIJUmShs4OeVN2yCVJkqSG7JBLkiQNnTcGasoOuSRJktSQHXJJkqShc4a8qSVZkG+WtE5h4jaLv5xYDtZP3dk6BWnetlyxResUJm7lZkvyP2sLasXmm7dOYeLW3npT6xSkRbX8/+WSJEnSrMoOeVMW5JIkSUNnQd6UcxOSJElSQ3bIJUmShm7KZQ9bskMuSZIkNWSHXJIkaeicIW/KDrkkSZLUkB1ySZKkobND3pQdckmSJKkhO+SSJEkDV2WHvCU75JIkSVJDdsglSZKGzhnypuyQS5IkSQ3ZIZckSRo6O+RN2SGXJEmSGrJDLkmSNHBlh7wpC3JJkqShsyBvypEVSZIkqSE75JIkSUM31TqBYbNDLkmSJDVkQS5JkjRwNVUT3RZaki8kqSRHj+3fPskHk1yX5OYkZybZfYb3b5XkHUmuSXJrknOS/NYMcZslWZXk8iS3JbkwyUEbyOmwJBcnWZfkkiQvm+/53OOCPMkDk9z/nr5fkiRJuruSPB94/Az7A5wO7AccARwEbAGcleQhY+EfAg4D3gw8G7gG+GKSJ4zFHQW8FXg/8CzgXOCUJPuPffZhwAnAqf3nnwIcn+TweZ1T1YZ/akmyF7B1VX1uZN8RwCrgAf2uq4A3VtWJ8/nAWT7rvOmvt1i545M25libgs3iLyeWg/VTd7ZOQZq3LVds0TqFiVu52fK/NGrF5pu3TmHi1t56U+sUJm797VendQ6jrn/+3hNdZmW7j521IOebZHvge8CrgY8Cx1TVG/vXDgQ+DexTVWf1+34VuAw4qaqO7Pc9HvgWcGhVfbjftwJYDVxSVQf0++4PXAkcW1VvGcnhy8D9qupxI+/9EfD5qnrJSNzfAwcAD6qqO2Y7r7mqwrcDu40c+L8D7+1P4jX9djHwv5IcMsexJEmSpI3xV8BFVfWxGV47APjRdDEOUFU30HXNDxyLuwM4eSRuPfBxYN8kW/a79wVWAieNfc5JwO5Jdu6fPw243wxxJwL3AZ4+10nN1Up4DF3xPe3VwN9U1ctH9r0nyd/Rdc1P5h6qqj2mv1655UNcDFOSJGmxbAKrrCR5OvBiZhhX6e0GXDTD/tXAi5NsU1U39XGXVdUtM8StBHbpv94NWAdcOkMcwK503ffp5vX4Z4/GncUs5irIN+MXv0UPp5uJGfcJ4EVzHEuSJEkDNDqaDL/YiJ3n+1fSzWgfV1WXbCBsB+DyGfav6R+3B27q49bOErfDyOP19cvz3TPFMcMxx+M2aK6RlfPpBtin/RB4xAxxj5ghCUmSJG0CNoFVVv4EuBdwzEIcbKmZq0P+V8Cnk/yQ7qeSo4C3J/kZcGYfsy9wNN3cjSRJkvQL7m5HfFSSnYA3AH8IbDky403/fDvg53TN4e1nOMR4B3st8LBZ4taMxG2XJGNd8pni6D/7mlniNmjWDnm/usoRdIX5z4BXAFsBnwJu7LdTgG/TzZBLkiRpUzM14W3jPIKu/jyJrvid3gBe23+9O3fNfY/bFbiinx+nj9s5ydYzxN3OXTPjq4EtgUfOEAfw3ZE4Zvjs8bgNmnPtvao6Afg14N10BfiP+kS/Afw98JyqesbISUqSJEkL5VvA3jNs0BXpe9PVpqcBOybZc/qNSbYFntO/Nu10uvXJDx6JWwEcApxRVev63V+gW43lBWP5vJBupZfL+ufnANdtIG4N8PW5TnBeC7ZW1Q/pFk6XJEnSMjOJu2kulKq6Hvjq+P7uPkD8sKq+2j8/ja44PinJ6+g656uA0C3lPX28C5KcTLdS4BZ0K6UcDuzMSFFdVdcmeRewKsnP6a6tPATYh27pxOm4O5K8ie5GQFfTjXXvAxwKHFFVt891jsv/DgqSJEma3Saw7OFcqmoqybOB44Dj6cZczgH2rqorx8JfSneB6NHAdsCFwH5Vdf5Y3BvoVmZ5JfBA4BLgeVX12bHP/kCSortHz+uAK4BXVNXx88l91jt1tjKEdci9U+fy4J06tSnxTp3Lg3fqXB6W2p061xy450Rrrx0+c/aSOt+lZvn/yyVJkqRZ1TLokG/KbNNKkiRJDdkhlyRJGjo75E3ZIZckSZIaskMuSZI0cM6Qt2WHXJIkSWrIDrkkSdLQ2SFvyg65JEmS1JAdckmSpIFzhrwtO+SSJElSQ3bIJUmSBs4OeVsW5JIkSQNnQd6WIyuSJElSQ3bIJUmShq7SOoNBW5IF+a9u9SutU5i4J237iNYpTNx2m23ZOoWJO/Waf2udghbIZln+/zHaZoutWqcwcT+79eetU5i47Qbw30hpaJZkQS5JkqTF4wx5W86QS5IkSQ3ZIZckSRq4mlr+Y3tLmR1ySZIkqSE75JIkSQPnDHlbdsglSZKkhuyQS5IkDVy5DnlTdsglSZKkhuyQS5IkDZwz5G3ZIZckSZIaskMuSZI0cK5D3pYFuSRJ0sBVtc5g2BxZkSRJkhqyQy5JkjRwjqy0ZYdckiRJasgOuSRJ0sDZIW/LDrkkSZLUkB1ySZKkgXOVlbbskEuSJEkN2SGXJEkaOGfI27JDLkmSJDVkh1ySJGngquyQt3S3C/Ik9wWOBJ4CFPAN4K+ras3GJJLkvOmv77vtozfmUJIkSdImY9aCPMka4Ler6vz++UOBfwEeCHy/D3sm8AdJnlpVP5lkspIkSVp4NdU6g2Gba4Z8O36xaD8WWAn8p6rarap2A54KbAO8dWMSqao9preNOY4kSZLunqnKRDfN7u5e1LkvcExVXTC9o6q+SVeo77+QiUmSJElDcHdnyLcDLphh//l0YyySJEnaxHhRZ1vzKcifnGSb/uufAtvOELMdcMtCJSVJkiQNxXwK8r/uH6d/dNoT+D9jMU8CfrhQSUmSJGnxeGOgtuYqyPeeYd8NM+zbGfj4xqcjSZIkDcusBXlVnT2fg1TVCxcmHUmSJC22qtYZDNvdXWVFkiRJ0gK623fqlCRJ0vLiDHlbdsglSZKkhuyQS5IkDZx302zLDrkkSZLUkB1ySZKkgfNOnW3ZIZckSZIaskMuSZI0cK5D3pYFuSRJ0sB5UWdbjqxIkiRJDdkhlyRJGjgv6mzLDrkkSZLUkB1ySZKkgfOizrbskEuSJEkN2SGXJEkaOFdZacsOuSRJktTQkuyQ77XdY1unMHGPzq+0TmHidr19+f+8d2rrBLRgpgYwQHlnTbVOQQvgtvV3tE5h4rZcsUXrFAbHVVbaWv4VkyRJkrSEWZBLkiQN3FRlotvGSrJvkq8k+XGSdUmuSvKJJLuOxT00ySeT3JDkxiSfSrLTDMfbPskHk1yX5OYkZybZfYa4rZK8I8k1SW5Nck6S35ohbrMkq5JcnuS2JBcmOWi+52dBLkmSpKVuB+A84BXAM4FVwG7AuUkeBpBka+ArwGOBlwAvAh4FnJXcNSucJMDpwH7AEcBBwBZ93EPGPvdDwGHAm4FnA9cAX0zyhLG4o4C3Au8HngWcC5ySZP/5nNySnCGXJEnS4lnqV9FU1ceAj43uS/KvwMXAc4F30hXOjwAeU1WX9jHfBv4v8MfAu/q3HgD8JrBPVZ3Vx50DXAb8CXBkv+/xwO8Dh1bVh/t9ZwOrgb/oj0OS+wOvBY6tquP6zzgryS7AscDn5jo/O+SSJEnaFP2sf1zfPx4AnDtdjANU1WXA14EDR953APCj6WK8j7uBrms+HncHcPJI3Hrg48C+Sbbsd+8LrAROGsvvJGD3JDvPdSIW5JIkSQO31GfIpyXZPMnKJI8CTgB+zF2d892Ai2Z422pgdNZ8tridkmwzEndZVd0yQ9xKYJeRuHXApTPEMfbZM3JkRZIkaeAmvexhkvN+8fNqj3t4qG8A0++9lG7s5Nr++Q7A2hneswbYfuT5DsDlG4ijj71pjuNNH2f68fqqX1o/dzxug+yQS5IkaVPxIuCpdLPdNwJfSvLwphktADvkkiRJAzfp24ZtREd8/Djf67/8RpLP03W6/wx4GV03e/sZ3jbe6Z4tjpHYtcDDZolbMxK3XZKMdcnH4zbIDrkkSZI2OVV1Pd3YyvQs92q6ee5xuwLfHXk+W9wVVXXTSNzO/XKK43G3c9fM+GpgS+CRM8Qx9tkzsiCXJEkauCIT3SYhyQPo1hz/Qb/rNOCpSR4xEvNwuiUOTxt562nAjkn2HInbFnjOWNzpdOuTHzwStwI4BDijqtb1u79AtxrLC8ZSfCFwUb/Sy6wcWZEkSdKSluQfgfOBb9PNjj8aeDXdkofv7MP+ju7GQZ9J8ka65dWPAq6kW5Fl2mnAOcBJSV5HN3KyCgjw9umgqrogycnAe5JsQbdO+eHAzowU31V1bZJ3AauS/LzP8xBgH/q1yudiQS5JkjRwU0v9zkDdnS+fB7yGbsnBK4GvAm+rqssBqurmJPsA7wZOpCuwvwy8amQMhaqaSvJs4DjgeGArugJ976q6cuxzXwocAxwNbAdcCOxXVeePxb2BbmWWVwIPBC4BnldVn53PyeWXV2hp7+CHHbj0klpgj77rDq7L1q63L/+JqJdcd9bcQdISsd1Wy//fnetvu7l1ChO31YqVrVOYuFry943ceDffcvlk1xm8m776gIMn+n/6Xj85ZUmd71Jjh1ySJGngpiY05635Wf4tTEmSJGkJs0MuSZI0cJNaCUXzY4dckiRJasgOuSRJ0sBN+k6dmt09LsiTbAb8OnBpVd2ycClJkiRpMTmy0tbGjKzcG7gA2GOBcpEkSZIGZ9YOeZK/mOXlLekWXP/DJL8DVFW9ZSGTkyRJ0uQ5stLWXCMr07cd3dDvMQp40cjX97ggT3Le9NfP3WledxmVJEmSNnlzjaycAfwEeH5VbTa6ATvQFep79fs2n3SykiRJWnhTE940u1kL8qraD3gN8J4kX0yyy+jLC5lIVe0xvS3kcSVJkqSlbM6LOqvqY8CuwA+Bbyf58yRbTjwzSZIkLYoiE900u3mtslJVa6vqj4BnAgcBq4H9WeAuuSRJkjQ0d2sd8qr65yRPBP4U+NBkUpIkSdJimrKJ3dTdXoe8qu6oqqOBxwD7AN9a6KQkSZKkobjHd+qsqiuBKxcwF0mSJDUw5Zx3Uxtzp05JkiRJG+ked8glSZK0PLhKR1t2yCVJkqSG7JBLkiQNnHfTbMuCXJIkaeCm4kWdLTmyIkmSJDVkh1ySJGngvKizLTvkkiRJUkN2yCVJkgbOizrbskMuSZIkNWSHXJIkaeCmXGSlKTvkkiRJUkN2yCVJkgZuClvkLdkhlyRJkhqyQy5JkjRwrkPelh1ySZIkqSE75JIkSQPnKittLcmC/A9u27p1ChN34NqzW6cwcf/7vnu3TkELYCj/Rg/h17U7b/PA1ilM3C33Wtc6hYn7wQ3XtE5h4rbeYsvWKUiLakkW5JIkSVo83qmzLQtySZKkgRvCbwmXMi/qlCRJkhqyQy5JkjRwXtTZlh1ySZIkqSE75JIkSQPnRZ1t2SGXJEmSGrJDLkmSNHB2yNuyQy5JkiQ1ZIdckiRp4MpVVpqyQy5JkiQ1ZIdckiRp4Jwhb8sOuSRJktSQHXJJkqSBs0PelgW5JEnSwFXrBAbOkRVJkiSpITvkkiRJAzflsodN2SGXJEmSGrJDLkmSNHBe1NmWHXJJkiSpITvkkiRJA2eHvK05C/IkDwWeC6wHPlZV1yXZCfgzYBfgUuBdVXXpRDOVJEmSlqFZC/IkvwacA2zb7/rTJM8AzgS2oSvGXwQckuSJVXXFPU0kyXnTX3/2/r93Tw8jSZKku8l1yNuaa4b8rcBVwGOB+wPfAE4Dfgw8vKqeQtclv5auYy5JkiTpbpirIP/PwNuq6vtVdR1d0f0o4LiqugGgqn4CvAfYe2MSqao9preNOY4kSZLunqlMdtPs5irI7weMjqFc3j/++1jcJcBDFygnSZIkaTDmuqhzLV1RPu1O4DzgxrG4bYHbFzAvSZIkLRJXWWlrrg75d4HfmH5SVVNV9ZSqumQs7nHADxY6OUmSJGm5m6tD/lfADvM4zpOAT2x8OpIkSVpsrrLS1qwFeVWdMZ+DVNV/XZh0JEmStNimLMmbmmtkRZIkSWoqyXOTnJrkh0luTXJJkrclufdY3PZJPpjkuiQ3Jzkzye4zHG+rJO9Ick1/vHOS/NYMcZslWZXk8iS3JbkwyUEbyPGwJBcnWdfn97L5np8FuSRJ0sBNTXhbAK+lW1zk9cB+wN8AhwNfSrIZQJIAp/evHwEcBGwBnJXkIWPH+xBwGPBm4NnANcAXkzxhLO4ouvvyvB94FnAucEqS/UeDkhwGnACc2n/+KcDxSQ6fz8nNNUMuSZIktfacqvrpyPOzk6wB/jewF/AV4ADgN4F9quosgCTnAJcBfwIc2e97PPD7wKFV9eF+39nAauAv+uOQ5P50PwgcW1XH9Z97VpJdgGOBz/VxK4BjgBOr6g0jcQ8Gjkrywaq6Y7aTs0MuSZI0cDXhbaPz+8VifNq/9Y879o8HAD+aLsb7991A1zU/cOR9BwB3ACePxK0HPg7sm2TLfve+wErgpLHPPQnYPcnO/fOn0S0TPh53InAf4OlznZ8FuSRJkjZFe/aP3+sfdwMumiFuNbBTkm1G4i6rqltmiFsJ7DIStw64dIY4gF1H4pjhs8fjNsiRFUmSpIGb9I2Bkpw3+ryq9tjI4+1IN15yZlV9s9+9A3fdVX7Umv5xe+CmPm7tLHE7jDxeX1XjTf6Z4pjhmONxG2SHXJIkSZuMvtP9GWA98NLG6SwIO+SSJEkDN5XJHn9jO+LTktyLbib8EcCeVXXVyMtr6brg48Y72GuBh80St2YkbrskGeuSzxRH/9nXzBK3QXbIJUmStOQl2QL4JPBkYP+q+s5YyGrumucetStwRVXdNBK3c5KtZ4i7nbtmxlcDWwKPnCEO4Lsjcczw2eNxG2RBLkmSNHBT1ES3jdWvNf4RYB/gd6vq3BnCTgN2TLLnyPu2BZ7TvzbtdLr1yQ8eiVsBHAKcUVXr+t1foFuN5QVjn/NC4KKquqx/fg5w3Qbi1gBfn+v8HFmRJEnSUvc/6QroY4Cbkzx15LWr+tGV0+iK45OSvI5ulGQVEODt08FVdUGSk4H39F33y+huMrQzI0V1VV2b5F3AqiQ/B86nK9r3oV+rvI+7I8mb6G4EdDVwZh9zKHBEVd0+18lZkEuSJA3cQqwVPmHP6h/f0G+j/hx4a1VNJXk2cBxwPLAVXYG+d1VdOfael9IV90cD2wEXAvtV1fljcW+gW5nllcADgUuA51XVZ0eDquoDSQp4DfA64ArgFVV1/HxOzoJckiRp4Ca97OHGqqqHzzNuDV1n+tA54m4F/ke/zRZ3J13RfvQ8PvsE4IT55DnOGXJJkiSpITvkkiRJA7cQF17qnrNDLkmSJDVkh1ySJGng7I+3tSQL8hs3t3G/HLzkurNap6AFsGLzJfnPxIJ74NbbtU5h4v6kHto6hYl7/tqvtk5h4iZ8Q0VJDQzjv7SSJEnaoKW+yspyZytakiRJasgOuSRJ0sC5ykpbdsglSZKkhuyQS5IkDZz98bbskEuSJEkN2SGXJEkaOFdZacsOuSRJktSQHXJJkqSBK6fIm7IglyRJGjhHVtpyZEWSJElqyA65JEnSwHljoLbskEuSJEkN2SGXJEkaOPvjbdkhlyRJkhqyQy5JkjRwzpC3ZYdckiRJasgOuSRJ0sC5DnlbdsglSZKkhuyQS5IkDVw5Q96UHXJJkiSpoXkV5En2SvKCJE/awOs7JnnzwqYmSZKkxTA14U2zm7UgT7JNkn8BvgycCPxbki8kefBY6EOAt2xMIknOm9425jiSJEnSpmSuDvnrgV8D/gDYFXg58ETgG0l2nWxqkiRJWgw14f9pdnMV5P8VeEtVnVhVF1fVB4AnAT8BvpbkKQuVSFXtMb0t1DElSZI0N0dW2pqrIN8JuGB0R1VdDewJfAc4M8leE8lMkiRJGoC5lj28lm4+/BdU1c1JngWcCvwf4J0TyE2SJEmLYKocK2lprg75N4EDZ3qhqm7rX/s/wBsXOC9JkiRpEOYqyD8GPCzJfWZ6sarWA4cAJwBXLHBukiRJWgQ14U2zm3VkpapOpRtLmS2mgMMXMilJkiRpKOaaIZckSdIyN2Ufu6l53alTkiRJ0mTYIZckSRo4b97Tlh1ySZIkqSE75JIkSQPn3TTbskMuSZIkNWSHXJIkaeBcZaUtC3JJkqSB86LOthxZkSRJkhqyQy5JkjRwXtTZlh1ySZIkqSE75JIkSQNX5Qx5S3bIJUmSpIbskEuSJA2cyx62ZYdckiRJasgOuSRJ0sC5ykpbFuSSZvW8Bzy5dQqL4jfuvFfrFCZuvwPXtE5h8v62dQKSdPdZkEuSJA2cd+psyxlySZIkqSE75JIkSQPnKitt2SGXJEmSGrJDLkmSNHDeqbMtC3JJkqSBc9nDthxZkSRJkhqyQy5JkjRwLnvYlh1ySZIkqSE75JIkSQPnsodt2SGXJEmSGrJDLkmSNHAue9iWHXJJkiQtaUkekuSvk5yT5JYkleThM8RtleQdSa5Jcmsf/1szxG2WZFWSy5PcluTCJAdt4LMPS3JxknVJLknysg3E/W6SC/rj/TDJG5NsPp/zsyCXJEkauClqotsC2AV4HrAW+KdZ4j4EHAa8GXg2cA3wxSRPGIs7Cngr8H7gWcC5wClJ9h8NSnIYcAJwKrAfcApwfJLDx+L27WP+rT/ee4E3An85n5NzZEWSJElL3deq6gEASf4QeOZ4QJLHA78PHFpVH+73nQ2sBv4COKDfd3/gtcCxVXVc//azkuwCHAt8ro9bARwDnFhVbxiJezBwVJIPVtUd/f5jgX+uqj8aidsGeGOSd1fVj2c7OTvkkiRJA1cT/t9G51c1n5uJHgDcAZw88r71wMeBfZNs2e/eF1gJnDT2/pOA3ZPs3D9/GnC/GeJOBO4DPB0gyUOBJ2wgbgu6jvmsLMglSZK0HOwGXFZVt4ztX01XgO8yErcOuHSGOIBdR+IALroncVV1GXDLSNwGObIiSZI0cFMTXmUlyXmjz6tqjwl8zA50M+bj1oy8Pv14ff3y0jIzxTHDMecbN71vhxn2/wILckmSpIFz0cO2LMglSZI0URPqiI9bCzxshv3THeo1I3HbJclYl3ymOIDt6VZrmU/cuO1H4jbIGXJJkqSB2wSWPZyP1cDOSbYe278rcDt3zYyvBrYEHjlDHMB3R+LgrhnxuxXXr5O+9UjcBlmQS5IkaTk4nW5Vk4Ond/RLFx4CnFFV6/rdX6BbjeUFY+9/IXBRfzEmwDnAdRuIWwN8HaCqrgAu3EDcHcDn50p8wUZW+rsgvbWq9rmH7/+PYf+PPuj3FyotSZIkzWERu9j3WJLn9l9Oj788K8lPgZ9W1dlVdUGSk4H3JNkCuAw4HNiZkWK5qq5N8i5gVZKfA+fTFe370K9V3sfdkeRNdDcCuho4s485FDiiqm4fSe/1wGeTnAB8DHgi3Y2B3jvXGuSwsDPk9wP2XMDjSZIkSdNOGXt+fP94NrBX//VL6W7mczSwHV3ner+qOn/svW8AbgJeCTwQuAR4XlV9djSoqj6QpIDXAK8DrgBeUVXHj8V9rv+B4S3AHwA/obtL5zHzObE5C/IkO83nQHQF+T02Ouz/sQe/YOn/mCZJkrRM/PIKgEtPVWUeMbcC/6PfZou7k65oP3oexzwBOGEecZ8CPjVX3Ezm0yG/nPmthpN5xkmSJEnqzacgvxX4GvDJOeKeDPzRRmckSZKkRbUpzJAvZ/MpyC8E7qyqD80WlOR6LMglSZKku2U+Bfl5wHPnjOrMOdsjSZKkpaXskDc1n4L8WOYeV6GqTsV1zSVJkqS7Zc6CvKquBq5ehFwkSZLUwKawyspyZkdbkiRJamghbwwkSZKkTZCrrLRlQS5JkjRwjqy05ciKJEmS1JAdckmSpIFzZKUtO+SSJElSQ3bIJUmSBs4bA7Vlh1ySJElqyA65JEnSwE25ykpTdsglSZKkhuyQS5IkDZwz5G3ZIZckSZIaskMuSZI0cM6Qt2WHXJIkSWrIDrkkSdLAOUPelh1ySZIkqSE75I38/IyjWqcwcSt+fa/WKUxc3frz1ilM3p13tM5gUdSNP2udwuRt9SutM5i8vz2kdQZaADeuu6V1CoPjDHlbFuSSJEkD58hKW46sSJIkSQ3ZIZckSRo4R1baskMuSZIkNWSHXJIkaeCcIW/LDrkkSZLUkB1ySZKkgauaap3CoNkhlyRJkhqyQy5JkjRwU86QN2WHXJIkSWrIDrkkSdLAleuQN2WHXJIkSWrIDrkkSdLAOUPelgW5JEnSwDmy0pYjK5IkSVJDdsglSZIGbsoOeVN2yCVJkqSG7JBLkiQNXHlRZ1N2yCVJkqSG7JBLkiQNnKustGWHXJIkSWrIDrkkSdLAeWOgtubskCe5V5JXJTkryU+S3N5vP+n3vSrJ1hubSJLzpreNPZYkSZK0qZi1Q57kocBXgIcDXwc+CazpX94B2BV4O/DyJM+oqisml6okSZImwRnytuYaWXkPcCvwqKq6fKaAJA8HPg28GzjoniZSVXtMf/2xB7/APxWSJEkahLkK8t8GXrihYhygqi5P8mbgxIVMTJIkSYvDO3W2NdcM+d357vidlCRJku6muTrkZwLHJLmoqi6bKaAfWTkK+NIC5yZJkqRF4Ax5W3MV5K8CzgK+n+Rc4CJgbf/a9sBuwFOBy4FXTyZFSZIkTZLLHrY1a0FeVVcleRzwR8BzgN+lW10FusJ8NfA64O+q6pYJ5ilJkiQtS3PeGKiqbgXe22+SJElaZhxZaWvOGwNJkiRJmpw5O+SSJEla3lz2sC075JIkSVJDdsglSZIGrlxlpSk75JIkSVJDdsglSZIGzhnytuyQS5IkSQ3ZIZckSRo41yFvyw65JEmS1JAdckmSpIFzlZW2LMglSZIGzpGVthxZkSRJ0pKX5KFJPpnkhiQ3JvlUkp1a57UQ7JBLkiQN3FLvkCfZGvgKsA54CVDA0cBZSR5XVTe3zG9jWZBLkiRpqTsMeATwmKq6FCDJt4H/C/wx8K6GuW00R1YkSZIGria8LYADgHOni3GAqroM+Dpw4MJ8RDt2yCVJkjRRSc4bfV5Ve9zNQ+wGfGaG/auBg+9pXkvFkizIn/+jj2SxPmv6D8g9+IOxSRnCeXqOy4PnuDy0Osf1t1+9aJ81hO8jDOM8h3COc1l/+9UTrb3GC/J7YAdg7Qz71wDbb+Sxm1uSBbkkSZKWjyH/sDMfzpBLkiRpqVvLzJ3wDXXONykW5JIkSVrqVtPNkY/bFfjuIuey4LLU152UJEnSsCV5FXAc8Oiq+vd+38Pplj38s6p6Z7vsNp4FuSRJkpa0JL8CXAjcCryRbjXFo4B7A4+rqpsaprfRHFmRJEnSktbfiXMf4PvAicBHgMuAfTb1YhzskEuSJElN2SGXJEmSGrIglyRJkhqyIJckSZIasiCXJEmSGrIglyRJkhqyIJckSZIasiCXJEmSGhpsQZ7koUk+meSGJDcm+VSSnVrntZCSPCTJXyc5J8ktSaq/zeyykeS5SU5N8sMktya5JMnbkty7dW4LJcm+Sb6S5MdJ1iW5KsknkuzaOrdJSvKF/s/s0a1zWQhJ9urPZ3y7vnVuCy3J/km+luSm/t/XbybZp3VeCyXJVzfwvawkX2id30JJ8ptJzkhybZKfJzk/yaGt81pISfZO8s/9fz/WJDkxyQNa56XhWdE6gRaSbA18BVgHvITu9qtHA2cleVx/N6jlYBfgecB5wD8Bz2ybzkS8FrgCeD1wFfBE4K3A3kn+c1VNNcxtoexA9z08HvgpsBPwZ8C5SXavqh+2TG4SkjwfeHzrPCbkSODfRp6vb5XIJCT5Y+D9/XYUXePnCcDWDdNaaP8d2HZs39OAdwGnLX46Cy/J44AzgXOBw4BbgOcCH0qyZVX9Tcv8FkKS/w84A/gicBBwH7pa4MtJ9qiqdS3z07AM8k6dSV5J9w/nY6rq0n7fzsD/Bf6kqt7VMr+FkmSz6YI0yR8CfwfsXFWXN01sASW5X1X9dGzfi4H/DTyjqr7SJrPJSvIY4GLgtVX1ztb5LKQk2wPfA14NfBQ4pqre2DarjZdkL+As4Heq6sy22UxG/xu47wGrquo9bbNZXEk+BLwQeFBVrWmdz8ZK8pd0DY8dRm9LnuQcgKp6WqvcFkqSM4GHA4+tqvX9vifT/cD88qo6vmF6GpihjqwcAJw7XYwDVNVlwNeBA5tltcCWSXd4VuPFeG+6+7jjYuayyH7WPy6r7mrvr4CLqupjrRPR3XYoMAV8oHUii6n/revBwOnLoRjvrQTuAG4d238Dy6d2eCrwpeliHKCqvkn37+t/aZaVBmm5/KW6u3YDLpph/2pgWc/lDsSe/eP3mmaxwJJsnmRlkkcBJwA/BpZV0Zrk6cCLgZe3zmWCPpLkziQ/S/LRZXbtytPpfnPze0l+kGR9kkuTLOfvJ3TF273pfjO3XPyv/vF9SR6cZLskhwHPAN7dLq0FdSdw+wz71wG/vsi5aOAGOUNON5O7dob9a4DtFzkXLaAkOwJ/AZzZdzqWk28Ae/RfXwrsU1XXNsxnQSVZSfeDxnFVdUnrfCbgBuCdwNnAjXTXO7weOCfJE5fJ9/LB/fYOunP7AV3n+P1JVlTVe1smN0EvBq4FPt86kYVSVRf1Y1b/SDczD13H/GVV9fFWeS2wS+i65P8hycOAB9Gdq7RohlqQaxlKsg3wGboxjpc2TmcSXkR3Idkj6GY7v5Tk6cvomoA/Ae4FHNM6kUmoqguAC0Z2nZ3ka8C/0l3oucnPydP91vXewB9U1af6fV/pZ8tXJXlfLbMLl5I8GPht4L2jow+buv43cafS/eb4ZXSjKwcCH0hyW1V9pGV+C+S9wEn9Sk7vo2vW/S3d2NWyH/nU0jLUkZW1zNwJ31DnXEtcknsBp9MVq/tW1VWNU1pwVfW9qvpGP1v9DGAbutVWNnn92MYbgDcBW/a/Ht+uf3n6+ebNEpyQqjof+D7wlNa5LJDpaxu+NLb/DOABdJ3H5eaFdP8tXU7jKgB/SdclfnZVfbaqvlxVRwKfAN6bZJOvH/ofKo4GXgP8BPgucDXwOeCahqlpgDb5v1D30Gq6OfJxu9L9hdQmJMkWwCeBJwP7V9V3Gqc0cVV1Pd3Yyi6NU1kojwC2Ak6i+6F4eoPutwFrgd3bpLYolkvXePUcry/HruNLgAur6sLWiSyw3enOa3x041/plge8/+KntPCq6k3AfYHH0a2Q83zgUcA/N01MgzPUgvw04KlJHjG9o/+V6m+yTNaQHYq+S/MRYB/gd6vq3MYpLYr+xhWPpZvRXQ6+Bew9wwZdkb433Q8gy0q/xNpj6Iqc5eAf+8d9x/bvB1xVVT9e5Hwmqv/+7cry645Dd9H4E/prO0b9BnAb3TVXy0JV3VxV36mqnyTZj+7f1kGtFKT2hjpD/nfAK4DPJHkjXXfqKOBKuovKlo0kz+2/nL4Y8FlJfgr8tKrObpTWQvqfdBeNHQPcnGT0Ap2rlsPoSpJ/BM4Hvk13MeCj6dboXk93keAmr+/4f3V8fxKAH1bVL722qUnyEeAyuu/l9XQXda6i+xX5+9pltqA+R7fW+glJ7gv8O93fz2eyPK/reDHd38PlME897v3AKcDpSY6nmyE/AHg+8O6qmml1kk1KkicCz6L7OwndKkGvA95eVf/SLDEN0iBvDAT/MbP6buB3gABfBl61jC6QAyDJhr7BZ1fVXouZyyQkuRx42AZe/vOqeuviZTMZSf6U7o6rj6RbG/hKuuL1bcvtz+u4/s/vcrkx0Cq6YuZhdHet/DHdqhxvqaplM6+aZFvgbXR3ddyebhnEY6vqo00TW2D9qNyP6O5p8ZzW+UxCkmcBf0o34rkV3W/k/hY4oarubJnbQkiyG10T7teBLemWyv3rqvpw08Q0SIMtyCVJkqSlYKgz5JIkSdKSYEEuSZIkNWRBLkmSJDVkQS5JkiQ1ZEEuSZIkNWRBLkmSJDVkQS5JkiQ1ZEEuSZIkNWRBLkmSJDX0/wCzG9bBkjwhzAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 921.6x633.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plot = sns.heatmap(state_visit_count)\n",
    "plot.invert_yaxis()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('conf'+str(conf)+'/heatmap-state-visit-count.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Q(Q, save_file='conf'+str(conf)+'/heatmap-policy.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1818.25it/s]\n"
     ]
    }
   ],
   "source": [
    "valuation = eval_Q(env,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = 17\n",
    "\n",
    "with open('conf'+str(conf)+'/hyperparameter-tuning-results.txt', 'w') as f:\n",
    "    f.write('conf '+ str(conf) + '\\n')\n",
    "    f.write('wind = ' + str(wind) + '\\n')\n",
    "    f.write('start state = ' + str(start_state) + '\\n')\n",
    "    f.write('p = ' + str(p) + '\\n')\n",
    "    f.write('strategy = e_greedy\\n\\n')\n",
    "    f.write('opt_alpha = ' + str(opt_alpha) + '\\n')\n",
    "    f.write('opt_gamma = ' + str(opt_gamma)+'\\n')\n",
    "    f.write('opt_epsilon = ' + str(opt_epsilon)+'\\n')\n",
    "    f.write('\\nconvergence: '+'\\n')\n",
    "    f.write('rewards = ' + str(conv_rewards)+'\\n')\n",
    "    f.write('steps = ' + str(conv_steps)+'\\n')\n",
    "    f.write('\\nevaluation: ' + str(valuation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
