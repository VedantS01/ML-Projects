{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon greedy\n",
    "def choose_action_epsilon(env,Q, state, hyper=0.1, rg=rg):\n",
    "    if not Q[int(state/(env.num_cols)), state%(env.num_cols)].any() or rg.rand() < hyper:\n",
    "        return rg.choice(Q.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(Q[int(state/(env.num_cols)), state%(env.num_cols)])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(env,Q, state,hyper=1, rg=rg):\n",
    "    return rg.choice(Q.shape[-1], p = softmax(Q[int(state/(env.num_cols)), state%(env.num_cols)] / hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 3\n",
    "# os.mkdir(\"conf\"+str(conf))\n",
    "wind = False\n",
    "start_state = [0,4]\n",
    "p = 0.7\n",
    "chosenAction = choose_action_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = 0.4\n",
    "opt_gamma = 0.8\n",
    "tol_alpha = 0.2\n",
    "tol_gamma = 0.1\n",
    "opt_epsilon = 0.\n",
    "tol_epsilon = 0.\n",
    "\n",
    "# hyper parameter set\n",
    "alphas = np.linspace(opt_alpha-tol_alpha,opt_alpha+tol_alpha,5)\n",
    "gammas = np.linspace(opt_gamma-tol_gamma,opt_gamma+tol_gamma,3)\n",
    "epsilons = [0,0.01,0.1]\n",
    "episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SpXfJ6XXLtTe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BqE09JUiL1B8"
   },
   "outputs": [],
   "source": [
    "# # specify world parameters\n",
    "# num_cols = 10\n",
    "# num_rows = 10\n",
    "# obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "#                          [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "#                          [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "#                          [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "# bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "# restart_states = np.array([[3,7],[8,2]])\n",
    "# start_state = np.array([[3,6]])\n",
    "# goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# # create model\n",
    "# gw = GridWorld(num_rows=num_rows,\n",
    "#                num_cols=num_cols,\n",
    "#                start_state=start_state,\n",
    "#                goal_states=goal_states, wind = False)\n",
    "# gw.add_obstructions(obstructed_states=obstructions,\n",
    "#                     bad_states=bad_states,\n",
    "#                     restart_states=restart_states)\n",
    "# gw.add_rewards(step_reward=-1,\n",
    "#                goal_reward=10,\n",
    "#                bad_state_reward=-6,\n",
    "#                restart_state_reward=-100)\n",
    "# gw.add_transition_probability(p_good_transition=0.7,\n",
    "#                               bias=0.5)\n",
    "# env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UdRce8oMZNb",
    "outputId": "ee3858e1-e109-42e6-80eb-336702f708e3"
   },
   "outputs": [],
   "source": [
    "# print(\"Number of actions\", env.num_actions) #0 -> UP, 1-> DOWN, 2 -> LEFT, 3-> RIGHT\n",
    "# print(\"Number of states\", env.num_states)\n",
    "# print(\"start state\", env.start_state_seq)\n",
    "# print(\"goal state(s)\", env.goal_states_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 1\n",
    "DOWN = 0\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "def plot_Q(Q, message = \"Q plot\", save_file=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(message)\n",
    "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
    "    plt.colorbar()\n",
    "    def x_direct(a):\n",
    "        if a in [UP, DOWN]:\n",
    "            return 0\n",
    "        return 1 if a == RIGHT else -1\n",
    "    def y_direct(a):\n",
    "        if a in [RIGHT, LEFT]:\n",
    "            return 0\n",
    "        return 1 if a == UP else -1\n",
    "    policy = Q.argmax(-1)\n",
    "    policyx = np.vectorize(x_direct)(policy)\n",
    "    policyy = np.vectorize(y_direct)(policy)\n",
    "    idx = np.indices(policy.shape)\n",
    "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
    "    if(save_file != None):\n",
    "        plt.savefig(save_file)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    # return fig\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sarsa(env,Q,gamma,alpha,epsilon,choose_action,plot_heat = False,max_timesteps=100) :\n",
    "  episode_rewards = []\n",
    "  state_visit_count = np.zeros((env.num_rows, env.num_cols))\n",
    "  steps_to_completion = []\n",
    "  steps = 0\n",
    "  for steps in tqdm(range(episodes)):\n",
    "    timesteps=0\n",
    "    env.reset()\n",
    "    current_state = env.start_state_seq[0]\n",
    "    current_action = choose_action(env,Q,current_state,hyper=epsilon)\n",
    "    rewards = []\n",
    "    tot_reward = 0 \n",
    "    while timesteps<max_timesteps :\n",
    "      timesteps+=1\n",
    "      next_state,reward = env.step(current_state,current_action)\n",
    "      next_action = choose_action(env,Q,next_state, hyper=epsilon)\n",
    "      # best next action\n",
    "#       best_next_action = np.argmax(Q[next_state//env.num_rows,next_state%env.num_cols])\n",
    "      Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action] += alpha*(reward[0] + gamma*Q[int(next_state/(env.num_cols)), next_state%(env.num_cols), next_action] - Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action])\n",
    "      rewards.append(reward[0])\n",
    "      tot_reward = tot_reward + reward[0]\n",
    "      # print(reward)\n",
    "      if reward == env.r_goal :\n",
    "        break\n",
    "      # print(current_state)\n",
    "      current_state = next_state\n",
    "      current_action = next_action\n",
    "      state_visit_count[int(current_state/(env.num_cols)), current_state%(env.num_cols)]+=1\n",
    "    episode_rewards.append(tot_reward)\n",
    "    steps_to_completion.append(timesteps)\n",
    "\n",
    "    if (steps+1)%10 == 0 and plot_heat:\n",
    "      clear_output(wait=True)\n",
    "      plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(steps+1, np.mean(episode_rewards[steps-10+1:steps]),\n",
    "                                                                           np.mean(steps_to_completion[steps-10+1:steps]),\n",
    "                                                                           Q.max(), Q.min()))\n",
    "  return Q, episode_rewards, steps_to_completion,state_visit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(env,Q, max_timesteps=100) :\n",
    "    episode_rewards = []\n",
    "    steps_to_completion = []\n",
    "    steps = 0\n",
    "    episodes = 100\n",
    "    for steps in tqdm(range(episodes)):\n",
    "        timesteps=0\n",
    "        env.reset()\n",
    "        current_state = env.start_state_seq[0]\n",
    "        current_action = np.argmax(Q[int(current_state/(env.num_cols)), current_state%(env.num_cols)])\n",
    "        rewards = []\n",
    "        tot_reward = 0 \n",
    "        while timesteps<max_timesteps :\n",
    "            timesteps+=1\n",
    "            next_state,reward = env.step(current_state,current_action)\n",
    "            next_action = np.argmax(Q[int(next_state/(env.num_cols)), next_state%(env.num_cols)])\n",
    "\n",
    "            rewards.append(reward[0])\n",
    "            tot_reward = tot_reward + reward[0]\n",
    "            # print(reward)\n",
    "            if reward == env.r_goal :\n",
    "                break\n",
    "            # print(current_state)\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "        episode_rewards.append(tot_reward)\n",
    "        steps_to_completion.append(timesteps)\n",
    "\n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([start_state])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = wind)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=p,\n",
    "                              bias=0.5)\n",
    "env = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:23<00:00, 836.54it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1754.62it/s]\n",
      "100%|██████████| 20000/20000 [00:46<00:00, 426.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 763.36it/s]\n",
      "100%|██████████| 20000/20000 [00:48<00:00, 411.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 819.69it/s]\n",
      "100%|██████████| 20000/20000 [00:29<00:00, 681.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 719.42it/s]\n",
      "100%|██████████| 20000/20000 [00:44<00:00, 450.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 729.93it/s]\n",
      "100%|██████████| 20000/20000 [00:48<00:00, 409.25it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 775.22it/s]\n",
      "100%|██████████| 20000/20000 [00:13<00:00, 1452.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1923.11it/s]\n",
      "100%|██████████| 20000/20000 [00:12<00:00, 1554.24it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.52it/s]\n",
      "100%|██████████| 20000/20000 [00:15<00:00, 1290.16it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2272.88it/s]\n",
      "100%|██████████| 20000/20000 [00:31<00:00, 634.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 793.66it/s]\n",
      "100%|██████████| 20000/20000 [00:45<00:00, 437.67it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 746.31it/s]\n",
      "100%|██████████| 20000/20000 [00:50<00:00, 399.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 781.26it/s]\n",
      "100%|██████████| 20000/20000 [00:31<00:00, 629.11it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 680.28it/s]\n",
      "100%|██████████| 20000/20000 [00:47<00:00, 424.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 763.37it/s]\n",
      "100%|██████████| 20000/20000 [00:50<00:00, 392.43it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 740.76it/s]\n",
      "100%|██████████| 20000/20000 [00:16<00:00, 1189.77it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1234.53it/s]\n",
      "100%|██████████| 20000/20000 [00:14<00:00, 1405.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2272.88it/s]\n",
      "100%|██████████| 20000/20000 [00:46<00:00, 430.51it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 793.65it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 581.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 769.26it/s]\n",
      "100%|██████████| 20000/20000 [00:49<00:00, 405.70it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 854.68it/s]\n",
      "100%|██████████| 20000/20000 [00:51<00:00, 386.14it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 746.28it/s]\n",
      "100%|██████████| 20000/20000 [00:37<00:00, 533.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 740.74it/s]\n",
      "100%|██████████| 20000/20000 [00:56<00:00, 356.23it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 613.50it/s]\n",
      "100%|██████████| 20000/20000 [00:48<00:00, 414.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 847.47it/s]\n",
      "100%|██████████| 20000/20000 [00:21<00:00, 912.33it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1315.86it/s]\n",
      "100%|██████████| 20000/20000 [00:22<00:00, 899.48it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 909.09it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 465.25it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 862.09it/s]\n",
      "100%|██████████| 20000/20000 [00:29<00:00, 670.22it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 884.99it/s]\n",
      "100%|██████████| 20000/20000 [00:40<00:00, 496.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 925.93it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 460.25it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 729.82it/s]\n",
      "100%|██████████| 20000/20000 [00:29<00:00, 673.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 847.49it/s]\n",
      "100%|██████████| 20000/20000 [00:39<00:00, 511.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 892.88it/s]\n",
      "100%|██████████| 20000/20000 [00:36<00:00, 547.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 970.89it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1041.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1428.57it/s]\n",
      "100%|██████████| 20000/20000 [00:27<00:00, 717.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1176.47it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 571.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1428.12it/s]\n",
      "100%|██████████| 20000/20000 [00:27<00:00, 735.24it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 970.86it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 578.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 990.10it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 555.74it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 709.22it/s]\n",
      "100%|██████████| 20000/20000 [00:25<00:00, 781.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1136.24it/s]\n",
      "100%|██████████| 20000/20000 [00:33<00:00, 590.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 900.91it/s]\n",
      "100%|██████████| 20000/20000 [00:36<00:00, 540.70it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1136.36it/s]\n",
      "100%|██████████| 20000/20000 [00:24<00:00, 830.91it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1030.87it/s]\n",
      "100%|██████████| 20000/20000 [00:33<00:00, 601.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1190.45it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 569.21it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1315.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004 0.9 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tune hyperparameters\n",
    "from math import inf\n",
    "seed = 42\n",
    "max_valuation = -inf\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "            rg = np.random.RandomState(seed)\n",
    "            if(conf % 2 == 0):\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)\n",
    "            else:\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "            valuation = eval_Q(env,Q)\n",
    "            if(valuation >= max_valuation):\n",
    "                opt_alpha = alpha\n",
    "                opt_gamma = gamma\n",
    "                opt_epsilon = epsilon\n",
    "                max_valuation = valuation\n",
    "            \n",
    "print(opt_alpha,opt_gamma,opt_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:13<00:00, 1483.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:17<00:00, 1123.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:10<00:00, 1867.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence of rewards approx.  -71.89999999999999\n",
      "Convergence of time steps approx.  73.06666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_avgs, reward_avgs, steps_avgs = [], [], []\n",
    "num_expts = 3\n",
    "\n",
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "    Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "    rg = np.random.RandomState(i)\n",
    "    if(conf % 2 == 0):\n",
    "        Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)\n",
    "    else:\n",
    "        Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "    Q_avgs.append(Q.copy())\n",
    "    reward_avgs.append(rewards)\n",
    "    steps_avgs.append(steps)\n",
    "    \n",
    "conv_rewards = np.mean(np.average(reward_avgs,axis=0)[episodes-10:episodes])\n",
    "conv_steps = np.mean(np.average(steps_avgs,axis=0)[episodes-10:episodes])\n",
    "print(\"Convergence of rewards approx. \", conv_rewards)\n",
    "print(\"Convergence of time steps approx. \", conv_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    os.mkdir('conf'+str(conf))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps to Goal')\n",
    "plt.plot(np.arange(episodes),np.average(steps_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/steps-vs-episodes.jpeg')\n",
    "plt.close()\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(np.arange(episodes),np.average(reward_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/rewards-vs-episodes.jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:35<00:00, 558.71it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "rg = np.random.RandomState(i)\n",
    "Q, rewards, steps, state_visit_count = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 921.6x633.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plot = sns.heatmap(state_visit_count)\n",
    "plot.invert_yaxis()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('conf'+str(conf)+'/heatmap-state-visit-count.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Q(Q, save_file='conf'+str(conf)+'/heatmap-policy.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2777.94it/s]\n"
     ]
    }
   ],
   "source": [
    "valuation = eval_Q(env,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = 17\n",
    "\n",
    "with open('conf'+str(conf)+'/hyperparameter-tuning-results.txt', 'w') as f:\n",
    "    f.write('conf '+ str(conf) + '\\n')\n",
    "    f.write('wind = ' + str(wind) + '\\n')\n",
    "    f.write('start state = ' + str(start_state) + '\\n')\n",
    "    f.write('p = ' + str(p) + '\\n')\n",
    "    f.write('strategy = e_greedy\\n\\n')\n",
    "    f.write('opt_alpha = ' + str(opt_alpha) + '\\n')\n",
    "    f.write('opt_gamma = ' + str(opt_gamma)+'\\n')\n",
    "    f.write('opt_epsilon = ' + str(opt_epsilon)+'\\n')\n",
    "    f.write('\\nconvergence: '+'\\n')\n",
    "    f.write('rewards = ' + str(conv_rewards)+'\\n')\n",
    "    f.write('steps = ' + str(conv_steps)+'\\n')\n",
    "    f.write('\\nevaluation: ' + str(valuation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
