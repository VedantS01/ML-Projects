{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon greedy\n",
    "def choose_action_epsilon(env,Q, state, hyper=0.1, rg=rg):\n",
    "    if not Q[int(state/(env.num_cols)), state%(env.num_cols)].any() or rg.rand() < hyper:\n",
    "        return rg.choice(Q.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(Q[int(state/(env.num_cols)), state%(env.num_cols)])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(env,Q, state,hyper=1, rg=rg):\n",
    "    return rg.choice(Q.shape[-1], p = softmax(Q[int(state/(env.num_cols)), state%(env.num_cols)] / hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 11\n",
    "# os.mkdir(\"conf\"+str(conf))\n",
    "wind = True\n",
    "start_state = [0,4]\n",
    "p = 0.7\n",
    "chosenAction = choose_action_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = 0.4\n",
    "opt_gamma = 0.7\n",
    "tol_alpha = 0.2\n",
    "tol_gamma = 0.2\n",
    "opt_epsilon = 0.\n",
    "tol_epsilon = 0.\n",
    "\n",
    "# hyper parameter set\n",
    "alphas = np.linspace(opt_alpha-tol_alpha,opt_alpha+tol_alpha,5)\n",
    "gammas = np.linspace(opt_gamma-tol_gamma,opt_gamma+tol_gamma,5)\n",
    "epsilons = [0,0.01,0.1]\n",
    "episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SpXfJ6XXLtTe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BqE09JUiL1B8"
   },
   "outputs": [],
   "source": [
    "# # specify world parameters\n",
    "# num_cols = 10\n",
    "# num_rows = 10\n",
    "# obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "#                          [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "#                          [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "#                          [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "# bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "# restart_states = np.array([[3,7],[8,2]])\n",
    "# start_state = np.array([[3,6]])\n",
    "# goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# # create model\n",
    "# gw = GridWorld(num_rows=num_rows,\n",
    "#                num_cols=num_cols,\n",
    "#                start_state=start_state,\n",
    "#                goal_states=goal_states, wind = False)\n",
    "# gw.add_obstructions(obstructed_states=obstructions,\n",
    "#                     bad_states=bad_states,\n",
    "#                     restart_states=restart_states)\n",
    "# gw.add_rewards(step_reward=-1,\n",
    "#                goal_reward=10,\n",
    "#                bad_state_reward=-6,\n",
    "#                restart_state_reward=-100)\n",
    "# gw.add_transition_probability(p_good_transition=0.7,\n",
    "#                               bias=0.5)\n",
    "# env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UdRce8oMZNb",
    "outputId": "ee3858e1-e109-42e6-80eb-336702f708e3"
   },
   "outputs": [],
   "source": [
    "# print(\"Number of actions\", env.num_actions) #0 -> UP, 1-> DOWN, 2 -> LEFT, 3-> RIGHT\n",
    "# print(\"Number of states\", env.num_states)\n",
    "# print(\"start state\", env.start_state_seq)\n",
    "# print(\"goal state(s)\", env.goal_states_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 1\n",
    "DOWN = 0\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "def plot_Q(Q, message = \"Q plot\", save_file=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(message)\n",
    "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
    "    plt.colorbar()\n",
    "    def x_direct(a):\n",
    "        if a in [UP, DOWN]:\n",
    "            return 0\n",
    "        return 1 if a == RIGHT else -1\n",
    "    def y_direct(a):\n",
    "        if a in [RIGHT, LEFT]:\n",
    "            return 0\n",
    "        return 1 if a == UP else -1\n",
    "    policy = Q.argmax(-1)\n",
    "    policyx = np.vectorize(x_direct)(policy)\n",
    "    policyy = np.vectorize(y_direct)(policy)\n",
    "    idx = np.indices(policy.shape)\n",
    "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
    "    if(save_file != None):\n",
    "        plt.savefig(save_file)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    # return fig\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sarsa(env,Q,gamma,alpha,epsilon,choose_action,plot_heat = False,max_timesteps=100) :\n",
    "  episode_rewards = []\n",
    "  state_visit_count = np.zeros((env.num_rows, env.num_cols))\n",
    "  steps_to_completion = []\n",
    "  steps = 0\n",
    "  for steps in tqdm(range(episodes)):\n",
    "    timesteps=0\n",
    "    env.reset()\n",
    "    current_state = env.start_state_seq[0]\n",
    "    current_action = choose_action(env,Q,current_state,hyper=epsilon)\n",
    "    rewards = []\n",
    "    tot_reward = 0 \n",
    "    while timesteps<max_timesteps :\n",
    "      timesteps+=1\n",
    "      next_state,reward = env.step(current_state,current_action)\n",
    "      next_action = choose_action(env,Q,next_state, hyper=epsilon)\n",
    "      # best next action\n",
    "#       best_next_action = np.argmax(Q[next_state//env.num_rows,next_state%env.num_cols])\n",
    "      Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action] += alpha*(reward[0] + gamma*Q[int(next_state/(env.num_cols)), next_state%(env.num_cols), next_action] - Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action])\n",
    "      rewards.append(reward[0])\n",
    "      tot_reward = tot_reward + reward[0]\n",
    "      # print(reward)\n",
    "      if reward == env.r_goal :\n",
    "        break\n",
    "      # print(current_state)\n",
    "      current_state = next_state\n",
    "      current_action = next_action\n",
    "      state_visit_count[int(current_state/(env.num_cols)), current_state%(env.num_cols)]+=1\n",
    "    episode_rewards.append(tot_reward)\n",
    "    steps_to_completion.append(timesteps)\n",
    "\n",
    "    if (steps+1)%10 == 0 and plot_heat:\n",
    "      clear_output(wait=True)\n",
    "      plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(steps+1, np.mean(episode_rewards[steps-10+1:steps]),\n",
    "                                                                           np.mean(steps_to_completion[steps-10+1:steps]),\n",
    "                                                                           Q.max(), Q.min()))\n",
    "  return Q, episode_rewards, steps_to_completion,state_visit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(env,Q, max_timesteps=100) :\n",
    "    episode_rewards = []\n",
    "    steps_to_completion = []\n",
    "    steps = 0\n",
    "    episodes = 100\n",
    "    for steps in tqdm(range(episodes)):\n",
    "        timesteps=0\n",
    "        env.reset()\n",
    "        current_state = env.start_state_seq[0]\n",
    "        current_action = np.argmax(Q[int(current_state/(env.num_cols)), current_state%(env.num_cols)])\n",
    "        rewards = []\n",
    "        tot_reward = 0 \n",
    "        while timesteps<max_timesteps :\n",
    "            timesteps+=1\n",
    "            next_state,reward = env.step(current_state,current_action)\n",
    "            next_action = np.argmax(Q[int(next_state/(env.num_cols)), next_state%(env.num_cols)])\n",
    "\n",
    "            rewards.append(reward[0])\n",
    "            tot_reward = tot_reward + reward[0]\n",
    "            # print(reward)\n",
    "            if reward == env.r_goal :\n",
    "                break\n",
    "            # print(current_state)\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "        episode_rewards.append(tot_reward)\n",
    "        steps_to_completion.append(timesteps)\n",
    "\n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([start_state])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = wind)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=p,\n",
    "                              bias=0.5)\n",
    "env = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:15<00:00, 264.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 446.43it/s]\n",
      "100%|██████████| 20000/20000 [01:19<00:00, 251.71it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 408.16it/s]\n",
      "100%|██████████| 20000/20000 [01:30<00:00, 221.83it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 338.98it/s]\n",
      "100%|██████████| 20000/20000 [01:19<00:00, 250.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 399.99it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 246.83it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 355.87it/s]\n",
      "100%|██████████| 20000/20000 [01:24<00:00, 236.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 380.22it/s]\n",
      "100%|██████████| 20000/20000 [01:19<00:00, 250.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 384.62it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 245.42it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 355.87it/s]\n",
      "100%|██████████| 20000/20000 [01:25<00:00, 233.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 369.00it/s]\n",
      "100%|██████████| 20000/20000 [01:18<00:00, 254.82it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 335.57it/s]\n",
      "100%|██████████| 20000/20000 [01:10<00:00, 281.81it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 384.62it/s]\n",
      "100%|██████████| 20000/20000 [01:24<00:00, 236.33it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 361.01it/s]\n",
      "100%|██████████| 20000/20000 [00:26<00:00, 740.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1123.63it/s]\n",
      "100%|██████████| 20000/20000 [00:27<00:00, 732.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1111.13it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 483.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1351.37it/s]\n",
      "100%|██████████| 20000/20000 [01:19<00:00, 251.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 377.36it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 245.99it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 359.72it/s]\n",
      "100%|██████████| 20000/20000 [01:27<00:00, 229.10it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 398.40it/s]\n",
      "100%|██████████| 20000/20000 [01:20<00:00, 249.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 386.11it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 244.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 359.71it/s]\n",
      "100%|██████████| 20000/20000 [01:25<00:00, 233.11it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 355.87it/s]\n",
      "100%|██████████| 20000/20000 [01:20<00:00, 248.68it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 353.37it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 243.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 341.30it/s]\n",
      "100%|██████████| 20000/20000 [01:26<00:00, 231.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 383.14it/s]\n",
      "100%|██████████| 20000/20000 [01:19<00:00, 252.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 396.83it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 246.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 340.14it/s]\n",
      "100%|██████████| 20000/20000 [01:25<00:00, 233.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 349.65it/s]\n",
      "100%|██████████| 20000/20000 [00:27<00:00, 719.01it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 943.40it/s]\n",
      "100%|██████████| 20000/20000 [00:30<00:00, 651.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1162.85it/s]\n",
      "100%|██████████| 20000/20000 [00:46<00:00, 426.86it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 943.45it/s]\n",
      "100%|██████████| 20000/20000 [01:22<00:00, 241.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 344.83it/s]\n",
      "100%|██████████| 20000/20000 [01:25<00:00, 233.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 377.36it/s]\n",
      "100%|██████████| 20000/20000 [01:26<00:00, 232.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 362.32it/s]\n",
      "100%|██████████| 20000/20000 [01:19<00:00, 250.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 359.71it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 244.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 374.53it/s]\n",
      "100%|██████████| 20000/20000 [01:27<00:00, 229.38it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 392.16it/s]\n",
      "100%|██████████| 20000/20000 [01:20<00:00, 249.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 401.60it/s]\n",
      "100%|██████████| 20000/20000 [01:31<00:00, 219.25it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 290.70it/s]\n",
      "100%|██████████| 20000/20000 [01:33<00:00, 214.79it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 366.30it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 245.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 432.91it/s]\n",
      "100%|██████████| 20000/20000 [01:21<00:00, 244.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 390.63it/s]\n",
      "100%|██████████| 20000/20000 [01:11<00:00, 278.64it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 444.44it/s]\n",
      "100%|██████████| 20000/20000 [00:40<00:00, 495.87it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 531.92it/s]\n",
      "100%|██████████| 20000/20000 [00:29<00:00, 676.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1149.42it/s]\n",
      "100%|██████████| 20000/20000 [00:53<00:00, 375.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 370.37it/s]\n",
      "100%|██████████| 20000/20000 [01:16<00:00, 262.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 357.15it/s]\n",
      "100%|██████████| 20000/20000 [01:22<00:00, 243.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 348.44it/s]\n",
      "100%|██████████| 20000/20000 [01:26<00:00, 230.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 375.94it/s]\n",
      "100%|██████████| 20000/20000 [01:08<00:00, 292.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 436.68it/s]\n",
      "100%|██████████| 20000/20000 [01:08<00:00, 292.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 458.71it/s]\n",
      "100%|██████████| 20000/20000 [01:11<00:00, 277.97it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 409.84it/s]\n",
      "100%|██████████| 20000/20000 [01:01<00:00, 325.01it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 406.51it/s]\n",
      "100%|██████████| 20000/20000 [01:06<00:00, 302.08it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 450.45it/s]\n",
      "100%|██████████| 20000/20000 [01:09<00:00, 289.37it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 485.43it/s]\n",
      "100%|██████████| 20000/20000 [01:07<00:00, 298.51it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 469.48it/s]\n",
      "100%|██████████| 20000/20000 [01:12<00:00, 276.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 458.72it/s]\n",
      "100%|██████████| 20000/20000 [01:17<00:00, 259.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 378.79it/s]\n",
      "100%|██████████| 20000/20000 [01:08<00:00, 292.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 543.48it/s]\n",
      "100%|██████████| 20000/20000 [00:52<00:00, 380.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 480.76it/s]\n",
      "100%|██████████| 20000/20000 [00:55<00:00, 357.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 465.12it/s]\n",
      "100%|██████████| 20000/20000 [01:01<00:00, 325.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 510.21it/s]\n",
      "100%|██████████| 20000/20000 [01:06<00:00, 302.27it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 460.84it/s]\n",
      "100%|██████████| 20000/20000 [01:08<00:00, 293.10it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 432.90it/s]\n",
      "100%|██████████| 20000/20000 [01:01<00:00, 327.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 442.48it/s]\n",
      "100%|██████████| 20000/20000 [01:11<00:00, 278.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 423.73it/s]\n",
      "100%|██████████| 20000/20000 [01:09<00:00, 287.22it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 454.55it/s]\n",
      "100%|██████████| 20000/20000 [01:07<00:00, 295.08it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 383.14it/s]\n",
      "100%|██████████| 20000/20000 [01:14<00:00, 266.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 414.93it/s]\n",
      "100%|██████████| 20000/20000 [01:18<00:00, 253.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 392.16it/s]\n",
      "100%|██████████| 20000/20000 [01:12<00:00, 276.80it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 413.23it/s]\n",
      "100%|██████████| 20000/20000 [01:15<00:00, 264.43it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 427.35it/s]\n",
      "100%|██████████| 20000/20000 [01:18<00:00, 253.50it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 395.26it/s]\n",
      "100%|██████████| 20000/20000 [01:05<00:00, 305.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 497.51it/s]\n",
      "100%|██████████| 20000/20000 [01:05<00:00, 304.18it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 404.86it/s]\n",
      "100%|██████████| 20000/20000 [01:05<00:00, 303.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 467.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.8999999999999999 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tune hyperparameters\n",
    "from math import inf\n",
    "seed = 42\n",
    "max_valuation = -inf\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "            rg = np.random.RandomState(seed)\n",
    "            if(conf % 2 == 0):\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)\n",
    "            else:\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "            valuation = eval_Q(env,Q)\n",
    "            if(valuation >= max_valuation):\n",
    "                opt_alpha = alpha\n",
    "                opt_gamma = gamma\n",
    "                opt_epsilon = epsilon\n",
    "                max_valuation = valuation\n",
    "            \n",
    "print(opt_alpha,opt_gamma,opt_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:28<00:00, 694.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:28<00:00, 700.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:28<00:00, 693.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence of rewards approx.  -34.733333333333334\n",
      "Convergence of time steps approx.  36.86666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_avgs, reward_avgs, steps_avgs = [], [], []\n",
    "num_expts = 3\n",
    "\n",
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "    Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "    rg = np.random.RandomState(i)\n",
    "    Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "    Q_avgs.append(Q.copy())\n",
    "    reward_avgs.append(rewards)\n",
    "    steps_avgs.append(steps)\n",
    "    \n",
    "conv_rewards = np.mean(np.average(reward_avgs,axis=0)[episodes-10:episodes])\n",
    "conv_steps = np.mean(np.average(steps_avgs,axis=0)[episodes-10:episodes])\n",
    "print(\"Convergence of rewards approx. \", conv_rewards)\n",
    "print(\"Convergence of time steps approx. \", conv_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    os.mkdir('conf'+str(conf))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps to Goal')\n",
    "plt.plot(np.arange(episodes),np.average(steps_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/steps-vs-episodes.jpeg')\n",
    "plt.close()\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(np.arange(episodes),np.average(reward_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/rewards-vs-episodes.jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:28<00:00, 709.76it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "rg = np.random.RandomState(i)\n",
    "Q, rewards, steps, state_visit_count = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAIKCAYAAABr6lqXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxNUlEQVR4nO3de5ztdV3v8dd7Cxsl5AAqKaiIlzQQb+hJyyMXTS4PhUrNY5olJ0pTSVMrvCugpKRgRnLSboBKSqWYF0QRy8B0gyRbwUNyUUARN6AoApv5nD9+v9HVcvbMbGat+c6e3+vpYz1m5re+81uf5Wzgs9/z+X1/qSokSZIktbGmdQGSJEnSkNmQS5IkSQ3ZkEuSJEkN2ZBLkiRJDdmQS5IkSQ3ZkEuSJEkNbdW6gLlstXZX92KUJEmr1sZbr0rrGkbddt3Xp9p7bX33+6+o97vSmJBLkiRJDa3IhFySJEnLaOb21hUMmg25JEnS0NVM6woGzZEVSZIkqSETckmSpKGbMSFvyYRckiRJasiEXJIkaeDKGfKmTMglSZKkhkzIJUmShs4Z8qZMyCVJkqSGTMglSZKGzhnypkzIJUmSpIZMyCVJkoZu5vbWFQyaCbkkSZLUkAm5JEnS0DlD3pQJuSRJktSQCbkkSdLQuQ95UzbkkiRJA1eOrDTlyIokSZLUkAm5JEnS0Dmy0pQJuSRJktTQggl5kq2B/wP8KvBQYCdgBrgG+DfgL6vq89MsUpIkSVPkDHlT8ybkSXYG1gEnAo8CClgLbA2sB/4n8O9J3rTUQpKsm30s9VySJEnSlmKhhPw4YHvgMVW1DiDJbsDfAzdV1R5JDgT+OcnFVfX30y1XkiRJEzdze+sKBi1Vteknk+8CR1TVqWPHHwJcBNyzqq5LcjRwYFU9ehJFbbV2100XJUmStIXbeOtVaV3DqFsuPmeqvdc2D9lnRb3flWahizrvAnx3juPf7b/3Z/uv/xX4+QnWJUmSpOVSM9N9aF4LNeTrgBckGV93BHAz8PWRY7dMsjBJkiRpCBaaIX8t8Ang4iSfBG4FHkt3MefRVXVzv+5RdBd5SpIkaUvjPuRNzduQV9XZSZ4IvA54LnA7cAnwm1X13pGlHwM+NLUqJUmSpFVqwX3Iq+pfgSctsOZLkypIkiRJy8w576YWbMglSZK0yjmy0tRCF3VKkiRJmiITckmSpIGr8sZALZmQS5IkSQ2ZkEuSJA2dF3U2ZUIuSZIkNWRCLkmSNHTustKUCbkkSZLUkAm5JEnS0DlD3pQJuSRJktSQCbkkSdLQzbgPeUsm5JIkSVJDJuSSJElD5wx5UybkkiRJUkMm5JIkSUPnPuRN2ZBLkiQNnSMrTTmyIkmSJDVkQi5JkjR0jqw0ZUIuSZIkNWRCLkmSNHQm5E2ZkEuSJEkNmZBLkiQNXNXtrUsYNBNySZIkqSETckmSpKFzhrwpE3JJkiSpIRNySZKkofNOnU2ZkEuSJEkNmZBLkiQNnTPkTZmQS5IkSQ2ZkEuSJA2dM+RN2ZBLkiQNnSMrTTmyIkmSJDVkQi5JkjR0jqw0ZUIuSZIkNWRCLkmSNHTOkDdlQi5JkiQ1ZEIuSZI0dCbkTZmQS5IkSQ0tKSFPsjOwoao2TqgeSZIkLTd3WWlqwYQ8ye8l+VySc5M8oz/2rCTXAtcANyZ5a5IspZAk62YfSzmPJEmSVp8kv5TkzCTXJvl+kvOTHDa25s59X3pNkpv7/vUJc5xrTZIjk1ye5EdJLkzytE287uFJLk5yS5JLkjx/E+t+JckF/fmuSPLqJHdazHubtyFP8jzgL4EANwCn9Mf+DjgTeCnwIeAPgd9dzAtKkiRphZmZme5jiZI8DDgL2Bo4HPg14AvAe5K8YGTpe/rnXws8hS48/kSSR4yd8ijg9cA7gYOA84APJDl47HUPB04CTgcOBD4AnDj2miQ5oF/zhf58JwCvBt60qPdXVfO9+XXAf1TVC0aK+nPgXVX1kpF17wR+saoetZgXXchWa3fddFGSJElbuI23XrWkyYJJu/nDx02197rLIS9f6iTFm4CXAztV1U0jx88FqKrHJXk48CXgsKr6m/75rYD1wCVVdUh/bGfgG8CxVfW6kXN9CrhHVT1s5HuvBj5WVb81su6vgUOAe1XVbf2xC4DvVdU+I+teS9eU37eqvjXf+1toZOVBwAdHvv4HYC1dKj7qQ8ADFjiXJEmSVqKame5j6dYCtwE3jx2/kZ/0s4f0a0778dvqrnN8P3BAkm36wwf05ztl7FynAHsl2b3/+nHAPeZYdzJwN+DxAEnuAzxiE+u2pkvM57VQQ34zsO3I17Of33ls3V2AHy30YpIkSVqBpjyyMnqt4B28XvBv+4/vSLJLkh36yY0nAm/vn9sTuKyqfjj2vevpGvAHjqy7Bbh0jnUAe4ysA7jojqyrqsuAH46s26SFGvIvAS9Jcpf+os1XAlcBL54dUu/j/N8fKU6SJEmamKq6CNgXOJSuF70e+Avg+VX1/n7ZTv3xcRtGnp/9eEP99Nz2XOuY45yLXTd7bKc5jv83C217+Ebgk/3JbuuP7Uc3tH5xkgvpIvrdgYPnOoEkSZJWuClve1hVey/l+5M8iK7/XA88n26K41DgXUl+VFWnLr3KduZtyKvqc0l+AXgW3QzM31bV+iRPBN4MPJRuKP6Pq+oTU69WkiRJQ/QmunD4KbMXUgKfSnI34IQk76MLkHeb43tnE+rZZPt6YIckGUvJ51oHsCPdbi2LWTdux5F1m7TgjYGq6svAl8eOXQo8Y6HvlSRJ0hZgAlsTTtlewIUjzfis/wB+A9iZLj3/1STbjs2R7wHcyk9mxtcD29BtSHLp2DqAr4ysg25G/JpFrjt3dlGS+9Fdf/kVFrDgjYEkSZKkxr4FPCLJ2rHjv0C3scgG4Ay6iY4fh8b9tY7PBM6sqlv6wx+nS9ufPXau5wAX9RdjQtdcX7eJdRuAzwFU1ZXAhZtYdxvwsYXe3IIJuSRJkla5lZ+Qv5PupjxnJDmRbob8ELqx6rdX1a3ABUlOA45PsjVwGfACumsdf9wsV9W1Sd4GHJnk+8D5dE37/v05Z9fdluQ1dDcCuoruxkT7A4cBL+5fc9YrgY8kOQl4H/BIuj3IT1hoD3KwIZckSdIKV1Uf7O+i+cfAu+m24P4v4IV0d9Kc9TzgGOBoYAe65PrAqjp/7JSvAm4C/gC4J3AJ8OtV9ZGx131XkgJeBrwCuBJ4UVWdOLbuo0meDrwO+G3g23Rz78cs5v3Ne6fOVrxTpyRJWs1W3J06T3vDdO/U+czXraj3u9I4Qy5JkiQ15MiKJEnS0K38GfJVzYRckiRJasiEXJIkaehMyJsyIZckSZIaMiGXJEkaujIhb8mGXJIkaegcWWnKkRVJkiSpIRNySZKkoVuBN4ocEhNySZIkqSETckmSpKFzhrwpE3JJkiSpIRNySZKkoTMhb8qEXJIkSWrIhFySJGnovDFQUybkkiRJUkMm5JIkSQNXM+5D3pIJuSRJktSQCbkkSdLQuctKUybkkiRJUkMm5JIkSUPnLitN2ZBLkiQNnRd1NuXIiiRJktSQCbkkSdLQeVFnUybkkiRJUkMm5JIkSUNnQt6UCbkkSZLUkAm5JEnS0JW7rLRkQi5JkiQ1ZEIuSZI0dM6QN2VCLkmSJDVkQi5JkjR03qmzKRNySZIkqSETckmSpKErZ8hbsiGXJEkaOkdWmpq3IU/yUeBDwGlVdcM0C0mybvbzO229yzRfSpIkSVoxFpohPxA4EbgmyWlJDk7i3LkkSdIqUjMzU31ofosZWXkZsBfw9P5xbZJTgL+vqi9PqpCq2vvHRa3d1d+bSJIkaRAWk3b/e1X9H+CewHOB/wReCnwpyflJjkhy92kWKUmSpCmaqek+NK9Fj59U1c1VdWpVHQDcBzgSWAscD1yV5J+nUqEkSZK0it2hefCquqaq3lJVDwUeA5wE/OJEK5MkSdLyqJnpPjSvJV+gWVXrquoIwK1RJEmSpM200EWd5wDfW8yJqmrj0suRJEnSsnPOu6l5G/Kq2m+5CpEkSZKGyDt1SpIkDZ17hTflTX4kSZKkhkzIJUmShs4Z8qZMyCVJkqSGTMglSZKGzr3Cm7IhlyRJGjpHVppyZEWSJElqyIRckiRp4MptD5syIZckSZIaMiGXJEkaOmfImzIhlyRJkhoyIZckSRo6E/KmTMglSZKkhkzIJUmShs4bAzVlQi5JkiQ1ZEIuSZI0dM6QN7UiG/K0LmAZrFmz+n85sSar/z3edvvG1iVIkqQt3IpsyCVJkrR8yoS8KRtySZKkobMhb2r1zxRIkiRJK5gJuSRJ0tDNuO1hSybkkiRJUkMm5JIkSUPnDHlTJuSSJElSQybkkiRJQ2dC3pQJuSRJktSQCbkkSdLAVZmQt2RCLkmSJDVkQi5JkjR0zpA3ZUIuSZIkNWRCLkmSNHQm5E2ZkEuSJEkNmZBLkiQNXJmQN2VDLkmSNHQ25E05siJJkiQ1ZEIuSZI0dDOtCxg2E3JJkiSpIRNySZKkgfOizrbucEKe5J5Jdp5kMZIkSdLQzNuQJ9k3ycFjx16c5GrgKuCaJFck+c2lFpJk3exjqeeSJEnSZpip6T40r4US8rcAe85+keT3gROALwEv6x8XA3+b5JlTqlGSJElatRaaIX8wXfM966XAX1bVC0eOHZ/kr4AjgdPuaCFVtffs51uv3dW/SkmSJC0Xd1lpaqGEfA3//Ud0P+ADc6z7B+AhE6pJkiRJGoyFGvLzgYNGvr4CuP8c6+4PXD+poiRJkrR8aqam+piUJAcn+WySm5J8L8kXk+w/8vyOSd6d5LokP0hyVpK95jjPnZO8Nck1SW5Ocm6SJ8yxbk2SI5NcnuRHSS5M8rRN1HZ4kouT3JLkkiTPX+z7Wqgh/1PgiP5CzrXAUcCbkxya5Gf6x68BRwMfXOyLSpIkSZsjye8BHwLWAb8KPINucmPb/vkAZwAHAi8GngZsDZyd5N5jp3sPcDjwWuApwDXAJ5I8YmzdUcDrgXfShdTnAR+YY9OTw4GTgNP71/8AcGKSFyzqvVXN/7eW/s2/Hbid7gLOnwO2G1v2GeDQqrppMS+6kCHMkK9Zs/rvybQmq/893nb7xtYlSJK2QBtvvSqtaxh1/dP2nWrvtePpn1nS+01yP+CrwJFVdfwm1hwK/DOwf1Wd3R/7H8BlwClVdUR/7OF010geVlV/0x/bClgPXFJVh/THdga+ARxbVa8beZ1PAfeoqoeNfO/VwMeq6rdG1v01cAhwr6q6bb73t2DHVFUnAT9P15R/r3/BS4HPA38NPLWqnjipZlySJEkacxjddY3vmmfNIcDVs804QFXdSJeaHzq27jZGNiOpqo3A+4EDkmzTHz4AWAucMvY6pwB7Jdm9//pxwD3mWHcycDfg8Qu9uUXdqbOqrqCL9CVJkrTKbAF36nw83aTG/07yGmA34HLg7VX1F/2aPYGL5vje9cBzk2zXB8h7ApdV1Q/nWLcWeGD/+Z7ALXRB9Pg6gD3o0vfZLcLHX3t03dnMY1ENuSRJklaxKW97OH7jx9Htrhdpl/7xVuCVwH/RzZC/M8lWVXUCsBNdkz5uQ/9xR+Cmft1cm5HMrttp5OMN9dPz3XOtY45zjq/bJBtySZIkrXRrgLsCv11V/9gf+3Q/W35kknc0q2wCbMglSZIGrqackN+BRHzcd4EHAZ8cO34m3a4m96JLqHec43vHE+zr6UZeNrVuw8i6HZJkLCWfax39a18zz7pNWv3bYEiSJGlLt36B52f4ydz3uD2AK0c2IFkP7J5k2znW3cpPZsbXA9sAD5hjHcBXxmobf+3xdZtkQy5JkjR0M1N+LN0/9R8PGDt+IPDNqvoW8GFg1yT7zD6ZZHvgqf1zs86g25/8GSPrtgKeCZxZVbf0hz9OtxvLs8de8znARVV1Wf/1ucB1m1i3AfjcQm/OkRVJkiStdB+l26nkpCR3B75O11A/GXhev+bDdM3xKUleQTdKciQQ4C2zJ6qqC5KcBhyfZGu6nVJeAOzOSFNdVdcmeRvdjPr36e5g/0xgf7qtE2fX3dbv/HJikquAs/o1hwEvrqpbF3pzNuSSJEkDN+0Z8qWqqkryK8CbgTfQzWtfDDy7qt7br5lJ8hTgOOBE4M50Dfp+VfWNsVM+DziG7m7zOwAXAgdW1flj615FtzPLHwD3BC4Bfr2qPjJW37uSFPAy4BXAlcCLqurExby/Be/U2YJ36lwdvFOnJElzW2l36rzuoH2m2nvd/WPnrKj3u9KYkEuSJA3dCk/IV7vVH2FKkiRJK5gJuSRJ0sCt9Bny1c6EXJIkSWrIhFySJGngTMjbsiGXJEkaOBvythxZkSRJkhoyIZckSRq6cpvwllZkQ77d2ru0LmHqtt9m29YlTN02a9a2LmHqvn7jNa1LkCRJW7gV2ZBLkiRp+ThD3pYz5JIkSVJDJuSSJEkDVzPOkLdkQi5JkiQ1ZEIuSZI0cM6Qt2VCLkmSJDVkQi5JkjRw5T7kTZmQS5IkSQ2ZkEuSJA2cM+RtmZBLkiRJDZmQS5IkDZz7kLdlQy5JkjRwVa0rGDZHViRJkqSGTMglSZIGzpGVtkzIJUmSpIZMyCVJkgbOhLwtE3JJkiSpIRNySZKkgXOXlbZMyCVJkqSGTMglSZIGzhnytkzIJUmSpIZMyCVJkgauyoS8pc1uyJPcHTgCeAxQwOeBP6+qDUspJMm62c93+JkHLOVUkiRJ0hZj3oY8yQbgSVV1fv/1fYB/B+4JfK1f9mTgt5M8tqq+Pc1iJUmSNHk107qCYVsoId9hbM2xwFrgf1bVBQBJHg18DHg98II7WkhV7T37+Y7bPdDNdyRJkpbJjCMrTW3uRZ0HAMfMNuMAVfVFukb94EkWJkmSJA3B5s6Q7wBcMMfx8+nGWCRJkrSF8aLOthbTkD86yXb9598Btp9jzQ7ADydVlCRJkjQUi2nI/7z/OPtXp32Afxlb8yjgikkVJUmSpOXjjYHaWqgh32+OYzfOcWx34P1LL0eSJEkalnkb8qo6ZzEnqarnTKYcSZIkLbdyf7umNneXFUmSJEkTtNl36pQkSdLq4gx5WybkkiRJUkMm5JIkSQPnnTrbMiGXJEmSGjIhlyRJGjjv1NmWCbkkSZLUkAm5JEnSwLkPeVs25JIkSQPnRZ1tObIiSZIkNWRCLkmSNHBe1NmWCbkkSZLUkAm5JEnSwHlRZ1sm5JIkSVJDJuSSJEkD5y4rbZmQS5IkSQ2tyIT8UTvcv3UJU7f7Vtu3LmHq7sna1iVM3bE3XtO6BEmSlsxdVtoyIZckSZIaWpEJuSRJkpaPM+RtmZBLkiRJDZmQS5IkDZzbkLdlQi5JkiQ1ZEIuSZI0cM6Qt2VDLkmSNHBue9iWIyuSJElSQybkkiRJAzfTuoCBMyGXJEmSGjIhlyRJGrjCGfKWTMglSZKkhkzIJUmSBm7GOwM1ZUIuSZIkNWRCLkmSNHAzzpA3ZUIuSZIkNWRCLkmSNHDustKWCbkkSZLUkAm5JEnSwHmnzrbucEOeZA3wUODSqvrh5EqSJEnScnJkpa2ljKzcFbgA2HtCtUiSJEmDM29CnuSN8zy9DRDgd5L8MlBV9bpJFidJkqTpc2SlrYVGVl4NFGzy9xgF/ObI53e4IU+ybvbz/Xf95Tt6GkmSJGmLstDIypnAt4FnVdWa0QewE12jvm9/7E7TLlaSJEmTNzPlh+Y3b0NeVQcCLwOOT/KJJA8cfXqShVTV3rOPSZ5XkiRJWskWvKizqt4H7AFcAfxnkjck2WbqlUmSJGlZFJnqQ/Nb1C4rVXV9Vf0u8GTgacB64GAmnJJLkiRJQ7NZ+5BX1b8leSTwx8B7plOSJEmSltOMIXZTm70PeVXdVlVHAw8G9ge+NOmiJEmSpKG4w3fqrKpvAN+YYC2SJElqYMY576aWcqdOSZIkSUtkQy5JkjRwNeXHpCX5eJJKcvTY8R2TvDvJdUl+kOSsJHvN8f13TvLWJNckuTnJuUmeMMe6NUmOTHJ5kh8luTDJ0zZR0+FJLk5yS5JLkjx/se/HhlySJElbjCTPAh4+x/EAZwAHAi+m2xlwa+DsJPceW/4e4HDgtcBTgGuATyR5xNi6o4DXA+8EDgLOAz6Q5OCx1z4cOAk4vX/9DwAnJnnBYt7THZ4hlyRJ0uqwpdxNM8mOwNuBlwLvHXv6EOCXgP2r6ux+/bnAZcAfAUf0xx4O/AZwWFX9TX/sHLptvd/Yn4ckOwMvB46tquP61zi7v1HmscBH+3VbAccAJ1fVq0bW7QIcleTdVXXbfO/LhFySJGngZpKpPiboT4GL+htXjjsEuHq2GQeoqhvpUvNDx9bdBpw2sm4j8H7ggJEbYB4ArAVOGXudU4C9kuzef/044B5zrDsZuBvw+IXelA25JEmSVrwkjweeC7xwE0v2BC6a4/h64L5JthtZd1lV/XCOdWuBB46suwW4dI510N3JfnYdc7z2+LpNcmRFkiRp4KZ96/Uk6/7b61XtvZnfv5ZuRvu4qrpkE8t2Ai6f4/iG/uOOwE39uuvnWbfTyMcbqmr8/5651jHHOcfXbZIJuSRJkla6PwLuQjerveqYkEuSJA3ctC/q3NxEfFSS+wKvAn4H2GZkxpv+6x2A79Ml1DvOcYrxBPt6YLd51m0YWbdDkoyl5HOto3/ta+ZZt0km5JIkSVrJ7g/cme6iyetHHtDtgnI9sBfdzPaec3z/HsCVVXVT//V6YPck286x7lZ+MjO+HtgGeMAc6wC+MrKOOV57fN0m2ZBLkiQN3Eym+1iiLwH7zfGArknfj66J/jCwa5J9Zr8xyfbAU/vnZp1Btz/5M0bWbQU8Ezizqm7pD3+cbjeWZ4/V8xy6nV4u678+F7huE+s2AJ9b6A06siJJkqQVq6puAD4zfry7DxBXVNVn+q8/TNccn5LkFXTJ+ZFAgLeMnO+CJKcBxyfZmm6f8hcAuzPSVFfVtUneBhyZ5PvA+XRN+/70e5X3625L8hq6GwFdBZzVrzkMeHFV3brQe7QhlyRJGrgZJrpXeBNVNZPkKcBxwIl0Yy7nAvtV1TfGlj+P7gLRo4EdgAuBA6vq/LF1r6LbmeUPgHsClwC/XlUfGXvtdyUp4GXAK4ArgRdV1YmLqT0/vZNLe0+895NXXlETtvtW27cuYeruydrWJUzdsVef07oESdIWaOOtV62oDvjUXZ4z1d7r2VefsqLe70pjQi5JkjRwqz4JXeG8qFOSJElqyIRckiRp4CawE4qWYEU25Aet2bl1CVP3J1ef3bqEqfvDXZ7QugRNwFD+He2vayVJrazIhlySJEnLZ9p36tT8bMglSZIGzt8StuVFnZIkSVJDJuSSJEkD50WdbZmQS5IkSQ2ZkEuSJA2cF3W2ZUIuSZIkNWRCLkmSNHAm5G2ZkEuSJEkNmZBLkiQNXLnLSlMm5JIkSVJDJuSSJEkD5wx5WybkkiRJUkMm5JIkSQNnQt6WDbkkSdLAVesCBs6RFUmSJKkhE3JJkqSBm3Hbw6ZMyCVJkqSGTMglSZIGzos62zIhlyRJkhoyIZckSRo4E/K2FmzIk9wHeDqwEXhfVV2X5L7AnwAPBC4F3lZVl061UkmSJGkVmndkJcnPA18G/gw4AfhSkgcDnwOeBewI/Cbw+b5Jv8OSrJt9LOU8kiRJ2jw15Yfmt9AM+euBbwIPAXYGPg98GPgWcL+qegxdSn4tXWIuSZIkaTMs1JD/IvDmqvpaVV1H13Q/CDiuqm4EqKpvA8cD+y2lkKrae/axlPNIkiRp88xkug/Nb6GG/B7AlSNfX95//PrYukuA+0yoJkmSJGkwFrqo83q6pnzW7cA64Htj67YHbp1gXZIkSVom7rLS1kIJ+VeAX5j9oqpmquoxVXXJ2LqHAf816eIkSZKk1W6hhPxPgZ0WcZ5HAf+w9HIkSZK03NwJpa15G/KqOnMxJ6mqX5tMOZIkSVpuM7bkTS00siJJkiRpiha8U6ckSZJWNy/qbMuEXJIkSWrIhFySJGngnCBvy4RckiRJasiEXJIkaeCcIW/LhFySJElqyIRckiRp4GbSuoJhMyGXJEmSGjIhlyRJGjjv1NmWCbkkSZLUkAm5JEnSwJmPt2VDLkmSNHBue9iWIyuSJElSQybkkiRJA+dFnW2ZkEuSJEkNmZBLkiQNnPl4WyuyIb8l/rFYDd529Wdbl6AJuNOaO7UuYVncac3q/4VhWP234vvRxltblyBJm21FNuSSJElaPu6y0tbqj4QkSZKkFcyEXJIkaeDcZaUtE3JJkiSpIRNySZKkgTMfb8uEXJIkSWrIhFySJGng3GWlLRNySZIkqSETckmSpIErp8ibsiGXJEkaOEdW2nJkRZIkSWrIhFySJGngvDFQWybkkiRJUkMm5JIkSQNnPt6WCbkkSZLUkAm5JEnSwDlD3pYJuSRJktSQCbkkSdLAuQ95WybkkiRJUkMm5JIkSQNXzpA3ZUIuSZIkNbSohjzJvkmeneRRm3h+1ySvnWxpkiRJWg4zU35ofvM25Em2S/LvwKeAk4EvJPl4kl3Glt4beN1SCkmybvaxlPNIkiRJW5KFEvJXAj8P/DawB/BC4JHA55PsMd3SJEmStBxqyv/T/BZqyH8NeF1VnVxVF1fVu4BHAd8GPpvkMZMqpKr2nn1M6pySJElamCMrbS3UkN8XuGD0QFVdBewDfBk4K8m+U6lMkiRJGoCFtj28lm4+/L+pqh8kOQg4HfgX4M+mUJskSZKWwUw5VtLSQgn5F4FD53qiqn7UP/cvwKsnXJckSZI0CAs15O8Ddktyt7merKqNwDOBk4ArJ1ybJEmSlkFN+aH5zTuyUlWn042lzLemgBdMsihJkiRpKBaaIZckSdIqN2OO3dSi7tQpSZIkaTpMyCVJkgbOm/e0ZUIuSZIkNWRCLkmSNHDeTbMtE3JJkiSpIRtySZKkgZuhpvpYqiRPT3J6kiuS3JzkkiRvTnLXsXU7Jnl3kuuS/CDJWUn2muN8d07y1iTX9Oc7N8kT5li3JsmRSS5P8qMkFyZ52iZqPDzJxUlu6et7/mLfnw25JEnSwNWU/zcBLwduB14JHAj8Jd19cD6ZZA1AkgBn9M+/GHgasDVwdpJ7j53vPcDhwGuBpwDXAJ9I8oixdUcBrwfeCRwEnAd8IMnBo4uSHE53o8zT+9f/AHBikkXdq8cZckmSJK10T62q74x8fU6SDcDfAfsCnwYOAX4J2L+qzgZIci5wGfBHwBH9sYcDvwEcVlV/0x87B1gPvLE/D0l2pvuLwLFVdVz/umcneSBwLPDRft1WwDHAyVX1qpF1uwBHJXl3Vd0235szIZckSRq4mSk/lmqsGZ/1hf7jrv3HQ4CrZ5vx/vtupEvNDx35vkOA24DTRtZtBN4PHJBkm/7wAcBa4JSx1z0F2CvJ7v3XjwPuMce6k4G7AY9f6P3ZkEuSJGmqkqwbfUzotPv0H7/af9wTuGiOdeuB+ybZbmTdZVX1wznWrQUeOLLuFuDSOdYB7DGyjjlee3zdJjmyIkmSNHBVW9aNgZLsSjdeclZVfbE/vBNw+RzLN/QfdwRu6tddP8+6nUY+3lA//X/OXOuY45zj6zbJhlySJElTVVV7T+pcfdL9IWAj8LxJnbclG3JJkqSBm8TWhMshyV3oZsLvD+xTVd8cefp6uhR83HiCfT2w2zzrNoys2yFJxlLyudbRv/Y186zbJGfIJUmStOIl2Rr4IPBo4OCq+vLYkvX8ZJ571B7AlVV108i63ZNsO8e6W/nJzPh6YBvgAXOsA/jKyDrmeO3xdZtkQy5JkjRwK32XlX6v8VOB/YFfqarz5lj2YWDXJPuMfN/2wFP752adQbc/+TNG1m0FPBM4s6pu6Q9/nG43lmePvc5zgIuq6rL+63OB6zaxbgPwuYXe34ocWbm9dQGSfmyvne7XuoRlsctW27cuYeoetOauCy/awh1/9WdblyBpOv6CroE+BvhBkseOPPfNfnTlw3TN8SlJXkE3SnIkEOAts4ur6oIkpwHH96n7ZXQ3Gdqdkaa6qq5N8jbgyCTfB86na9r3p9+rvF93W5LX0N0I6CrgrH7NYcCLq+rWhd7cimzIJUmStHwmdDfNaTqo//iq/jHqDcDrq2omyVOA44ATgTvTNej7VdU3xr7neXTN/dHADsCFwIFVdf7YulfR7czyB8A9gUuAX6+qj4wuqqp3JSngZcArgCuBF1XViYt5czbkkiRJWtGq6n6LXLeBLpk+bIF1NwN/2D/mW3c7XdN+9CJe+yTgpMXUOc6GXJIkaeC2lF1WVisv6pQkSZIaMiGXJEkauC3tTp2rjQ25JEnSwE1ia0LdcY6sSJIkSQ2ZkEuSJA3cFrDt4apmQi5JkiQ1ZEIuSZI0cG572JYJuSRJktSQCbkkSdLAue1hWybkkiRJUkMm5JIkSQPnDHlbJuSSJElSQybkkiRJA+c+5G2ZkEuSJEkNmZBLkiQN3Iy7rDRlQy5JkjRwtuNtObIiSZIkNWRCLkmSNHBue9iWCbkkSZLU0MQa8iRPSPLpJXz/utnHpGqSJEnSwmaoqT40v0km5PcA9png+SRJkqRVb8EZ8iT3XeS57rGUQqpq79nP37jbs/2rlCRJ0jIptz1sajEXdV7O4nbDySLXSZIkSeotpiG/Gfgs8MEF1j0a+N0lVyRJkqRl5Zx3W4tpyC8Ebq+q98y3KMkN2JBLkiRJm2UxDfk64OmLPF+WUIskSZIaKBPyphbTkB/LwuMqVNXpuK+5JEmStFkWbMir6irgqmWoRZIkSQ24y0pbJtqSJElSQ4sZWZEkSdIq5i4rbdmQS5IkDZwjK205siJJkiQ1ZEIuSZI0cI6stGVCLkmSJDVkQi5JkjRw3hioLRNySZIkqSETckmSpIGbcZeVpkzIJUmSpIZMyCVJkgbOGfK2TMglSZKkhkzIJUmSBs4Z8rZMyCVJkqSGTMglSZIGzhnytkzIJUmSpIZWZEKe1gUsgxvf8KTWJUzdnfY7uHUJU5ftdmpdwtTd/qnTW5ewLG770qWtS5i6bd/6jtYlTN3xu/yv1iVIWyRnyNtakQ25JEmSlo8jK205siJJkiQ1ZEIuSZI0cI6stGVCLkmSJDVkQi5JkjRwzpC3ZUIuSZIkNWRCLkmSNHBVM61LGDQTckmSJKkhE3JJkqSBm3GGvCkTckmSJKkhE3JJkqSBK/chb8qEXJIkSWrIhFySJGngnCFvy4ZckiRp4BxZacuRFUmSJKkhE3JJkqSBmzEhb8qEXJIkSWrIhFySJGngyos6mzIhlyRJkhoyIZckSRo4d1lpy4RckiRJasiEXJIkaeC8MVBbCybkSe6S5CVJzk7y7SS39o9v98dekmTbpRaSZN3sY6nnkiRJkrYU8ybkSe4DfBq4H/A54IPAhv7pnYA9gLcAL0zyxKq6cnqlSpIkaRqcIW9roZGV44GbgQdV1eVzLUhyP+CfgbcDT7ujhVTV3rOfH7Xbs/1TIUmSpEFYqCF/EvCcTTXjAFV1eZLXAidPsjBJkiQtD+/U2dZCM+Sb89PxJylJkiRtpoUS8rOAY5JcVFWXzbWgH1k5CvjkhGuTJEnSMnCGvK2FGvKXAGcDX0tyHnARcH3/3I7AnsBjgcuBl06nREmSJE2T2x62NW9DXlXfTPIw4HeBpwK/Qre7CnSN+XrgFcBfVdUPp1inJEmStCoteGOgqroZOKF/SJIkaZVxZKWtBW8MJEmSJGl6FkzIJUmStLq57WFbJuSSJElSQybkkiRJA1fustKUCbkkSZLUkAm5JEnSwDlD3pYJuSRJktSQCbkkSdLAuQ95WybkkiRJUkMm5JIkSQPnLitt2ZBLkiQNnCMrbTmyIkmSpBUvyX2SfDDJjUm+l+Qfk9y3dV2TYEIuSZI0cCs9IU+yLfBp4Bbgt4ACjgbOTvKwqvpBy/qWyoZckiRJK93hwP2BB1fVpQBJ/hP4f8DvAW9rWNuSObIiSZI0cDXlxwQcApw324wDVNVlwOeAQyfzEu2YkEuSJGmqkqwb/bqq9t7MU+wJfGiO4+uBZ9zRulaKFdmQv+aKU7NcrzX7B+QO/MHYogzhffoep+Shv7xsLwX+HFeLVu9x461XLdtrDeHnCMN4n0N4jwvZeOtVU+29xhvyO2An4Po5jm8AdlziuZtbkQ25JEmSVo8h/2VnMZwhlyRJ0kp3PXMn4ZtKzrcoNuSSJEla6dbTzZGP2wP4yjLXMnFZ6ftOSpIkadiSvAQ4Dvi5qvp6f+x+dNse/klV/Vm76pbOhlySJEkrWpKfAS4EbgZeTbeb4lHAXYGHVdVNDctbMkdWJEmStKL1d+LcH/gacDJwKnAZsP+W3oyDCbkkSZLUlAm5JEmS1JANuSRJktSQDbkkSZLUkA25JEmS1JANuSRJktSQDbkkSZLUkA25JEmS1NBgG/Ik90nywSQ3Jvlekn9Mct/WdU1Sknsn+fMk5yb5YZLqbzO7aiR5epLTk1yR5OYklyR5c5K7tq5tUpIckOTTSb6V5JYk30zyD0n2aF3bNCX5eP9n9ujWtUxCkn379zP+uKF1bZOW5OAkn01yU//v1y8m2b91XZOS5DOb+FlWko+3rm9SkvxSkjOTXJvk+0nOT3JY67omKcl+Sf6t/+/HhiQnJ/nZ1nVpeLZqXUALSbYFPg3cAvwW3e1XjwbOTvKw/m5Qq8EDgV8H1gH/Cjy5bTlT8XLgSuCVwDeBRwKvB/ZL8otVNdOwtknZie5neCLwHeC+wJ8A5yXZq6quaFncNCR5FvDw1nVMyRHAF0a+3tiqkGlI8nvAO/vHUXTBzyOAbRuWNWm/D2w/duxxwNuADy9/OZOX5GHAWcB5wOHAD4GnA+9Jsk1V/WXL+iYhyf8CzgQ+ATwNuBtdL/CpJHtX1S0t69OwDPJOnUn+gO5fnA+uqkv7Y7sD/w/4o6p6W8v6JiXJmtmGNMnvAH8F7F5VlzctbIKS3KOqvjN27LnA3wFPrKpPt6lsupI8GLgYeHlV/VnreiYpyY7AV4GXAu8FjqmqV7etaumS7AucDfxyVZ3Vtprp6H8D91XgyKo6vm01yyvJe4DnAPeqqg2t61mqJG+iCzx2Gr0teZJzAarqca1qm5QkZwH3Ax5SVRv7Y4+m+wvzC6vqxIblaWCGOrJyCHDebDMOUFWXAZ8DDm1W1YStknR4XuPNeG82fdx1OWtZZt/tP66qdLX3p8BFVfW+1oVosx0GzADval3Icup/6/oM4IzV0Iz31gK3ATePHb+R1dM7PBb45GwzDlBVX6T79+uvNqtKg7Ra/qHaXHsCF81xfD2wqudyB2Kf/uNXm1YxYUnulGRtkgcBJwHfAlZV05rk8cBzgRe2rmWKTk1ye5LvJnnvKrt25fF0v7n530n+K8nGJJcmWc0/T+iat7vS/WZutfjb/uM7kuySZIckhwNPBN7erqyJuh24dY7jtwAPXeZaNHCDnCGnm8m9fo7jG4Adl7kWTVCSXYE3Amf1Scdq8nlg7/7zS4H9q+rahvVMVJK1dH/ROK6qLmldzxTcCPwZcA7wPbrrHV4JnJvkkavkZ7lL/3gr3Xv7L7rk+J1JtqqqE1oWN0XPBa4FPta6kEmpqov6Mat/opuZhy4xf35Vvb9VXRN2CV1K/mNJdgPuRfdepWUz1IZcq1CS7YAP0Y1xPK9xOdPwm3QXkt2fbrbzk0kev4quCfgj4C7AMa0LmYaqugC4YOTQOUk+C/wH3YWeW/ycPN1vXe8K/HZV/WN/7NP9bPmRSd5Rq+zCpSS7AE8CThgdfdjS9b+JO53uN8fPpxtdORR4V5IfVdWpLeubkBOAU/qdnN5BF9b9X7qxq1U/8qmVZagjK9czdxK+qeRcK1ySuwBn0DWrB1TVNxuXNHFV9dWq+nw/W/1EYDu63Va2eP3YxquA1wDb9L8e36F/evbrOzUrcEqq6nzga8BjWtcyIbPXNnxy7PiZwM/SJY+rzXPo/lu6msZVAN5ElxI/pao+UlWfqqojgH8ATkiyxfcP/V8qjgZeBnwb+ApwFfBR4JqGpWmAtvh/oO6g9XRz5OP2oPsHUluQJFsDHwQeDRxcVV9uXNLUVdUNdGMrD2xcyqTcH7gzcArdX4pnH9D9NuB6YK82pS2L1ZIar1/g+dWYOv4WcGFVXdi6kAnbi+59jY9u/Afd9oA7L39Jk1dVrwHuDjyMboecZwEPAv6taWEanKE25B8GHpvk/rMH+l+p/hKrZA/ZoehTmlOB/YFfqarzGpe0LPobVzyEbkZ3NfgSsN8cD+ia9P3o/gKyqvRbrD2YrslZDf6p/3jA2PEDgW9W1beWuZ6p6n9+e7D60nHoLhp/RH9tx6hfAH5Ed83VqlBVP6iqL1fVt5McSPfv1kHtFKT2hjpD/lfAi4APJXk1XTp1FPANuovKVo0kT+8/nb0Y8KAk3wG+U1XnNCprkv6C7qKxY4AfJBm9QOebq2F0Jck/AecD/0l3MeDP0e3RvZHuIsEtXp/4f2b8eBKAK6rqp57b0iQ5FbiM7md5A91FnUfS/Yr8He0qm6iP0u21flKSuwNfp/vn88mszus6nkv3z+FqmKce907gA8AZSU6kmyE/BHgW8Paqmmt3ki1KkkcCB9H9MwndLkGvAN5SVf/erDAN0iBvDAQ/nll9O/DLQIBPAS9ZRRfIAZBkUz/gc6pq3+WsZRqSXA7stomn31BVr1++aqYjyR/T3XH1AXR7A3+Drnl982r78zqu//O7Wm4MdCRdM7Mb3V0rv0W3K8frqmrVzKsm2R54M91dHXek2wbx2Kp6b9PCJqwflbua7p4WT21dzzQkOQj4Y7oRzzvT/Ubu/wInVdXtLWubhCR70oVwDwW2odsq98+r6m+aFqZBGmxDLkmSJK0EQ50hlyRJklYEG3JJkiSpIRtySZIkqSEbckmSJKkhG3JJkiSpIRtySZIkqSEbckmSJKkhG3JJkiSpIRtySZIkqaH/D7b0IK5pXGUOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 921.6x633.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plot = sns.heatmap(state_visit_count)\n",
    "plot.invert_yaxis()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('conf'+str(conf)+'/heatmap-state-visit-count.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Q(Q, save_file='conf'+str(conf)+'/heatmap-policy.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1249.99it/s]\n"
     ]
    }
   ],
   "source": [
    "valuation = eval_Q(env,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = 17\n",
    "\n",
    "with open('conf'+str(conf)+'/hyperparameter-tuning-results.txt', 'w') as f:\n",
    "    f.write('conf '+ str(conf) + '\\n')\n",
    "    f.write('wind = ' + str(wind) + '\\n')\n",
    "    f.write('start state = ' + str(start_state) + '\\n')\n",
    "    f.write('p = ' + str(p) + '\\n')\n",
    "    f.write('strategy = e_greedy\\n\\n')\n",
    "    f.write('opt_alpha = ' + str(opt_alpha) + '\\n')\n",
    "    f.write('opt_gamma = ' + str(opt_gamma)+'\\n')\n",
    "    f.write('opt_epsilon = ' + str(opt_epsilon)+'\\n')\n",
    "    f.write('\\nconvergence: '+'\\n')\n",
    "    f.write('rewards = ' + str(conv_rewards)+'\\n')\n",
    "    f.write('steps = ' + str(conv_steps)+'\\n')\n",
    "    f.write('\\nevaluation: ' + str(valuation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
