{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon greedy\n",
    "def choose_action_epsilon(env,Q, state, hyper=0.1, rg=rg):\n",
    "    if not Q[int(state/(env.num_cols)), state%(env.num_cols)].any() or rg.rand() < hyper:\n",
    "        return rg.choice(Q.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(Q[int(state/(env.num_cols)), state%(env.num_cols)])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(env,Q, state,hyper=1, rg=rg):\n",
    "    return rg.choice(Q.shape[-1], p = softmax(Q[int(state/(env.num_cols)), state%(env.num_cols)] / hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 13\n",
    "# os.mkdir(\"conf\"+str(conf))\n",
    "wind = True\n",
    "start_state = [3,6]\n",
    "p = 1.0\n",
    "chosenAction = choose_action_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = 0.4\n",
    "opt_gamma = 0.8\n",
    "tol_alpha = 0.2\n",
    "tol_gamma = 0.1\n",
    "opt_epsilon = 0.\n",
    "tol_epsilon = 0.\n",
    "\n",
    "# hyper parameter set\n",
    "alphas = np.linspace(opt_alpha-tol_alpha,opt_alpha+tol_alpha,5)\n",
    "gammas = np.linspace(opt_gamma-tol_gamma,opt_gamma+tol_gamma,3)\n",
    "epsilons = [0,0.01,0.1]\n",
    "episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SpXfJ6XXLtTe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BqE09JUiL1B8"
   },
   "outputs": [],
   "source": [
    "# # specify world parameters\n",
    "# num_cols = 10\n",
    "# num_rows = 10\n",
    "# obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "#                          [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "#                          [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "#                          [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "# bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "# restart_states = np.array([[3,7],[8,2]])\n",
    "# start_state = np.array([[3,6]])\n",
    "# goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# # create model\n",
    "# gw = GridWorld(num_rows=num_rows,\n",
    "#                num_cols=num_cols,\n",
    "#                start_state=start_state,\n",
    "#                goal_states=goal_states, wind = False)\n",
    "# gw.add_obstructions(obstructed_states=obstructions,\n",
    "#                     bad_states=bad_states,\n",
    "#                     restart_states=restart_states)\n",
    "# gw.add_rewards(step_reward=-1,\n",
    "#                goal_reward=10,\n",
    "#                bad_state_reward=-6,\n",
    "#                restart_state_reward=-100)\n",
    "# gw.add_transition_probability(p_good_transition=0.7,\n",
    "#                               bias=0.5)\n",
    "# env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UdRce8oMZNb",
    "outputId": "ee3858e1-e109-42e6-80eb-336702f708e3"
   },
   "outputs": [],
   "source": [
    "# print(\"Number of actions\", env.num_actions) #0 -> UP, 1-> DOWN, 2 -> LEFT, 3-> RIGHT\n",
    "# print(\"Number of states\", env.num_states)\n",
    "# print(\"start state\", env.start_state_seq)\n",
    "# print(\"goal state(s)\", env.goal_states_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 1\n",
    "DOWN = 0\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "def plot_Q(Q, message = \"Q plot\", save_file=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(message)\n",
    "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
    "    plt.colorbar()\n",
    "    def x_direct(a):\n",
    "        if a in [UP, DOWN]:\n",
    "            return 0\n",
    "        return 1 if a == RIGHT else -1\n",
    "    def y_direct(a):\n",
    "        if a in [RIGHT, LEFT]:\n",
    "            return 0\n",
    "        return 1 if a == UP else -1\n",
    "    policy = Q.argmax(-1)\n",
    "    policyx = np.vectorize(x_direct)(policy)\n",
    "    policyy = np.vectorize(y_direct)(policy)\n",
    "    idx = np.indices(policy.shape)\n",
    "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
    "    if(save_file != None):\n",
    "        plt.savefig(save_file)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    # return fig\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sarsa(env,Q,gamma,alpha,epsilon,choose_action,plot_heat = False,max_timesteps=100) :\n",
    "  episode_rewards = []\n",
    "  state_visit_count = np.zeros((env.num_rows, env.num_cols))\n",
    "  steps_to_completion = []\n",
    "  steps = 0\n",
    "  for steps in tqdm(range(episodes)):\n",
    "    timesteps=0\n",
    "    env.reset()\n",
    "    current_state = env.start_state_seq[0]\n",
    "    current_action = choose_action(env,Q,current_state,hyper=epsilon)\n",
    "    rewards = []\n",
    "    tot_reward = 0 \n",
    "    while timesteps<max_timesteps :\n",
    "      timesteps+=1\n",
    "      next_state,reward = env.step(current_state,current_action)\n",
    "      next_action = choose_action(env,Q,next_state, hyper=epsilon)\n",
    "      # best next action\n",
    "#       best_next_action = np.argmax(Q[next_state//env.num_rows,next_state%env.num_cols])\n",
    "      Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action] += alpha*(reward[0] + gamma*Q[int(next_state/(env.num_cols)), next_state%(env.num_cols), next_action] - Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action])\n",
    "      rewards.append(reward[0])\n",
    "      tot_reward = tot_reward + reward[0]\n",
    "      # print(reward)\n",
    "      if reward == env.r_goal :\n",
    "        break\n",
    "      # print(current_state)\n",
    "      current_state = next_state\n",
    "      current_action = next_action\n",
    "      state_visit_count[int(current_state/(env.num_cols)), current_state%(env.num_cols)]+=1\n",
    "    episode_rewards.append(tot_reward)\n",
    "    steps_to_completion.append(timesteps)\n",
    "\n",
    "    if (steps+1)%10 == 0 and plot_heat:\n",
    "      clear_output(wait=True)\n",
    "      plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(steps+1, np.mean(episode_rewards[steps-10+1:steps]),\n",
    "                                                                           np.mean(steps_to_completion[steps-10+1:steps]),\n",
    "                                                                           Q.max(), Q.min()))\n",
    "  return Q, episode_rewards, steps_to_completion,state_visit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(env,Q, max_timesteps=100) :\n",
    "    episode_rewards = []\n",
    "    steps_to_completion = []\n",
    "    steps = 0\n",
    "    episodes = 100\n",
    "    for steps in tqdm(range(episodes)):\n",
    "        timesteps=0\n",
    "        env.reset()\n",
    "        current_state = env.start_state_seq[0]\n",
    "        current_action = np.argmax(Q[int(current_state/(env.num_cols)), current_state%(env.num_cols)])\n",
    "        rewards = []\n",
    "        tot_reward = 0 \n",
    "        while timesteps<max_timesteps :\n",
    "            timesteps+=1\n",
    "            next_state,reward = env.step(current_state,current_action)\n",
    "            next_action = np.argmax(Q[int(next_state/(env.num_cols)), next_state%(env.num_cols)])\n",
    "\n",
    "            rewards.append(reward[0])\n",
    "            tot_reward = tot_reward + reward[0]\n",
    "            # print(reward)\n",
    "            if reward == env.r_goal :\n",
    "                break\n",
    "            # print(current_state)\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "        episode_rewards.append(tot_reward)\n",
    "        steps_to_completion.append(timesteps)\n",
    "\n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([start_state])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = wind)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=p,\n",
    "                              bias=0.5)\n",
    "env = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:09<00:00, 2126.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3226.31it/s]\n",
      "100%|██████████| 20000/20000 [00:16<00:00, 1232.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 531.92it/s]\n",
      "100%|██████████| 20000/20000 [00:37<00:00, 532.50it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 2380.83it/s]\n",
      "100%|██████████| 20000/20000 [00:10<00:00, 1995.41it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2857.14it/s]\n",
      "100%|██████████| 20000/20000 [00:14<00:00, 1424.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1923.17it/s]\n",
      "100%|██████████| 20000/20000 [00:16<00:00, 1178.41it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2083.43it/s]\n",
      "100%|██████████| 20000/20000 [00:11<00:00, 1742.01it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2777.98it/s]\n",
      "100%|██████████| 20000/20000 [00:13<00:00, 1479.51it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2500.09it/s]\n",
      "100%|██████████| 20000/20000 [00:15<00:00, 1253.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2380.98it/s]\n",
      "100%|██████████| 20000/20000 [00:11<00:00, 1817.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2380.99it/s]\n",
      "100%|██████████| 20000/20000 [00:28<00:00, 704.97it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 408.16it/s]\n",
      "100%|██████████| 20000/20000 [00:51<00:00, 390.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 216.45it/s]\n",
      "100%|██████████| 20000/20000 [00:11<00:00, 1755.00it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2941.27it/s]\n",
      "100%|██████████| 20000/20000 [00:14<00:00, 1345.08it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1784.61it/s]\n",
      "100%|██████████| 20000/20000 [00:20<00:00, 990.44it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1515.17it/s]\n",
      "100%|██████████| 20000/20000 [00:12<00:00, 1650.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2173.83it/s]\n",
      "100%|██████████| 20000/20000 [00:16<00:00, 1245.33it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.79it/s]\n",
      "100%|██████████| 20000/20000 [00:18<00:00, 1084.54it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1785.81it/s]\n",
      "100%|██████████| 20000/20000 [00:11<00:00, 1739.58it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.52it/s]\n",
      "100%|██████████| 20000/20000 [00:52<00:00, 383.92it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1639.49it/s]\n",
      "100%|██████████| 20000/20000 [00:54<00:00, 364.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 485.44it/s]\n",
      "100%|██████████| 20000/20000 [00:09<00:00, 2008.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2941.45it/s]\n",
      "100%|██████████| 20000/20000 [00:14<00:00, 1351.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.67it/s]\n",
      "100%|██████████| 20000/20000 [00:21<00:00, 922.30it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1923.13it/s]\n",
      "100%|██████████| 20000/20000 [00:10<00:00, 1865.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.80it/s]\n",
      "100%|██████████| 20000/20000 [00:14<00:00, 1399.97it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1724.14it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1051.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2127.78it/s]\n",
      "100%|██████████| 20000/20000 [00:12<00:00, 1559.70it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2381.03it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 487.12it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 387.59it/s]\n",
      "100%|██████████| 20000/20000 [01:04<00:00, 311.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2380.99it/s]\n",
      "100%|██████████| 20000/20000 [00:10<00:00, 1983.34it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3030.37it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1041.18it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.64it/s]\n",
      "100%|██████████| 20000/20000 [00:30<00:00, 645.99it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1587.31it/s]\n",
      "100%|██████████| 20000/20000 [00:10<00:00, 1834.02it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2631.65it/s]\n",
      "100%|██████████| 20000/20000 [00:15<00:00, 1305.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2083.27it/s]\n",
      "100%|██████████| 20000/20000 [00:19<00:00, 1013.74it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2500.14it/s]\n",
      "100%|██████████| 20000/20000 [00:12<00:00, 1572.45it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2272.94it/s]\n",
      "100%|██████████| 20000/20000 [00:49<00:00, 403.18it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1851.79it/s]\n",
      "100%|██████████| 20000/20000 [01:03<00:00, 316.59it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 248.76it/s]\n",
      "100%|██████████| 20000/20000 [00:10<00:00, 1860.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2777.80it/s]\n",
      "100%|██████████| 20000/20000 [00:23<00:00, 838.72it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 2083.33it/s]\n",
      "100%|██████████| 20000/20000 [00:33<00:00, 592.14it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 364.97it/s]\n",
      "100%|██████████| 20000/20000 [00:10<00:00, 1879.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3030.57it/s]\n",
      "100%|██████████| 20000/20000 [00:13<00:00, 1442.38it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1449.28it/s]\n",
      "100%|██████████| 20000/20000 [00:23<00:00, 867.75it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1470.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000000000000001 0.7000000000000001 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tune hyperparameters\n",
    "from math import inf\n",
    "seed = 42\n",
    "max_valuation = -inf\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "            rg = np.random.RandomState(seed)\n",
    "            if(conf % 2 == 0):\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)\n",
    "            else:\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "            valuation = eval_Q(env,Q)\n",
    "            if(valuation >= max_valuation):\n",
    "                opt_alpha = alpha\n",
    "                opt_gamma = gamma\n",
    "                opt_epsilon = epsilon\n",
    "                max_valuation = valuation\n",
    "            \n",
    "print(opt_alpha,opt_gamma,opt_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:10<00:00, 285.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:48<00:00, 410.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:39<00:00, 502.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence of rewards approx.  -100.0\n",
      "Convergence of time steps approx.  100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_avgs, reward_avgs, steps_avgs = [], [], []\n",
    "num_expts = 3\n",
    "\n",
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "    Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "    rg = np.random.RandomState(i)\n",
    "    Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "    Q_avgs.append(Q.copy())\n",
    "    reward_avgs.append(rewards)\n",
    "    steps_avgs.append(steps)\n",
    "    \n",
    "conv_rewards = np.mean(np.average(reward_avgs,axis=0)[episodes-10:episodes])\n",
    "conv_steps = np.mean(np.average(steps_avgs,axis=0)[episodes-10:episodes])\n",
    "print(\"Convergence of rewards approx. \", conv_rewards)\n",
    "print(\"Convergence of time steps approx. \", conv_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    os.mkdir('conf'+str(conf))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps to Goal')\n",
    "plt.plot(np.arange(episodes),np.average(steps_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/steps-vs-episodes.jpeg')\n",
    "plt.close()\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(np.arange(episodes),np.average(reward_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/rewards-vs-episodes.jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:32<00:00, 623.34it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "rg = np.random.RandomState(i)\n",
    "Q, rewards, steps, state_visit_count = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 921.6x633.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plot = sns.heatmap(state_visit_count)\n",
    "plot.invert_yaxis()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('conf'+str(conf)+'/heatmap-state-visit-count.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Q(Q, save_file='conf'+str(conf)+'/heatmap-policy.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2499.88it/s]\n"
     ]
    }
   ],
   "source": [
    "valuation = eval_Q(env,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = 17\n",
    "\n",
    "with open('conf'+str(conf)+'/hyperparameter-tuning-results.txt', 'w') as f:\n",
    "    f.write('conf '+ str(conf) + '\\n')\n",
    "    f.write('wind = ' + str(wind) + '\\n')\n",
    "    f.write('start state = ' + str(start_state) + '\\n')\n",
    "    f.write('p = ' + str(p) + '\\n')\n",
    "    f.write('strategy = e_greedy\\n\\n')\n",
    "    f.write('opt_alpha = ' + str(opt_alpha) + '\\n')\n",
    "    f.write('opt_gamma = ' + str(opt_gamma)+'\\n')\n",
    "    f.write('opt_epsilon = ' + str(opt_epsilon)+'\\n')\n",
    "    f.write('\\nconvergence: '+'\\n')\n",
    "    f.write('rewards = ' + str(conv_rewards)+'\\n')\n",
    "    f.write('steps = ' + str(conv_steps)+'\\n')\n",
    "    f.write('\\nevaluation: ' + str(valuation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
