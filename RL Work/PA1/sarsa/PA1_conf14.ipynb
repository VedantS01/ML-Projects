{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon greedy\n",
    "def choose_action_epsilon(env,Q, state, hyper=0.1, rg=rg):\n",
    "    if not Q[int(state/(env.num_cols)), state%(env.num_cols)].any() or rg.rand() < hyper:\n",
    "        return rg.choice(Q.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(Q[int(state/(env.num_cols)), state%(env.num_cols)])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(env,Q, state,hyper=1, rg=rg):\n",
    "    return rg.choice(Q.shape[-1], p = softmax(Q[int(state/(env.num_cols)), state%(env.num_cols)] / hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 14\n",
    "wind = True\n",
    "start_state = [3,6]\n",
    "p = 1.0\n",
    "chosenAction = choose_action_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha = 0.4\n",
    "opt_gamma = 0.8\n",
    "tol_alpha = 0.2\n",
    "tol_gamma = 0.1\n",
    "opt_epsilon = 1.\n",
    "tol_epsilon = 0.\n",
    "\n",
    "# hyper parameter set\n",
    "alphas = np.linspace(opt_alpha-tol_alpha,opt_alpha+tol_alpha,5)\n",
    "gammas = np.linspace(opt_gamma-tol_gamma,opt_gamma+tol_gamma,3)\n",
    "epsilons = np.linspace(opt_epsilon-tol_epsilon,opt_epsilon+tol_epsilon,1)\n",
    "episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SpXfJ6XXLtTe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BqE09JUiL1B8"
   },
   "outputs": [],
   "source": [
    "# # specify world parameters\n",
    "# num_cols = 10\n",
    "# num_rows = 10\n",
    "# obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "#                          [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "#                          [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "#                          [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "# bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "# restart_states = np.array([[3,7],[8,2]])\n",
    "# start_state = np.array([[3,6]])\n",
    "# goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# # create model\n",
    "# gw = GridWorld(num_rows=num_rows,\n",
    "#                num_cols=num_cols,\n",
    "#                start_state=start_state,\n",
    "#                goal_states=goal_states, wind = False)\n",
    "# gw.add_obstructions(obstructed_states=obstructions,\n",
    "#                     bad_states=bad_states,\n",
    "#                     restart_states=restart_states)\n",
    "# gw.add_rewards(step_reward=-1,\n",
    "#                goal_reward=10,\n",
    "#                bad_state_reward=-6,\n",
    "#                restart_state_reward=-100)\n",
    "# gw.add_transition_probability(p_good_transition=0.7,\n",
    "#                               bias=0.5)\n",
    "# env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UdRce8oMZNb",
    "outputId": "ee3858e1-e109-42e6-80eb-336702f708e3"
   },
   "outputs": [],
   "source": [
    "# print(\"Number of actions\", env.num_actions) #0 -> UP, 1-> DOWN, 2 -> LEFT, 3-> RIGHT\n",
    "# print(\"Number of states\", env.num_states)\n",
    "# print(\"start state\", env.start_state_seq)\n",
    "# print(\"goal state(s)\", env.goal_states_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 1\n",
    "DOWN = 0\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "def plot_Q(Q, message = \"Q plot\", save_file=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(message)\n",
    "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
    "    plt.colorbar()\n",
    "    def x_direct(a):\n",
    "        if a in [UP, DOWN]:\n",
    "            return 0\n",
    "        return 1 if a == RIGHT else -1\n",
    "    def y_direct(a):\n",
    "        if a in [RIGHT, LEFT]:\n",
    "            return 0\n",
    "        return 1 if a == UP else -1\n",
    "    policy = Q.argmax(-1)\n",
    "    policyx = np.vectorize(x_direct)(policy)\n",
    "    policyy = np.vectorize(y_direct)(policy)\n",
    "    idx = np.indices(policy.shape)\n",
    "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
    "    if(save_file != None):\n",
    "        plt.savefig(save_file)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    # return fig\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sarsa(env,Q,gamma,alpha,epsilon,choose_action,plot_heat = False,max_timesteps=100) :\n",
    "  episode_rewards = []\n",
    "  state_visit_count = np.zeros((env.num_rows, env.num_cols))\n",
    "  steps_to_completion = []\n",
    "  steps = 0\n",
    "  for steps in tqdm(range(episodes)):\n",
    "    timesteps=0\n",
    "    env.reset()\n",
    "    current_state = env.start_state_seq[0]\n",
    "    current_action = choose_action(env,Q,current_state,hyper=epsilon)\n",
    "    rewards = []\n",
    "    tot_reward = 0 \n",
    "    while timesteps<max_timesteps :\n",
    "      timesteps+=1\n",
    "      next_state,reward = env.step(current_state,current_action)\n",
    "      next_action = choose_action(env,Q,next_state, hyper=epsilon)\n",
    "      # best next action\n",
    "#       best_next_action = np.argmax(Q[next_state//env.num_rows,next_state%env.num_cols])\n",
    "      Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action] += alpha*(reward[0] + gamma*Q[int(next_state/(env.num_cols)), next_state%(env.num_cols), next_action] - Q[int(current_state/(env.num_cols)), current_state%(env.num_cols), current_action])\n",
    "      rewards.append(reward[0])\n",
    "      tot_reward = tot_reward + reward[0]\n",
    "      # print(reward)\n",
    "      if reward == env.r_goal :\n",
    "        break\n",
    "      # print(current_state)\n",
    "      current_state = next_state\n",
    "      current_action = next_action\n",
    "      state_visit_count[int(current_state/(env.num_cols)), current_state%(env.num_cols)]+=1\n",
    "    episode_rewards.append(tot_reward)\n",
    "    steps_to_completion.append(timesteps)\n",
    "\n",
    "    if (steps+1)%10 == 0 and plot_heat:\n",
    "      clear_output(wait=True)\n",
    "      plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(steps+1, np.mean(episode_rewards[steps-10+1:steps]),\n",
    "                                                                           np.mean(steps_to_completion[steps-10+1:steps]),\n",
    "                                                                           Q.max(), Q.min()))\n",
    "  return Q, episode_rewards, steps_to_completion,state_visit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(env,Q, max_timesteps=100) :\n",
    "    episode_rewards = []\n",
    "    steps_to_completion = []\n",
    "    steps = 0\n",
    "    episodes = 100\n",
    "    for steps in tqdm(range(episodes)):\n",
    "        timesteps=0\n",
    "        env.reset()\n",
    "        current_state = env.start_state_seq[0]\n",
    "        current_action = np.argmax(Q[int(current_state/(env.num_cols)), current_state%(env.num_cols)])\n",
    "        rewards = []\n",
    "        tot_reward = 0 \n",
    "        while timesteps<max_timesteps :\n",
    "            timesteps+=1\n",
    "            next_state,reward = env.step(current_state,current_action)\n",
    "            next_action = np.argmax(Q[int(next_state/(env.num_cols)), next_state%(env.num_cols)])\n",
    "\n",
    "            rewards.append(reward[0])\n",
    "            tot_reward = tot_reward + reward[0]\n",
    "            # print(reward)\n",
    "            if reward == env.r_goal :\n",
    "                break\n",
    "            # print(current_state)\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "        episode_rewards.append(tot_reward)\n",
    "        steps_to_completion.append(timesteps)\n",
    "\n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([start_state])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = wind)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=p,\n",
    "                              bias=0.5)\n",
    "env = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3211/20000 [00:26<02:17, 121.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Files\\RL Playground\\sarsa\\PA1_conf14.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000010?line=8'>9</a>\u001b[0m rg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState(seed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000010?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m(conf \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000010?line=10'>11</a>\u001b[0m     Q, rewards, steps, _ \u001b[39m=\u001b[39m sarsa(env, Q,gamma,alpha,epsilon,choose_action \u001b[39m=\u001b[39;49m choose_action_softmax)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000010?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000010?line=12'>13</a>\u001b[0m     Q, rewards, steps, _ \u001b[39m=\u001b[39m sarsa(env, Q,gamma,alpha,epsilon,choose_action \u001b[39m=\u001b[39m choose_action_epsilon)\n",
      "\u001b[1;32md:\\Files\\RL Playground\\sarsa\\PA1_conf14.ipynb Cell 8'\u001b[0m in \u001b[0;36msarsa\u001b[1;34m(env, Q, gamma, alpha, epsilon, choose_action, plot_heat, max_timesteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000007?line=15'>16</a>\u001b[0m       timesteps\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000007?line=16'>17</a>\u001b[0m       next_state,reward \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(current_state,current_action)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000007?line=17'>18</a>\u001b[0m       next_action \u001b[39m=\u001b[39m choose_action(env,Q,next_state, hyper\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000007?line=18'>19</a>\u001b[0m       \u001b[39m# best next action\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000007?line=19'>20</a>\u001b[0m \u001b[39m#       best_next_action = np.argmax(Q[next_state//env.num_rows,next_state%env.num_cols])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000007?line=20'>21</a>\u001b[0m       Q[\u001b[39mint\u001b[39m(current_state\u001b[39m/\u001b[39m(env\u001b[39m.\u001b[39mnum_cols)), current_state\u001b[39m%\u001b[39m(env\u001b[39m.\u001b[39mnum_cols), current_action] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m alpha\u001b[39m*\u001b[39m(reward[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m gamma\u001b[39m*\u001b[39mQ[\u001b[39mint\u001b[39m(next_state\u001b[39m/\u001b[39m(env\u001b[39m.\u001b[39mnum_cols)), next_state\u001b[39m%\u001b[39m(env\u001b[39m.\u001b[39mnum_cols), next_action] \u001b[39m-\u001b[39m Q[\u001b[39mint\u001b[39m(current_state\u001b[39m/\u001b[39m(env\u001b[39m.\u001b[39mnum_cols)), current_state\u001b[39m%\u001b[39m(env\u001b[39m.\u001b[39mnum_cols), current_action])\n",
      "\u001b[1;32md:\\Files\\RL Playground\\sarsa\\PA1_conf14.ipynb Cell 1'\u001b[0m in \u001b[0;36mchoose_action_softmax\u001b[1;34m(env, Q, state, hyper, rg)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000000?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchoose_action_softmax\u001b[39m(env,Q, state,hyper\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, rg\u001b[39m=\u001b[39mrg):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000000?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rg\u001b[39m.\u001b[39;49mchoice(Q\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], p \u001b[39m=\u001b[39;49m softmax(Q[\u001b[39mint\u001b[39;49m(state\u001b[39m/\u001b[39;49m(env\u001b[39m.\u001b[39;49mnum_cols)), state\u001b[39m%\u001b[39;49m(env\u001b[39m.\u001b[39;49mnum_cols)] \u001b[39m/\u001b[39;49m hyper))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# tune hyperparameters\n",
    "from math import inf\n",
    "seed = 42\n",
    "max_valuation = -inf\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "            rg = np.random.RandomState(seed)\n",
    "            if(conf % 2 == 0):\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)\n",
    "            else:\n",
    "                Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_epsilon)\n",
    "            valuation = eval_Q(env,Q)\n",
    "            if(valuation >= max_valuation):\n",
    "                opt_alpha = alpha\n",
    "                opt_gamma = gamma\n",
    "                opt_epsilon = epsilon\n",
    "                max_valuation = valuation\n",
    "            \n",
    "print(opt_alpha,opt_gamma,opt_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_avgs, reward_avgs, steps_avgs = [], [], []\n",
    "num_expts = 3\n",
    "\n",
    "alpha = opt_alpha = 0.2\n",
    "gamma = opt_gamma = 0.9\n",
    "# epsilon = opt_epsilon\n",
    "episodes = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "    Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "    rg = np.random.RandomState(i)\n",
    "    Q, rewards, steps, _ = sarsa(env, Q,gamma,alpha,1,choose_action = choose_action_softmax)\n",
    "    Q_avgs.append(Q.copy())\n",
    "    reward_avgs.append(rewards)\n",
    "    steps_avgs.append(steps)\n",
    "    \n",
    "conv_rewards = np.mean(np.average(reward_avgs,axis=0)[episodes-10:episodes])\n",
    "conv_steps = np.mean(np.average(steps_avgs,axis=0)[episodes-10:episodes])\n",
    "print(\"Convergence of rewards approx. \", conv_rewards)\n",
    "print(\"Convergence of time steps approx. \", conv_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    os.mkdir('conf'+str(conf))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps to Goal')\n",
    "plt.plot(np.arange(episodes),np.average(steps_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/steps-vs-episodes.jpeg')\n",
    "plt.close()\n",
    "plt.style.use('seaborn-poster')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(np.arange(episodes),np.average(reward_avgs, 0))\n",
    "# plt.show()\n",
    "plt.savefig('conf'+str(conf)+'/rewards-vs-episodes.jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:53<00:00, 370.92it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha = opt_alpha\n",
    "gamma = opt_gamma\n",
    "epsilon = opt_epsilon\n",
    "episodes = 20000\n",
    "\n",
    "Q = np.zeros((env.num_rows, env.num_cols, env.num_actions))\n",
    "rg = np.random.RandomState(seed)\n",
    "Q, rewards, steps, state_visit_count = sarsa(env, Q,gamma,alpha,epsilon,choose_action = choose_action_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plot = sns.heatmap(state_visit_count)\n",
    "plot.invert_yaxis()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('conf'+str(conf)+'/heatmap-state-visit-count.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_Q(Q, save_file='conf'+str(conf)+'/heatmap-policy.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3845.94it/s]\n"
     ]
    }
   ],
   "source": [
    "valuation = eval_Q(env,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.7\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conv_rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Files\\RL Playground\\sarsa\\PA1_conf14.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000017?line=10'>11</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mopt_epsilon = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(opt_epsilon)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000017?line=11'>12</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mconvergence: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000017?line=12'>13</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mrewards = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(conv_rewards)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000017?line=13'>14</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39msteps = \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(conv_steps)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/sarsa/PA1_conf14.ipynb#ch0000017?line=14'>15</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mevaluation: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(valuation)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conv_rewards' is not defined"
     ]
    }
   ],
   "source": [
    "# conf = 17\n",
    "print(valuation)\n",
    "with open('conf'+str(conf)+'/hyperparameter-tuning-results.txt', 'w') as f:\n",
    "    f.write('conf '+ str(conf) + '\\n')\n",
    "    f.write('wind = ' + str(wind) + '\\n')\n",
    "    f.write('start state = ' + str(start_state) + '\\n')\n",
    "    f.write('p = ' + str(p) + '\\n')\n",
    "    f.write('strategy = e_greedy\\n\\n')\n",
    "    f.write('opt_alpha = ' + str(opt_alpha) + '\\n')\n",
    "    f.write('opt_gamma = ' + str(opt_gamma)+'\\n')\n",
    "    f.write('opt_epsilon = ' + str(opt_epsilon)+'\\n')\n",
    "    f.write('\\nconvergence: '+'\\n')\n",
    "    f.write('rewards = ' + str(conv_rewards)+'\\n')\n",
    "    f.write('steps = ' + str(conv_steps)+'\\n')\n",
    "    f.write('\\nevaluation: ' + str(valuation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
