{"cells":[{"cell_type":"markdown","metadata":{"id":"pn7PKu9r0asK"},"source":["#Tutorial 5 - Options Intro\n","\n","Please complete this tutorial to get an overview of options and an implementation of SMDP Q-Learning and Intra-Option Q-Learning.\n","\n","\n","### References:\n","\n"," [Recent Advances in Hierarchical Reinforcement\n","Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf) is a strong recommendation for topics in HRL that was covered in class. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed. \n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"P_DODRgW_ZKS","executionInfo":{"status":"ok","timestamp":1649354740550,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vedant Ashish Saboo cs19b074","userId":"15300228120683378524"}}},"outputs":[],"source":["'''\n","A bunch of imports, you don't have to worry about these\n","'''\n","\n","import numpy as np\n","import random\n","import gym\n","from gym.wrappers import Monitor\n","import glob\n","import io\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"fYNA5kiH_esJ","outputId":"1aa6fc9e-2a94-4590-aa90-c150df1d9eeb","executionInfo":{"status":"ok","timestamp":1649354741278,"user_tz":-330,"elapsed":731,"user":{"displayName":"Vedant Ashish Saboo cs19b074","userId":"15300228120683378524"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["36\n","Number of states: 48\n","Number of actions that an agent can take: 4\n","Action taken: down\n","Transition probability: {'prob': 1.0}\n","Next state: 36\n","Reward recieved: -1\n","Terminal state: False\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","o  o  o  o  o  o  o  o  o  o  o  o\n","x  C  C  C  C  C  C  C  C  C  C  T\n","\n"]}],"source":["'''\n","The environment used here is extremely similar to the openai gym ones.\n","At first glance it might look slightly different. \n","The usual commands we use for our experiments are added to this cell to aid you\n","work using this environment.\n","'''\n","\n","#Setting up the environment\n","from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n","env = CliffWalkingEnv()\n","\n","env.reset()\n","\n","#Current State\n","print(env.s)\n","\n","# 4x12 grid = 48 states\n","print (\"Number of states:\", env.nS)\n","\n","# Primitive Actions\n","action = [\"up\", \"right\", \"down\", \"left\"]\n","#correspond to [0,1,2,3] that's actually passed to the environment\n","\n","# either go left, up, down or right\n","print (\"Number of actions that an agent can take:\", env.nA)\n","\n","# Example Transitions\n","rnd_action = random.randint(0, 3)\n","print (\"Action taken:\", action[rnd_action])\n","next_state, reward, is_terminal, t_prob = env.step(rnd_action)\n","print (\"Transition probability:\", t_prob)\n","print (\"Next state:\", next_state)\n","print (\"Reward recieved:\", reward)\n","print (\"Terminal state:\", is_terminal)\n","env.render()"]},{"cell_type":"markdown","metadata":{"id":"apuaOxavDXus"},"source":["#### Options\n","We custom define very simple options here. They might not be the logical options for this settings deliberately chosen to visualise the Q Table better.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"g4MRC1p2DZbp","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1649354741279,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vedant Ashish Saboo cs19b074","userId":"15300228120683378524"}},"outputId":"2d30ea94-ccbb-4481-d0c3-d1577ab291d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nNow the new action space will contain\\nPrimitive Actions: [\"up\", \"right\", \"down\", \"left\"]\\nOptions: [\"Away\",\"Close\"]\\nTotal Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\\nCorresponding to [0,1,2,3,4,5]\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["# We are defining two more options here\n","# Option 1 [\"Away\"] - > Away from Cliff (ie keep going up)\n","# Option 2 [\"Close\"] - > Close to Cliff (ie keep going down) \n","\n","def Away(env,state):\n","    \n","    optdone = False\n","    optact = 0\n","    \n","    if (int(state/12) == 0):\n","        optdone = True\n","    \n","    return [optact,optdone]\n","    \n","def Close(env,state):\n","    \n","    optdone = False\n","    optact = 2\n","    \n","    if (int(state/12) == 2):\n","        optdone = True\n","\n","    if (int(state/12) == 3):\n","        optdone = True\n","    \n","    return [optact,optdone]\n","    \n","    \n","'''\n","Now the new action space will contain\n","Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n","Options: [\"Away\",\"Close\"]\n","Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n","Corresponding to [0,1,2,3,4,5]\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Jmv5c0XoK8GA"},"source":["# Task 1\n","Complete the code cell below\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bh_oghc7Ledh","executionInfo":{"status":"ok","timestamp":1649354741279,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vedant Ashish Saboo cs19b074","userId":"15300228120683378524"}}},"outputs":[],"source":["#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n","q_values_SMDP = np.zeros((48,6))\n","\n","#Update_Frequency Data structure? Check TODO 4\n","\n","# TODO: epsilon-greedy action selection function\n","def egreedy_policy(q_values,state,epsilon):\n","    rnd = np.random.random_sample()\n","    if(rnd < epsilon):\n","        return np.random.randint(q_values.shape[-1]) # total actions are last dimension of q values, ie. 6\n","    else:\n","        return np.argmax(q_values[state])"]},{"cell_type":"markdown","metadata":{"id":"N8VJYkqoLqlO"},"source":["# Task 2\n","Below is an incomplete code cell with the flow of SMDP Q-Learning. Complete the cell and train the agent using SMDP Q-Learning algorithm.\n","Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ok_5eQM7OCTj","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1649354771380,"user_tz":-330,"elapsed":30106,"user":{"displayName":"Vedant Ashish Saboo cs19b074","userId":"15300228120683378524"}},"outputId":"932762cd-e07c-4d48-af77-f4e5e6801774"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode  1000  Reward  -328\n"]}],"source":["#### SMDP Q-Learning \n","\n","# Add parameters you might need here\n","gamma = 0.99\n","alpha = 0.6\n","update_frequency_smdp = np.zeros(48)\n","rewards = []\n","\n","# Iterate over 1000 episodes\n","for epi in range(1000):\n","    state = env.reset()    \n","    done = False\n","\n","    total_reward = 0\n","\n","    # While episode is not over\n","    while not done:\n","        \n","        # Choose action        \n","        action = egreedy_policy(q_values_SMDP, state, epsilon=0.01)\n","        \n","        # Checking if primitive action\n","        if action < 4:\n","            # Perform regular Q-Learning update for state-action pair\n","            next_state, reward, done,_ = env.step(action)\n","            q_values_SMDP[state, action] += alpha * (reward + gamma*(q_values_SMDP[next_state].max()) - q_values_SMDP[state, action])\n","            update_frequency_smdp[state] +=1\n","            total_reward += reward\n","        \n","        # Checking if action chosen is an option\n","        reward_bar = 0\n","        if action == 4: # action => Away option\n","            \n","            optdone = False\n","            ini_state = state\n","            count = 0\n","            while (optdone == False):\n","                \n","                # Think about what this function might do?\n","                optact,optdone = Away(env,state) \n","                next_state, reward, done,_ = env.step(optact)\n","                total_reward += reward\n","                \n","                # Is this formulation right? What is this term?\n","                reward_bar = gamma*reward_bar + reward\n","                count += 1\n","            \n","                # Complete SMDP Q-Learning Update\n","                # Remember SMDP Updates. When & What do you update? \n","                if(optdone):\n","                    q_values_SMDP[ini_state, action] += alpha * (reward_bar + (gamma ** count)*(q_values_SMDP[next_state].max()) - q_values_SMDP[ini_state, action])\n","                    update_frequency_smdp[ini_state] +=1\n","                \n","                state = next_state\n","           \n","        if action == 5: # action => Close option\n","            \n","            optdone = False\n","            ini_state = state\n","            count = 0\n","            while (optdone == False):\n","                \n","                # Think about what this function might do?\n","                optact,optdone = Close(env,state) \n","                next_state, reward, done,_ = env.step(optact)\n","                total_reward += reward\n","                \n","                # Is this formulation right? What is this term?\n","                reward_bar = gamma*reward_bar + reward\n","                count += 1\n","            \n","                # Complete SMDP Q-Learning Update\n","                # Remember SMDP Updates. When & What do you update? \n","                if(optdone):\n","                    q_values_SMDP[ini_state, action] += alpha * (reward_bar + (gamma ** count)*(q_values_SMDP[next_state].max()) - q_values_SMDP[ini_state, action])\n","                    update_frequency_smdp[ini_state] +=1\n","                \n","                state = next_state\n","    print(\"\\rEpisode \", epi+1, \" Reward \", total_reward, end='')\n","    rewards.append(total_reward)\n","print(\"\")\n"]},{"cell_type":"markdown","metadata":{"id":"3SQFbRCHWQyO"},"source":["# Task 3\n","Using the same options and the SMDP code, implement Intra Option Q-Learning (In the code cell below). You *might not* always have to search through options to find the options with similar policies, think about it. Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"r6A2TdUHWVUN","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"84c0cfee-74bc-45eb-b704-21bc3b03b62a","executionInfo":{"status":"ok","timestamp":1649354903572,"user_tz":-330,"elapsed":132202,"user":{"displayName":"Vedant Ashish Saboo cs19b074","userId":"15300228120683378524"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode  1000  Reward  -545\n"]}],"source":["#### Intra-Option Q-Learning \n","\n","# Add parameters you might need here\n","#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n","q_values_intraq = np.zeros((48,6))\n","gamma = 0.99\n","alpha = 0.6\n","update_frequency_intraq = np.zeros(48)\n","rewards = []\n","\n","# Iterate over 1000 episodes\n","for epi in range(1000):\n","    state = env.reset()    \n","    done = False\n","\n","    total_reward = 0\n","\n","    # While episode is not over\n","    while not done:\n","        \n","        # Choose action        \n","        action = egreedy_policy(q_values_intraq, state, epsilon=0.01)\n","        \n","        # Checking if primitive action\n","        if action < 4:\n","            # Perform regular Q-Learning update for state-action pair\n","            next_state, reward, done,_ = env.step(action)\n","            q_values_intraq[state, action] += alpha * (reward + gamma*(q_values_intraq[next_state].max()) - q_values_intraq[state, action])\n","            update_frequency_intraq[state] +=1\n","            total_reward += reward\n","        \n","        # Checking if action chosen is an option\n","        if action == 4: # action => Away option\n","            \n","            optdone = False\n","            while (optdone == False):\n","                \n","                # Think about what this function might do?\n","                optact,optdone = Away(env,state) \n","                next_state, reward, done,_ = env.step(optact)\n","                total_reward += reward\n","            \n","                # Complete IntraQ Q Learning update\n","                q_values_intraq[state, optact] += alpha * (reward + gamma*(q_values_intraq[next_state].max()) - q_values_intraq[state, optact])\n","                if(optdone):\n","                    q_values_intraq[state, action] += alpha * (reward + gamma*(q_values_intraq[next_state].max()) - q_values_intraq[state, action])\n","                else:\n","                    q_values_intraq[state, action] += alpha * (reward + gamma*(q_values_intraq[next_state,action]) - q_values_intraq[state, action])\n","                update_frequency_intraq[state] += 1\n","                \n","                state = next_state\n","           \n","        if action == 5: # action => Close option\n","            \n","            optdone = False\n","            while (optdone == False):\n","                \n","                # Think about what this function might do?\n","                optact,optdone = Away(env,state) \n","                next_state, reward, done,_ = env.step(optact)\n","                total_reward += reward\n","            \n","                # Complete IntraQ Q Learning update\n","                q_values_intraq[state, optact] += alpha * (reward + gamma*(q_values_intraq[next_state].max()) - q_values_intraq[state, optact])\n","                if(optdone):\n","                    q_values_intraq[state, action] += alpha * (reward + gamma*(q_values_intraq[next_state].max()) - q_values_intraq[state, action])\n","                else:\n","                    q_values_intraq[state, action] += alpha * (reward + gamma*(q_values_intraq[next_state,action]) - q_values_intraq[state, action])\n","                update_frequency_intraq[state] += 1\n","                \n","                state = next_state\n","    print(\"\\rEpisode \", epi+1, \" Reward \", total_reward, end='')\n","    rewards.append(total_reward)\n","print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"JzUgcwL-VfkO"},"source":["# Task 4\n","Compare the two Q-Tables and Update Frequencies and provide comments."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"v8mZE74_Vhmg","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1649354903573,"user_tz":-330,"elapsed":12,"user":{"displayName":"Vedant Ashish Saboo cs19b074","userId":"15300228120683378524"}},"outputId":"2f6906d7-93ef-4c84-f52e-37f0b1a41855"},"outputs":[{"output_type":"stream","name":"stdout","text":["POLICY FOR SMDP\n","['-R', '-R', '-L', '-R', '-D', '-R', '-R', '-D', '-R', 'OA', '-R', 'OC']\n","['-D', '-R', '-R', '-D', '-R', 'OA', '-R', 'OC', '-U', 'OA', '-R', 'OC']\n","['-R', 'OA', '-R', 'OC', '-U', 'OA', '-R', 'OC', '-U', '-U', '-U', '-U']\n","['-U', 'OA', '-R', 'OC', '-U', '-U', '-U', '-U', '-U', '-U', 'OC', 'OC']\n","POLICY FOR INTRA OPTION\n","['-R', '-R', '-R', '-R', '-L', '-R', 'OA', '-R', '-R', '-R', '-R', '-D']\n","['-L', '-R', 'OA', '-R', '-R', '-R', '-R', '-D', '-R', '-U', '-U', '-R']\n","['-R', '-R', '-R', '-D', '-R', '-U', '-U', '-R', '-R', '-R', '-R', 'OC']\n","['-R', '-U', '-U', '-R', '-R', '-R', '-R', 'OC', '-R', '-U', '-R', '-D']\n","Q VALUES SMDP\n","[[  -4.78922951   -3.940399    -12.65797985   -4.94104057   -4.62599965\n","    -7.04855492]\n"," [  -5.02903269   -2.9952025    -5.32044538  -12.80774137   -4.11509961\n","   -68.07265895]\n"," [ -28.09716271  -21.31157379  -69.94756204   -2.39719026   -9.87557177\n","   -46.26082065]\n"," [  -4.60254709   -2.9701      -24.0566821    -3.18671626   -4.20988567\n","   -13.27162582]\n"," [  -1.1964       -1.194        -0.84         -0.84         -1.1964\n","   -67.83786518]\n"," [  -5.65985456   -3.940399    -26.36681457   -7.74852333   -7.36597952\n","   -14.30668777]\n"," [  -3.35438318   -1.95903061   -5.80461056   -4.37773527   -2.52553656\n","   -28.59376109]\n"," [  -1.1964       -1.09896      -0.936        -0.936        -2.64550719\n","   -64.6226366 ]\n"," [  -4.89477003   -3.940399    -10.83844972   -5.97836267   -8.87708734\n","   -14.37294642]\n"," [ -10.06372249  -64.9040161   -64.82225866   -5.42225866   -2.32810206\n","   -62.34059701]\n"," [  -6.86653544   -3.94037313   -6.7334577   -17.79107837   -3.94037425\n","   -13.33597233]\n"," [  -7.48285304   -4.55231606   -4.7663637   -27.08849062  -12.78488415\n","    -2.9701    ]\n"," [ -41.42030054  -91.95611739  -41.73822019  -42.10803673  -41.97350731\n","   -42.48258108]\n"," [ -33.4292759   -79.90743809  -79.21169961  -28.91000097  -10.74371356\n","   -79.60921648]\n"," [  -5.87114345   -3.67967501  -62.61557376   -5.16253449   -4.30313445\n","   -67.37604274]\n"," [  -0.6          -0.9564       -0.6          -0.6          -1.194\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [  -2.79407582   -2.3642394    -1.78206      -0.6          -3.36613506\n","     0.        ]\n"," [  -3.7920547    -2.77132938   -6.96998888   -8.85243565   -3.4037255\n","    -1.99      ]\n"," [ -39.29816223 -108.8188965   -38.94115618  -39.30423953  -39.67747061\n","   -38.98995889]\n"," [  -2.68027558   -0.6          -0.6          -0.6          -3.14185904\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [  -0.6          -0.6          -0.6          -0.6          -2.13136764\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [ -27.58508427   -5.08407804  -62.13787247   -9.43550346   -5.53795083\n","   -62.00232208]\n"," [ -25.69144477  -62.9522794   -42.92219764  -16.58443629   -7.76924728\n","   -44.28523032]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]\n"," [   0.            0.            0.            0.            0.\n","     0.        ]]\n","Q VALUES INTRA OPTION\n","[[ -4.12461523  -3.54375542 -26.37538036 -15.13949232 -14.60912724\n","   -3.84391305]\n"," [ -3.51457877  -2.12429886 -27.66723165  -5.60410279  -3.37885128\n","   -5.37116926]\n"," [ -3.23481868  -1.          -2.94204384  -4.8214566   -3.20583724\n","   -4.68067475]\n"," [ -3.16322337  -2.09052308 -13.45090205  -6.39455112  -4.08082397\n","   -4.17014014]\n"," [ -3.25065456  -9.46276975 -12.62405051  -1.99        -5.13659996\n","   -4.13471196]\n"," [ -5.12568895  -1.          -1.40624339  -4.48137598  -4.75702226\n","   -4.91480585]\n"," [ -3.70210008  -3.69612335  -8.64628161  -4.56220869  -3.49988428\n","   -3.70151237]\n"," [ -2.24078031  -2.09052364  -3.18192795  -3.36366706  -3.0684898\n","   -2.44444281]\n"," [ -3.12743595  -1.         -11.1213124   -2.41482986  -3.35944961\n","   -4.7450694 ]\n"," [ -2.6822906   -2.09052308  -7.7751727   -3.724141    -2.67131812\n","   -5.57255596]\n"," [ -2.19567053  -2.09052308 -14.40280043  -3.2698244   -6.20756321\n","   -2.19842583]\n"," [ -2.40717087  -2.18446157  -1.1015385   -4.91073508  -2.10651311\n","   -2.09058768]\n"," [ -4.36905265  -1.99        -8.24324983  -3.40669091  -6.78767801\n","  -23.03976709]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [ -2.08453891   0.           0.           0.          -2.452752\n","    0.        ]\n"," [ -3.3436431    0.           0.           0.          -3.6073525\n","   -5.57731949]\n"," [ -2.62406233   0.           0.           0.          -3.88075873\n","   -2.37643394]\n"," [ -2.38045685   0.           0.           0.           0.\n","   -2.40003237]\n"," [ -6.20370305  -3.63252122  -0.6         -0.6         -4.6001209\n","    0.        ]\n"," [ -4.40370942  -0.6         -0.6         -0.6         -4.33160418\n","   -4.87709411]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [ -2.37823117   0.           0.           0.           0.\n","   -2.77021913]\n"," [ -2.09100231  -6.27384801  -1.          -5.79778001  -3.82539251\n","   -3.11340856]\n"," [ -5.74012852 -19.70805366 -64.78508964  -6.33123875  -6.42313044\n","   -7.63888465]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [ -3.06962661 -62.07797994  -6.39700261  -6.60031485 -11.26448992\n","   -5.57747976]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]\n"," [  0.           0.           0.           0.           0.\n","    0.        ]]\n","UPDATE FREQUENCY SMDP\n","[2.62041e+05 4.76430e+04 2.92930e+04 4.48200e+03 1.00000e+01 1.28000e+03\n"," 4.77900e+03 1.30000e+01 1.99900e+03 1.90000e+01 3.66600e+03 2.27600e+03\n"," 7.38130e+04 1.03700e+03 2.10000e+01 5.00000e+00 0.00000e+00 0.00000e+00\n"," 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 5.00000e+00 2.03000e+02\n"," 1.99580e+04 5.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n"," 5.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 6.00000e+01\n"," 2.23951e+05 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n"," 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n","UPDATE FREQUENCY INTRA OPTION\n","[2.361892e+06 7.320700e+04 2.649400e+04 8.089000e+03 1.938600e+04\n"," 9.606000e+03 6.956000e+03 1.415500e+04 6.347000e+03 4.133000e+03\n"," 1.447700e+04 2.573500e+04 4.821200e+04 0.000000e+00 0.000000e+00\n"," 1.000000e+00 3.000000e+00 2.000000e+00 1.000000e+00 5.000000e+00\n"," 7.000000e+00 0.000000e+00 2.000000e+00 1.615000e+03 2.118778e+06\n"," 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n"," 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n"," 0.000000e+00 1.839030e+05 0.000000e+00 0.000000e+00 0.000000e+00\n"," 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n"," 0.000000e+00 0.000000e+00 0.000000e+00]\n"]}],"source":["# Use this cell for Task 4 Code\n","def plot_q(q_values):\n","  # act = []\n","  for i in range(4):\n","    row = []\n","    for j in range(12):\n","      s = 4*i + j\n","      values = q_values[s]\n","      astar = np.argmax(values)\n","      if(astar == 0):\n","        row.append('-U')\n","      elif(astar == 1):\n","        row.append('-R')\n","      elif(astar == 2):\n","        row.append('-D')\n","      elif(astar == 3):\n","        row.append('-L')\n","      elif(astar == 4):\n","        row.append('OA')\n","      elif(astar == 5):\n","        row.append('OC')\n","    print(row)\n","\n","print(\"POLICY FOR SMDP\")\n","plot_q(q_values_SMDP)\n","\n","print(\"POLICY FOR INTRA OPTION\")\n","plot_q(q_values_intraq)\n","\n","print(\"Q VALUES SMDP\")\n","print(q_values_SMDP)\n","print(\"Q VALUES INTRA OPTION\")\n","print(q_values_intraq)\n","print(\"UPDATE FREQUENCY SMDP\")\n","print(update_frequency_smdp)\n","print(\"UPDATE FREQUENCY INTRA OPTION\")\n","print(update_frequency_intraq)"]},{"cell_type":"markdown","metadata":{"id":"SemE13ORV04n"},"source":["We can clearly see the differences between the SMDP and INTRA OPTION update frequencies.\n","\n","Intra option has significantly larger frequencies than smdp models. By the way, by update frequency, I mean the frequency with which each state was updated. \n","\n","This leads to two things. SMDP is better because it doesn't have to update again and again, but it also does not update multiple states while it is executing an option. In which regard, Intra option is better.\n","\n","We can also see this through the Q values. Intra option q values are better adjusted, but SMDP has better estimated the policy at only but a few states.\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["5LBh6_lOVBdN"],"name":"Tutorial 5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":0}