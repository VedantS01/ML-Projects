{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn7PKu9r0asK"
   },
   "source": [
    "#**Tutorial 4 - DQN and Actor-Critic**\n",
    "\n",
    "Please follow this tutorial to understand the structure (code) of DQNs & get familiar with Actor Critic methods.\n",
    "\n",
    "\n",
    "### References:\n",
    "\n",
    "Please follow [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) for the original publication as well as the psuedocode. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed. \n",
    "\n",
    "\n",
    "##**Part 1: DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P_DODRgW_ZKS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A bunch of imports, you don't have to worry about these\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYNA5kiH_esJ",
    "outputId": "d623e007-d532-4162-d93b-6023b34c80ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "0\n",
      "----\n",
      "[ 0.01369617 -0.02302133 -0.04590265 -0.04834723]\n",
      "----\n",
      "1\n",
      "----\n",
      "[ 0.01323574  0.17272775 -0.04686959 -0.3551522 ]\n",
      "1.0\n",
      "False\n",
      "{}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Please refer to the first tutorial for more details on the specifics of environments\n",
    "We've only added important commands you might find useful for experiments.\n",
    "'''\n",
    "\n",
    "'''\n",
    "List of example environments\n",
    "(Source - https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "'Acrobot-v1'\n",
    "'CartPole-v0'\n",
    "'MountainCar-v0'\n",
    "'''\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "print(env.action_space.sample())\n",
    "print(\"----\")\n",
    "\n",
    "'''\n",
    "# Understanding State, Action, Reward Dynamics\n",
    "\n",
    "The agent decides an action to take depending on the state.\n",
    "\n",
    "The Environment keeps a variable specifically for the current state.\n",
    "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
    "- It returns the new current state and reward for the agent to take the next action\n",
    "\n",
    "'''\n",
    "\n",
    "state = env.reset()   \n",
    "''' This returns the initial state (when environment is reset) '''\n",
    "\n",
    "print(state)\n",
    "print(\"----\")\n",
    "\n",
    "action = env.action_space.sample()  \n",
    "''' We take a random action now '''\n",
    "\n",
    "print(action)\n",
    "print(\"----\")\n",
    "\n",
    "next_state, reward, done, info = env.step(action) \n",
    "''' env.step is used to calculate new state and obtain reward based on old state and action taken  ''' \n",
    "\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(\"----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apuaOxavDXus"
   },
   "source": [
    "## DQN\n",
    "\n",
    "Using NNs as substitutes isn't something new. It has been tried earlier, but the 'human control' paper really popularised using NNs by providing a few stability ideas (Q-Targets, Experience Replay & Truncation). The 'Deep-Q Network' (DQN) Algorithm can be broken down into having the following components. \n",
    "\n",
    "### Q-Network:\n",
    "The neural network used as a function approximator is defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g4MRC1p2DZbp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### Q Network & Some 'hyperparameters'\n",
    "\n",
    "QNetwork1:\n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "QNetwork2: Feel free to experiment more\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "'''\n",
    "Bunch of Hyper parameters (Which you might have to tune later **wink wink**)\n",
    "'''\n",
    "BUFFER_SIZE = int(1e5)  \n",
    "'''\n",
    "replay buffer size \n",
    "'''\n",
    "BATCH_SIZE = 256         \n",
    "''' \n",
    "minibatch size \n",
    "'''\n",
    "GAMMA = 0.99            \n",
    "''' \n",
    "discount factor \n",
    "'''\n",
    "LR = 5e-4              \n",
    "''' \n",
    "learning rate \n",
    "'''\n",
    "UPDATE_EVERY = 20       \n",
    "''' \n",
    "how often to update the network (When Q target is present) \n",
    "'''\n",
    "\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64,fc3_units=32):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork1, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.tanh(self.fc1(state))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmv5c0XoK8GA"
   },
   "source": [
    "### Replay Buffer:\n",
    "\n",
    "This is a 'deque' that helps us store experiences. Recall why we use such a technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bh_oghc7Ledh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8VJYkqoLqlO"
   },
   "source": [
    "## Truncation:\n",
    "We add a line (optionally) in the code to truncate the gradient in hopes that it would help with the stability of the learning process.\n",
    "\n",
    "## Tutorial Agent Code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ok_5eQM7OCTj"
   },
   "outputs": [],
   "source": [
    "class TutorialAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed,196,98).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed,196,98).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "        self.rg = np.random.RandomState(seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "#         probabs = softmax(action_values.cpu().data.numpy()[0])\n",
    "#         probabs /= probabs.sum()\n",
    "#         return self.rg.choice(np.arange(self.action_size), p = probabs)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ''' Get max predicted Q values (for next states) from target model'''\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ''' Compute Q targets for current states '''\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        ''' Get expected Q values from local model '''\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ''' Compute loss '''\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        ''' Minimize the loss '''\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        ''' Gradiant Clipping '''\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SQFbRCHWQyO"
   },
   "source": [
    "### Here, we present the DQN algorithm code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_episodes = []\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6A2TdUHWVUN",
    "outputId": "eae554fe-3450-40d8-d462-64b92d755866"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 138.04\n",
      "Episode 200\tAverage Score: 318.70\n",
      "Episode 300\tAverage Score: 395.74\n",
      "Episode 400\tAverage Score: 378.49\n",
      "Episode 500\tAverage Score: 377.66\n",
      "Episode 600\tAverage Score: 398.95\n",
      "Episode 700\tAverage Score: 352.90\n",
      "Episode 800\tAverage Score: 354.26\n",
      "Episode 900\tAverage Score: 383.04\n",
      "Episode 950\tAverage Score: 375.33"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Files\\RL Playground\\PA2\\Assignment2-CartPole-v1_1-Copy13 copy 3.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=52'>53</a>\u001b[0m begin_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=53'>54</a>\u001b[0m agent \u001b[39m=\u001b[39m TutorialAgent(state_size\u001b[39m=\u001b[39mstate_shape,action_size \u001b[39m=\u001b[39m action_shape,seed \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=56'>57</a>\u001b[0m dqn()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=59'>60</a>\u001b[0m time_taken \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m begin_time\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(time_taken)\n",
      "\u001b[1;32md:\\Files\\RL Playground\\PA2\\Assignment2-CartPole-v1_1-Copy13 copy 3.ipynb Cell 12'\u001b[0m in \u001b[0;36mdqn\u001b[1;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=21'>22</a>\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_t):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=23'>24</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(state, eps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=24'>25</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000011?line=25'>26</a>\u001b[0m     agent\u001b[39m.\u001b[39mstep(state, action, reward, next_state, done)\n",
      "\u001b[1;32md:\\Files\\RL Playground\\PA2\\Assignment2-CartPole-v1_1-Copy13 copy 3.ipynb Cell 9'\u001b[0m in \u001b[0;36mTutorialAgent.act\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000008?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnetwork_local\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000008?line=42'>43</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000008?line=43'>44</a>\u001b[0m     action_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqnetwork_local(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000008?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnetwork_local\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000008?line=46'>47</a>\u001b[0m \u001b[39m''' Epsilon-greedy action selection (Already Present) '''\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Files\\RL Playground\\PA2\\Assignment2-CartPole-v1_1-Copy13 copy 3.ipynb Cell 5'\u001b[0m in \u001b[0;36mQNetwork1.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000004?line=63'>64</a>\u001b[0m \u001b[39m\"\"\"Build a network that maps state -> action values.\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000004?line=64'>65</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(state))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000004?line=65'>66</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mtanh(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Files/RL%20Playground/PA2/Assignment2-CartPole-v1_1-Copy13%20copy%203.ipynb#ch0000004?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1934\u001b[0m, in \u001b[0;36mtanh\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/functional.py?line=1925'>1926</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"tanh(input) -> Tensor\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/functional.py?line=1926'>1927</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/functional.py?line=1927'>1928</a>\u001b[0m \u001b[39mApplies element-wise,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/functional.py?line=1930'>1931</a>\u001b[0m \u001b[39mSee :class:`~torch.nn.Tanh` for more details.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/functional.py?line=1931'>1932</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/functional.py?line=1932'>1933</a>\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mnn.functional.tanh is deprecated. Use torch.tanh instead.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vedant/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/functional.py?line=1933'>1934</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mtanh()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' Defining DQN Algorithm '''\n",
    "for i in range(10) :\n",
    "    state_shape = env.observation_space.shape[0]\n",
    "    action_shape = env.action_space.n\n",
    "    reward_list = []\n",
    "    def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.975):\n",
    "\n",
    "        scores = []                 \n",
    "        ''' list containing scores from each episode '''\n",
    "\n",
    "        scores_window_printing = deque(maxlen=10) \n",
    "        ''' For printing in the graph '''\n",
    "\n",
    "        scores_window= deque(maxlen=100)  \n",
    "        ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "        eps = eps_start                    \n",
    "        ''' initialize epsilon '''\n",
    "\n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, eps)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break \n",
    "\n",
    "            reward_list.append(score)\n",
    "            scores_window.append(score)       \n",
    "            scores_window_printing.append(score)   \n",
    "            ''' save most recent score '''           \n",
    "\n",
    "            eps = max(eps_end, eps_decay*eps) \n",
    "            ''' decrease epsilon '''\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "            if i_episode % 10 == 0: \n",
    "                scores.append(np.mean(scores_window_printing))        \n",
    "            if i_episode % 100 == 0: \n",
    "               print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window)>=475.0:\n",
    "               solved_episodes.append(i_episode)\n",
    "               print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "               break\n",
    "        return [np.array(scores),i_episode-100]\n",
    "\n",
    "    ''' Trial run to check if algorithm runs and saves the data '''\n",
    "\n",
    "    begin_time = datetime.datetime.now()\n",
    "    agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "\n",
    "\n",
    "    dqn()\n",
    "\n",
    "\n",
    "    time_taken = datetime.datetime.now() - begin_time\n",
    "\n",
    "    print(time_taken)\n",
    "    rewards.append(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards1=rewards\n",
    "average_reward_list = []\n",
    "for i in range(10000) :\n",
    "        average_reward_list.append(0)\n",
    "for j in range(10) :\n",
    "    for i in range(len(rewards[j])) :\n",
    "        average_reward_list[i]+=rewards[j][i]\n",
    "    for k in range(i,10000) :\n",
    "        average_reward_list[k]+=rewards[j][i]\n",
    "for i in range(len(average_reward_list)) :\n",
    "    average_reward_list[i]/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20b07989250>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD4CAYAAAAaT9YAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARJklEQVR4nO3dfYxldX3H8fdHFvAB5KE7IIXdLhgwpbayOphValW0SKkpTWMaSMW1WjelxrCU1AAmGv9pLCVY6ZNuhGobSkXZqjE1lFJ8IClrZ7eLC7sgUJ8WVneoqZA2PlC+/eOe1Xk4w9wd5u7sb/b9Sm723N85997vb87uZ8+c87u/k6pCktSmZy11AZKkhTPEJalhhrgkNcwQl6SGGeKS1LAVB/LDVq5cWWvWrDmQHylJzdu6detjVTXWt+6AhviaNWuYmJg4kB8pSc1L8s251nk6RZIaZohLUsMMcUlqmCEuSQ0zxCWpYYa4JDXMEJekhjUR4nfs+i5/9YWHlroMSTroNBHiX3hgko9++etLXYYkHXSaCHFJUj9DXJIaNm+IJ1mV5M4kO5Pcl+Syrv2sJHcn2Z5kIsnLR1+uJGmqYSbAehK4oqq2JTka2JrkduAa4P1V9fkkF3TPXzO6UiVJM80b4lW1B9jTLT+RZBdwMlDA87vNjgEeHVWR3WeP8u0lqUn7NRVtkjXAWmALsBG4Lcm1DE7LvHKO12wANgCsXr16QUUmC3qZJC17Q1/YTHIUcCuwsaoeBy4FLq+qVcDlwA19r6uqTVU1XlXjY2O9c5pLkhZoqBBPcjiDAL+pqjZ3zeuBfcufBLywKUkH2DCjU8LgKHtXVV03ZdWjwKu75XOBBxe/PEnS0xnmnPg5wCXAjiTbu7argXcAH0qyAvgB3XlvSdKBM8zolLuAuS4tvmxxy5Ek7Y9mvrHpAENJmq2JEHeEoST1ayLEJUn9DHFJapghLkkNM8QlqWHNhLjzX0nSbE2EeJwBS5J6NRHikqR+hrgkNcwQl6SGGeKS1LBmQtzbs0nSbM2EuCRpNkNckhpmiEtSwwxxSWqYIS5JDRvmRsmrktyZZGeS+5JcNmXdu5Lc37VfM9pSJUkzDXOj5CeBK6pqW5Kjga1JbgdOBC4EXlJVP0xywigLdYChJM02zI2S9wB7uuUnkuwCTmZwt/sPVNUPu3V7R1Wk819JUr/9OieeZA2wFtgCnAG8KsmWJF9McvYI6pMkPY1hTqcAkOQo4FZgY1U9nmQFcDywDjgbuCXJaTXjq5VJNgAbAFavXr1ohUuShjwST3I4gwC/qao2d827gc018BXgKWDlzNdW1aaqGq+q8bGxscWqW5LEcKNTAtwA7Kqq66as+jTw2m6bM4AjgMdGUKMkaQ7DnE45B7gE2JFke9d2NXAjcGOSe4EfAetnnkpZVA5PkaRZhhmdchcw1/iQNy9uOf0y58dL0qHNb2xKUsMMcUlqmCEuSQ0zxCWpYYa4JDWsmRB3hKEkzdZEiDsBliT1ayLEJUn9DHFJapghLkkNM8QlqWHNhPgo59aSpFY1EeIOTpGkfk2EuCSpnyEuSQ0zxCWpYYa4JDWsmRB3bIokzdZMiEuSZhvmbverktyZZGeS+5JcNmP9FUkqycpRFekEWJLUb5i73T8JXFFV25IcDWxNcntV7UyyCjgP+NZIq5Qk9Zr3SLyq9lTVtm75CWAXcHK3+oPAu/GUtSQtif06J55kDbAW2JLkQuCRqrpnntdsSDKRZGJycnLhlUqSZhk6xJMcBdwKbGRwiuVq4L3zva6qNlXVeFWNj42NLbROSVKPoUI8yeEMAvymqtoMvBA4FbgnyTeAU4BtSV4wqkKd/0qSZpv3wmaSADcAu6rqOoCq2gGcMGWbbwDjVfXYKIqMw1MkqdcwR+LnAJcA5ybZ3j0uGHFdkqQhzHskXlV3Mc9ssFW1ZrEKkiQNz29sSlLDDHFJalgzIV5+n0iSZmkixB2bIkn9mghxSVI/Q1ySGmaIS1LDDHFJapghLkkNaybEnQBLkmZrI8QdYyhJvdoIcUlSL0NckhpmiEtSwwxxSWpYMyHu4BRJmq2JEI/DUySpVxMhLknqZ4hLUsPmDfEkq5LcmWRnkvuSXNa1/2mS+5N8Nck/Jjl25NVKkqYZ5kj8SeCKqjoTWAe8M8mZwO3Ai6vql4CvAVeNrkxJUp95Q7yq9lTVtm75CWAXcHJV/XNVPdltdjdwyujKxOEpktRjv86JJ1kDrAW2zFj1NuDzc7xmQ5KJJBOTk5MLKlKS1G/oEE9yFHArsLGqHp/S/h4Gp1xu6ntdVW2qqvGqGh8bG1tQkXGEoST1WjHMRkkOZxDgN1XV5intbwXeCLyuysliJelAmzfEkwS4AdhVVddNaT8feDfw6qr639GVKEmayzBH4ucAlwA7kmzv2q4GrgeOBG4f5Dx3V9Xvj6JISVK/eUO8qu6i/7YM/7T45UiS9kcz39gsxxhK0ixNhLiDUySpXxMhLknqZ4hLUsMMcUlqmCEuSQ1rJsT9PqgkzdZEiDt3iiT1ayLEJUn9DHFJapghLkkNM8QlqWHNhLiDUyRptmZCXJI0WxMhHqfAkqReTYS4JKmfIS5JDTPEJalhhrgkNWzeEE+yKsmdSXYmuS/JZV378UluT/Jg9+dxoyy0nAFLkmYZ5kj8SeCKqjoTWAe8M8mZwJXAHVV1OnBH93wknABLkvrNG+JVtaeqtnXLTwC7gJOBC4GPd5t9HPjNEdUoSZrDfp0TT7IGWAtsAU6sqj3dqu8AJ87xmg1JJpJMTE5OPpNaJUkzDB3iSY4CbgU2VtXjU9fV4IR170nrqtpUVeNVNT42NvaMipUkTTdUiCc5nEGA31RVm7vm7yY5qVt/ErB3NCVKkuYyzOiUADcAu6rquimrPgus75bXA59Z/PJ+yrEpkjTbiiG2OQe4BNiRZHvXdjXwAeCWJG8Hvgn89kgqlCTNad4Qr6q7YM4ZqF63uOX0c4ShJPXzG5uS1DBDXJIaZohLUsMMcUlqWDMh7vxXkjRbGyHuDFiS1KuNEJck9TLEJalhhrgkNcwQl6SGGeKS1LAmQtyxKZLUr4kQlyT1M8QlqWGGuCQ1zBCXpIY1FeLlBCqSNE1TIS5Jmq6JEHf+K0nqN8zd7m9MsjfJvVPazkpyd5LtSSaSvHy0ZUqS+gxzJP4x4PwZbdcA76+qs4D3ds8lSQfYvCFeVV8CvjezGXh+t3wM8Ogi1yVJGsKKBb5uI3BbkmsZ/Efwyrk2TLIB2ACwevXqBX6cJKnPQi9sXgpcXlWrgMuBG+basKo2VdV4VY2PjY0t8OP2vdczerkkLTsLDfH1wOZu+ZPASC9sximwJKnXQkP8UeDV3fK5wIOLU44kaX/Me048yc3Aa4CVSXYD7wPeAXwoyQrgB3TnvCVJB9a8IV5VF8+x6mWLXIskaT818Y1NSVK/pkLcwSmSNF1TIS5Jmq6JEHcCLEnq10SIS5L6GeKS1DBDXJIa1lSIe3s2SZquqRCXJE3XRIg7OEWS+jUR4pKkfoa4JDXMEJekhhniktSwpkLcAYaSNF0TIe7cKZLUr4kQlyT1M8QlqWGGuCQ1bN4QT3Jjkr1J7p3R/q4k9ye5L8k1oytRkjSXYY7EPwacP7UhyWuBC4GXVNUvANcufmmzOf+VJE03b4hX1ZeA781ovhT4QFX9sNtm7whqkyTNY6HnxM8AXpVkS5IvJjl7rg2TbEgykWRicnJyQR8WxxhKUq+FhvgK4HhgHfBHwC2ZI2mralNVjVfV+NjY2AI/TpLUZ6EhvhvYXANfAZ4CVi5eWZKkYSw0xD8NvBYgyRnAEcBji1STJGlIK+bbIMnNwGuAlUl2A+8DbgRu7IYd/ghYX947TZIOuHlDvKounmPVmxe5lnmVU2BJ0jR+Y1OSGmaIS1LDDHFJapghLkkNM8QlqWFNhbiDGCVpuqZCXJI0XRMh7vxXktSviRCXJPUzxCWpYYa4JDXMEJekhhniktSwJkI8ODxFkvo0EeKSpH6GuCQ1zBCXpIYZ4pLUsHlvz3Yw+fXrv8yz/A6+pAb98W/9ImevOX7R33eYGyXfCLwR2FtVL56x7grgWmCsqkZ2t/vX//wJ7NzzOP/31FOj+ghJGqnnHH7YSN53mCPxjwF/Afzt1MYkq4DzgG8tflnTnX7i0fz5xWtH/TGS1Jx5z4lX1ZeA7/Ws+iDwbvAW9JK0VBZ0YTPJhcAjVXXPENtuSDKRZGJycnIhHydJmsN+h3iS5wJXA+8dZvuq2lRV41U1PjY2tr8fJ0l6Ggs5En8hcCpwT5JvAKcA25K8YDELkyTNb7+HGFbVDuCEfc+7IB8f5egUSVK/eY/Ek9wM/BvwoiS7k7x99GVJkoYx75F4VV08z/o1i1aNJGm/+LV7SWpYqg7cMO8kk8A3F/jylcChdt7dPh8a7POh4Zn0+eeqqnd43wEN8WciyURVjS91HQeSfT402OdDw6j67OkUSWqYIS5JDWspxDctdQFLwD4fGuzzoWEkfW7mnLgkabaWjsQlSTMY4pLUsCZCPMn5SR5I8lCSK5e6noVKsirJnUl2JrkvyWVd+/FJbk/yYPfncV17klzf9furSV465b3Wd9s/mGT9UvVpWEkOS/IfST7XPT81yZaub59IckTXfmT3/KFu/Zop73FV1/5AkjcsUVeGkuTYJJ9Kcn+SXUlesdz3c5LLu7/X9ya5Ocmzl9t+TnJjkr1J7p3Stmj7NcnLkuzoXnN9MsT9KKvqoH4AhwEPA6cBRwD3AGcudV0L7MtJwEu75aOBrwFnAtcAV3btVwJ/0i1fAHweCLAO2NK1Hw/8Z/fncd3ycUvdv3n6/ofA3wOf657fAlzULX8YuLRb/gPgw93yRcAnuuUzu31/JINZNB8GDlvqfj1Nfz8O/F63fARw7HLez8DJwNeB50zZv29dbvsZ+BXgpcC9U9oWbb8CX+m2TffaX5u3pqX+oQzxQ3sFcNuU51cBVy11XYvUt88Avwo8AJzUtZ0EPNAtfwS4eMr2D3TrLwY+MqV92nYH24PBdMV3AOcCn+v+gj4GrJi5j4HbgFd0yyu67TJzv0/d7mB7AMd0gZYZ7ct2P3ch/u0umFZ0+/kNy3E/A2tmhPii7Ndu3f1T2qdtN9ejhdMp+/5y7LO7a2ta9+vjWmALcGJV7elWfQc4sVueq++t/Uz+jMGt/Pbd6fpngP+uqie751Pr/0nfuvXf77Zvqc+nApPA33SnkD6a5Hks4/1cVY8wuGn6t4A9DPbbVpb3ft5nsfbryd3yzPan1UKILztJjgJuBTZW1eNT19Xgv+BlM+4zyRuBvVW1dalrOYBWMPiV+6+rai3wPwx+zf6JZbifjwMuZPAf2M8CzwPOX9KilsBS7NcWQvwRYNWU56d0bU1KcjiDAL+pqjZ3zd9NclK3/iRgb9c+V99b+pmcA/xGBjcP+QcGp1Q+BBybZN9UyFPr/0nfuvXHAP9FW33eDeyuqi3d808xCPXlvJ9fD3y9qiar6sfAZgb7fjnv530Wa78+0i3PbH9aLYT4vwOnd1e5j2BwEeSzS1zTgnRXmm8AdlXVdVNWfRbYd4V6PYNz5fva39Jd5V4HfL/7te024Lwkx3VHQOd1bQedqrqqqk6pwbzzFwH/WlW/A9wJvKnbbGaf9/0s3tRtX137Rd2ohlOB0xlcBDroVNV3gG8neVHX9DpgJ8t4PzM4jbIuyXO7v+f7+rxs9/MUi7Jfu3WPJ1nX/QzfMuW95rbUFwmGvJBwAYORHA8D71nqep5BP36Zwa9aXwW2d48LGJwLvAN4EPgX4Phu+wB/2fV7B4Pb4O17r7cBD3WP313qvg3Z/9fw09EppzH4x/kQ8EngyK792d3zh7r1p015/Xu6n8UDDHHVfon7ehYw0e3rTzMYhbCs9zPwfuB+4F7g7xiMMFlW+xm4mcE5/x8z+I3r7Yu5X4Hx7uf3MPAXzLg43vfwa/eS1LAWTqdIkuZgiEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SG/T+znTBkAlbxIgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20b083a9a60>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_reward_list1 = []\n",
    "for i in range(100,len(average_reward_list)) :\n",
    "  average_reward_list1.append(np.average(average_reward_list[i-100:i]))\n",
    "plt.plot(average_reward_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1.0\n",
      "[[14.0], [14.0], [14.0], [12.0], [14.0], [18.0], [14.0], [13.0], [16.0], [13.0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(solved_episodes))\n",
    "print(solved_episodes)\n",
    "print(np.mean(solved_episodes))\n",
    "print(rewards)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5LBh6_lOVBdN"
   ],
   "name": "Tutorial_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
