{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P_DODRgW_ZKS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A bunch of imports, you don't have to worry about these\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYNA5kiH_esJ",
    "outputId": "d623e007-d532-4162-d93b-6023b34c80ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n",
      "1\n",
      "----\n",
      "[ 0.99962485  0.02738891  0.9989402  -0.04602639 -0.09180529 -0.09669447]\n",
      "----\n",
      "0\n",
      "----\n",
      "[ 0.9998245   0.01873245  0.995746   -0.09214022  0.00529764 -0.3585254 ]\n",
      "-1.0\n",
      "False\n",
      "{}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Please refer to the first tutorial for more details on the specifics of environments\n",
    "We've only added important commands you might find useful for experiments.\n",
    "'''\n",
    "\n",
    "'''\n",
    "List of example environments\n",
    "(Source - https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "'Acrobot-v1'\n",
    "'CartPole-v0'\n",
    "'MountainCar-v0'\n",
    "'''\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "print(env.action_space.sample())\n",
    "print(\"----\")\n",
    "\n",
    "'''\n",
    "# Understanding State, Action, Reward Dynamics\n",
    "\n",
    "The agent decides an action to take depending on the state.\n",
    "\n",
    "The Environment keeps a variable specifically for the current state.\n",
    "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
    "- It returns the new current state and reward for the agent to take the next action\n",
    "\n",
    "'''\n",
    "\n",
    "state = env.reset()   \n",
    "''' This returns the initial state (when environment is reset) '''\n",
    "\n",
    "print(state)\n",
    "print(\"----\")\n",
    "\n",
    "action = env.action_space.sample()  \n",
    "''' We take a random action now '''\n",
    "\n",
    "print(action)\n",
    "print(\"----\")\n",
    "\n",
    "next_state, reward, done, info = env.step(action) \n",
    "''' env.step is used to calculate new state and obtain reward based on old state and action taken  ''' \n",
    "\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g4MRC1p2DZbp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### Q Network & Some 'hyperparameters'\n",
    "\n",
    "QNetwork1:\n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "QNetwork2: Feel free to experiment more\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "'''\n",
    "Bunch of Hyper parameters (Which you might have to tune later **wink wink**)\n",
    "'''\n",
    "BUFFER_SIZE = int(5e4)  \n",
    "'''\n",
    "replay buffer size \n",
    "'''\n",
    "BATCH_SIZE = 256         \n",
    "''' \n",
    "minibatch size \n",
    "'''\n",
    "GAMMA = 0.99            \n",
    "''' \n",
    "discount factor \n",
    "'''\n",
    "LR = 6e-4              \n",
    "''' \n",
    "learning rate \n",
    "'''\n",
    "UPDATE_EVERY = 250       \n",
    "''' \n",
    "how often to update the network (When Q target is present) \n",
    "'''\n",
    "\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64,fc3_units=32):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork1, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.tanh(self.fc1(state))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmv5c0XoK8GA"
   },
   "source": [
    "### Replay Buffer:\n",
    "\n",
    "This is a 'deque' that helps us store experiences. Recall why we use such a technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bh_oghc7Ledh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8VJYkqoLqlO"
   },
   "source": [
    "## Truncation:\n",
    "We add a line (optionally) in the code to truncate the gradient in hopes that it would help with the stability of the learning process.\n",
    "\n",
    "## Tutorial Agent Code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ok_5eQM7OCTj"
   },
   "outputs": [],
   "source": [
    "class TutorialAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed,128,128).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed,128,128).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "        self.rg = np.random.RandomState(seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "#         probabs = softmax(action_values.cpu().data.numpy()[0])\n",
    "#         probabs /= probabs.sum()\n",
    "#         return self.rg.choice(np.arange(self.action_size), p = probabs)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ''' Get max predicted Q values (for next states) from target model'''\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ''' Compute Q targets for current states '''\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        ''' Get expected Q values from local model '''\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ''' Compute loss '''\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        ''' Minimize the loss '''\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        ''' Gradiant Clipping '''\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SQFbRCHWQyO"
   },
   "source": [
    "### Here, we present the DQN algorithm code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_episodes = []\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6A2TdUHWVUN",
    "outputId": "eae554fe-3450-40d8-d462-64b92d755866"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -262.53\n",
      "Episode 200\tAverage Score: -114.80\n",
      "Episode 300\tAverage Score: -90.489\n",
      "Episode 400\tAverage Score: -81.30\n",
      "Episode 441\tAverage Score: -79.91\n",
      "Environment solved in 341 episodes!\tAverage Score: -79.91\n",
      "0:10:18.035278\n",
      "Episode 100\tAverage Score: -261.67\n",
      "Episode 200\tAverage Score: -109.82\n",
      "Episode 300\tAverage Score: -91.277\n",
      "Episode 400\tAverage Score: -83.33\n",
      "Episode 456\tAverage Score: -79.53\n",
      "Environment solved in 356 episodes!\tAverage Score: -79.53\n",
      "0:10:43.131236\n",
      "Episode 100\tAverage Score: -285.21\n",
      "Episode 200\tAverage Score: -109.61\n",
      "Episode 300\tAverage Score: -88.290\n",
      "Episode 400\tAverage Score: -81.35\n",
      "Episode 411\tAverage Score: -79.86\n",
      "Environment solved in 311 episodes!\tAverage Score: -79.86\n",
      "0:10:27.132510\n",
      "Episode 100\tAverage Score: -249.90\n",
      "Episode 200\tAverage Score: -107.73\n",
      "Episode 300\tAverage Score: -86.663\n",
      "Episode 400\tAverage Score: -81.48\n",
      "Episode 434\tAverage Score: -79.84\n",
      "Environment solved in 334 episodes!\tAverage Score: -79.84\n",
      "0:09:58.547811\n",
      "Episode 100\tAverage Score: -261.04\n",
      "Episode 200\tAverage Score: -108.30\n",
      "Episode 300\tAverage Score: -94.256\n",
      "Episode 400\tAverage Score: -84.85\n",
      "Episode 500\tAverage Score: -81.82\n",
      "Episode 514\tAverage Score: -79.89\n",
      "Environment solved in 414 episodes!\tAverage Score: -79.89\n",
      "0:11:30.782422\n",
      "Episode 100\tAverage Score: -264.15\n",
      "Episode 200\tAverage Score: -111.14\n",
      "Episode 300\tAverage Score: -88.871\n",
      "Episode 400\tAverage Score: -83.71\n",
      "Episode 500\tAverage Score: -81.86\n",
      "Episode 532\tAverage Score: -79.96\n",
      "Environment solved in 432 episodes!\tAverage Score: -79.96\n",
      "0:11:12.199891\n",
      "Episode 100\tAverage Score: -257.88\n",
      "Episode 200\tAverage Score: -103.26\n",
      "Episode 300\tAverage Score: -87.101\n",
      "Episode 365\tAverage Score: -79.87\n",
      "Environment solved in 265 episodes!\tAverage Score: -79.87\n",
      "0:09:07.444363\n",
      "Episode 100\tAverage Score: -242.71\n",
      "Episode 200\tAverage Score: -105.22\n",
      "Episode 300\tAverage Score: -89.631\n",
      "Episode 383\tAverage Score: -80.00\n",
      "Environment solved in 283 episodes!\tAverage Score: -80.00\n",
      "0:09:30.027729\n",
      "Episode 36\tAverage Score: -440.94"
     ]
    }
   ],
   "source": [
    "''' Defining DQN Algorithm '''\n",
    "for i in range(10) :\n",
    "    state_shape = env.observation_space.shape[0]\n",
    "    action_shape = env.action_space.n\n",
    "    reward_list = []\n",
    "    def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.99):\n",
    "\n",
    "        scores = []                 \n",
    "        ''' list containing scores from each episode '''\n",
    "\n",
    "        scores_window_printing = deque(maxlen=10) \n",
    "        ''' For printing in the graph '''\n",
    "\n",
    "        scores_window= deque(maxlen=100)  \n",
    "        ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "        eps = eps_start                    \n",
    "        ''' initialize epsilon '''\n",
    "\n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, eps)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break \n",
    "\n",
    "            reward_list.append(score)\n",
    "            scores_window.append(score)       \n",
    "            scores_window_printing.append(score)   \n",
    "            ''' save most recent score '''           \n",
    "\n",
    "            eps = max(eps_end, eps_decay*eps) \n",
    "            ''' decrease epsilon '''\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "            if i_episode % 10 == 0: \n",
    "                scores.append(np.mean(scores_window_printing))        \n",
    "            if i_episode % 100 == 0: \n",
    "               print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window)>=-80.0:\n",
    "               solved_episodes.append(i_episode)\n",
    "               print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "               break\n",
    "        return [np.array(scores),i_episode-100]\n",
    "\n",
    "    ''' Trial run to check if algorithm runs and saves the data '''\n",
    "\n",
    "    begin_time = datetime.datetime.now()\n",
    "    agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "\n",
    "\n",
    "    dqn()\n",
    "\n",
    "\n",
    "    time_taken = datetime.datetime.now() - begin_time\n",
    "\n",
    "    print(time_taken)\n",
    "    rewards.append(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards1=rewards\n",
    "average_reward_list = []\n",
    "for i in range(10000) :\n",
    "        average_reward_list.append(0)\n",
    "for j in range(10) :\n",
    "    for i in range(len(rewards[j])) :\n",
    "        average_reward_list[i]+=rewards[j][i]\n",
    "    for k in range(i,10000) :\n",
    "        average_reward_list[k]+=rewards[j][i]\n",
    "for i in range(len(average_reward_list)) :\n",
    "    average_reward_list[i]/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward_list1 = []\n",
    "for i in range(100,len(average_reward_list)) :\n",
    "  average_reward_list1.append(np.average(average_reward_list[i-100:i]))\n",
    "plt.plot(average_reward_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(solved_episodes))\n",
    "print(solved_episodes)\n",
    "print(np.mean(solved_episodes))\n",
    "print(rewards)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5LBh6_lOVBdN"
   ],
   "name": "Tutorial_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
