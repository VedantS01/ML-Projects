{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:11.270433Z",
     "iopub.status.busy": "2022-01-26T05:23:11.269836Z",
     "iopub.status.idle": "2022-01-26T05:23:13.774671Z",
     "shell.execute_reply": "2022-01-26T05:23:13.774154Z"
    },
    "id": "tT4N3qYviUJr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FULL RETURNS\n",
    "'''\n",
    "\n",
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:13.781209Z",
     "iopub.status.busy": "2022-01-26T05:23:13.780629Z",
     "iopub.status.idle": "2022-01-26T05:23:13.782635Z",
     "shell.execute_reply": "2022-01-26T05:23:13.782236Z"
    },
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units1: int,\n",
    "      num_hidden_units2: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.common2 = layers.Dense(num_hidden_units2, activation=\"relu\")\n",
    "    self.common1 = layers.Dense(num_hidden_units1, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x = self.common1(inputs)\n",
    "    y = self.common2(x)\n",
    "    return self.actor(y), self.critic(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:13.787513Z",
     "iopub.status.busy": "2022-01-26T05:23:13.786039Z",
     "iopub.status.idle": "2022-01-26T05:23:15.327234Z",
     "shell.execute_reply": "2022-01-26T05:23:15.327656Z"
    },
    "id": "nWyxJgjLn68c"
   },
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units1 = 72\n",
    "num_hidden_units2 = 36\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units1,num_hidden_units2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:15.334407Z",
     "iopub.status.busy": "2022-01-26T05:23:15.333557Z",
     "iopub.status.idle": "2022-01-26T05:23:15.335339Z",
     "shell.execute_reply": "2022-01-26T05:23:15.335713Z"
    },
    "id": "5URrbGlDSAGx"
   },
   "outputs": [],
   "source": [
    "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  return (state.astype(np.float32), \n",
    "          np.array(reward, np.int32), \n",
    "          np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], \n",
    "                           [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:15.343778Z",
     "iopub.status.busy": "2022-01-26T05:23:15.343000Z",
     "iopub.status.idle": "2022-01-26T05:23:15.344931Z",
     "shell.execute_reply": "2022-01-26T05:23:15.345267Z"
    },
    "id": "a4qVRV063Cl9"
   },
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "  \n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state)\n",
    "  \n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "  \n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward, done = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "  \n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "    if tf.cast(done, tf.bool):\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "  \n",
    "  return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:15.351637Z",
     "iopub.status.busy": "2022-01-26T05:23:15.350879Z",
     "iopub.status.idle": "2022-01-26T05:23:15.353333Z",
     "shell.execute_reply": "2022-01-26T05:23:15.352864Z"
    },
    "id": "jpEwFyl315dl"
   },
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:15.358845Z",
     "iopub.status.busy": "2022-01-26T05:23:15.358077Z",
     "iopub.status.idle": "2022-01-26T05:23:15.360025Z",
     "shell.execute_reply": "2022-01-26T05:23:15.360372Z"
    },
    "id": "9EXwbEez6n9m"
   },
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:15.367094Z",
     "iopub.status.busy": "2022-01-26T05:23:15.366298Z",
     "iopub.status.idle": "2022-01-26T05:23:15.368110Z",
     "shell.execute_reply": "2022-01-26T05:23:15.368545Z"
    },
    "id": "QoccrkF3IFCg"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode) \n",
    "\n",
    "    # Calculate expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "    # Calculating loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:23:15.374476Z",
     "iopub.status.busy": "2022-01-26T05:23:15.373652Z",
     "iopub.status.idle": "2022-01-26T05:24:28.546242Z",
     "shell.execute_reply": "2022-01-26T05:24:28.546605Z"
    },
    "id": "kbmBxnzLiUJx"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    " \n",
    "# consecutive trials\n",
    "reward_threshold = -100\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "# rewards\n",
    "rewards = []\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    episode_reward = int(train_step(\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    \n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "  \n",
    "    t.set_description(f'Episode {i}')\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "  \n",
    "    # Show average episode reward every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "  \n",
    "    # if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "    #     break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.initializers import glorot_uniform  # Or your initializer of choice\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    " \n",
    "# consecutive trials\n",
    "reward_threshold = -100\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "def actor_critic():\n",
    "  # initial_weights = model.get_weights()\n",
    "\n",
    "  # backend_name = K.backend()\n",
    "  # if backend_name == 'tensorflow': \n",
    "  #     k_eval = lambda placeholder: placeholder.eval(session=K.get_session())\n",
    "  # elif backend_name == 'theano': \n",
    "  #     k_eval = lambda placeholder: placeholder.eval()\n",
    "  # else: \n",
    "  #     raise ValueError(\"Unsupported backend\")\n",
    "\n",
    "  # new_weights = [k_eval(glorot_uniform()(w.shape)) for w in initial_weights]\n",
    "\n",
    "  # model.set_weights(new_weights)\n",
    "      \n",
    "  # Keep last episodes reward\n",
    "  episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "  # rewards\n",
    "  rewards = []\n",
    "\n",
    "  with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "      initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "      episode_reward = int(train_step(\n",
    "          initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "      \n",
    "      rewards.append(episode_reward)\n",
    "      \n",
    "      episodes_reward.append(episode_reward)\n",
    "      running_reward = statistics.mean(episodes_reward)\n",
    "    \n",
    "      t.set_description(f'Episode {i}')\n",
    "      t.set_postfix(\n",
    "          episode_reward=episode_reward, running_reward=running_reward)\n",
    "    \n",
    "      # Show average episode reward every 10 episodes\n",
    "      if i % 10 == 0:\n",
    "        pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "    \n",
    "      # if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "      #     break\n",
    "  return rewards\n",
    "\n",
    "# accumator = []\n",
    "for _ in range(1):\n",
    "  l = actor_critic()\n",
    "  accumator.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in avg_rew_plots:\n",
    "    avg = []\n",
    "    for i in range(len(l)):\n",
    "        if(i <= 100):\n",
    "            avg.append(np.mean(l[:i]))\n",
    "        else:\n",
    "            avg.append(np.mean(l[i-100:i]))\n",
    "    plt.plot(avg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_DODRgW_ZKS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SINGLE STEP RETURNS\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXFHQcJjVKYu"
   },
   "outputs": [],
   "source": [
    "class ActorCriticModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Defining policy and value networkss\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, n_hidden1=24, n_hidden2=24):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "\n",
    "        #Hidden Layer 1\n",
    "        self.fc1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
    "        #Hidden Layer 2\n",
    "        self.fc2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
    "        \n",
    "        #Output Layer for policy\n",
    "        self.pi_out = tf.keras.layers.Dense(action_size, activation='softmax')\n",
    "        #Output Layer for state-value\n",
    "        self.v_out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        \"\"\"\n",
    "        Computes policy distribution and state-value for a given state\n",
    "        \"\"\"\n",
    "        layer1 = self.fc1(state)\n",
    "        layer2 = self.fc2(layer1)\n",
    "\n",
    "        pi = self.pi_out(layer2)\n",
    "        v = self.v_out(layer2)\n",
    "\n",
    "        return pi, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RNpp9WMfkTE"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, lr=3e-5, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.ac_model = ActorCriticModel(action_size=action_size)\n",
    "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, compute the policy distribution over all actions and sample one action\n",
    "        \"\"\"\n",
    "        pi,_ = self.ac_model(state)\n",
    "\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=pi)\n",
    "        sample = action_probabilities.sample()\n",
    "\n",
    "        return int(sample.numpy()[0])\n",
    "\n",
    "    def actor_loss(self, action, pi, delta):\n",
    "        \"\"\"\n",
    "        Compute Actor Loss\n",
    "        \"\"\"\n",
    "        return -tf.math.log(pi[0,action]) * delta\n",
    "\n",
    "    def critic_loss(self,delta):\n",
    "        \"\"\"\n",
    "        Critic loss aims to minimize TD error\n",
    "        \"\"\"\n",
    "        return delta**2\n",
    "\n",
    "    @tf.function\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        For a given transition (s,a,s',r) update the paramters by computing the\n",
    "        gradient of the total loss\n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            pi, V_s = self.ac_model(state)\n",
    "            _, V_s_next = self.ac_model(next_state)\n",
    "\n",
    "            V_s = tf.squeeze(V_s)\n",
    "            V_s_next = tf.squeeze(V_s_next)\n",
    "\n",
    "            #### TO DO: Write the equation for delta (TD error)\n",
    "            ## Write code below\n",
    "            delta = reward + self.gamma * V_s_next - V_s\n",
    "            \n",
    "            V_s_next = tf.stop_gradient(V_s_next)\n",
    "\n",
    "            loss_a = self.actor_loss(action, pi, delta)\n",
    "            loss_c =self.critic_loss(delta)\n",
    "            loss_total = loss_a + loss_c\n",
    "\n",
    "        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n",
    "        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUJwznIzwBIX"
   },
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0SB0o_OfyGN",
    "outputId": "9a717e3c-9545-46b2-fc89-5c3b633388f1"
   },
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "# print env info\n",
    "print(\"Action Space: \", env.action_space)\n",
    "print(\"Action Size: \", env.action_space.n)\n",
    "print(env.reset().reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initializing Agent\n",
    "agent = Agent(action_size=env.action_space.n)\n",
    "#Number of episodes\n",
    "episodes = 1000\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "reward_list = []\n",
    "average_reward_list = []\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "for ep in tqdm(range(1, episodes + 1)):\n",
    "    state = env.reset().reshape(1,-1)\n",
    "    done = False\n",
    "    ep_rew = 0\n",
    "    while not done:\n",
    "        action = agent.sample_action(state) ##Sample Action\n",
    "        next_state, reward, done, info = env.step(action) ##Take action\n",
    "        next_state = next_state.reshape(1,-1)\n",
    "        ep_rew += reward  ##Updating episode reward\n",
    "        agent.learn(state, action, reward, next_state, done) ##Update Parameters\n",
    "        state = next_state ##Updating State\n",
    "    reward_list.append(ep_rew)\n",
    "    if(len(reward_list) >= 20):\n",
    "      avg_20 =  np.mean(reward_list[-20:])\n",
    "      average_reward_list.append(avg_20)\n",
    "    else:\n",
    "      avg = np.mean(reward_list)\n",
    "      average_reward_list.append(avg)\n",
    "\n",
    "    if ep % 1 == 0:\n",
    "        avg_rew = np.mean(reward_list[-1:])\n",
    "        print('Episode ', ep, 'Reward %f' % ep_rew, 'Average Reward %f' % avg_rew)\n",
    "\n",
    "    if ep % 20:\n",
    "        avg_20 =  np.mean(reward_list[-20:])\n",
    "        if avg_20 > -100.0:\n",
    "            print('Stopped at Episode ',ep-20)\n",
    "            break\n",
    "\n",
    "time_taken = datetime.datetime.now() - begin_time\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "wOZzkLIgvHgS",
    "outputId": "9d7e346b-dbe5-4f00-da6b-95962afb0392"
   },
   "outputs": [],
   "source": [
    "### Plot of total reward vs episode\n",
    "## Write Code Below\n",
    "\n",
    "plt.style.use('seaborn-poster')\n",
    "# plt.figure(figsize = (10,8))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average reward, over last 100 episodes')\n",
    "plt.plot(range(len(average_reward_list)),average_reward_list, 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, episodes=10000, lr = 3e-5, gamma = 0.99):\n",
    "    # rs = np.random.RandomState()\n",
    "    #Initializing Agent\n",
    "    agent = Agent(lr=lr, gamma=gamma, action_size=env.action_space.n)\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    reward_list = []\n",
    "\n",
    "    for ep in tqdm(range(1, episodes + 1)):\n",
    "        state = env.reset().reshape(1,-1)\n",
    "        done = False\n",
    "        ep_rew = 0\n",
    "        while not done:\n",
    "            action = agent.sample_action(state) ##Sample Action\n",
    "            next_state, reward, done, info = env.step(action) ##Take action\n",
    "            next_state = next_state.reshape(1,-1)\n",
    "            ep_rew += reward  ##Updating episode reward\n",
    "            agent.learn(state, action, reward, next_state, done) ##Update Parameters\n",
    "            state = next_state ##Updating State\n",
    "        reward_list.append(ep_rew)\n",
    "        # if(len(reward_list) >= 20):\n",
    "        #     avg_20 =  np.mean(reward_list[-20:])\n",
    "        #     average_reward_list.append(avg_20)\n",
    "        # else:\n",
    "        #     avg = np.mean(reward_list)\n",
    "        #     average_reward_list.append(avg)\n",
    "\n",
    "        if ep % 5 == 0:\n",
    "            avg_rew = np.mean(reward_list[-5:])\n",
    "            # print('\\rEpisode ', ep, 'Reward %f' % ep_rew, 'Average Reward %f' % avg_rew, end='')\n",
    "\n",
    "        # if ep % 20:\n",
    "        #     avg_20 =  np.mean(reward_list[-20:])\n",
    "        #     if avg_20 > -100.0:\n",
    "        #         print('\\rStopped at Episode ',ep-20)\n",
    "        #         break\n",
    "    return avg_rew, reward_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "my_queue = queue.Queue()\n",
    "\n",
    "def storeInQueue(f):\n",
    "  def wrapper(*args):\n",
    "    my_queue.put(f(*args))\n",
    "  return wrapper\n",
    "\n",
    "\n",
    "@storeInQueue\n",
    "def actor_critic_mt(env,episodes,lr,gamma):\n",
    "   _, l = actor_critic(env,episodes,lr,gamma)\n",
    "   return l\n",
    "\n",
    "\n",
    "\n",
    "# t = threading.Thread(target=get_name, args = (\"foo\", ))\n",
    "# t.start()\n",
    "\n",
    "\n",
    "experiments = 10\n",
    "episodes = 1000\n",
    "gamma = 0.99\n",
    "lr = 3e-5\n",
    "avg_rew_plots = np.zeros((experiments,episodes))\n",
    "for e in range(experiments):\n",
    "    try:\n",
    "        env = gym.make('Acrobot-v1')\n",
    "        t = threading.Thread(target=actor_critic_mt, args = (env,episodes,lr,gamma))\n",
    "        t.start()\n",
    "    except:\n",
    "        print(\"threading failed for e = \", e)\n",
    "        break\n",
    "        # avg_rew, avg_rew_plots[e] = actor_critic(env, lr = lr,episodes=episodes)\n",
    "\n",
    "for e in range(experiments):\n",
    "    avg_rew_plots[e] = my_queue.get()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_avg_rew_plot = np.var(avg_rew_plots, axis=0)\n",
    "plt.plot(avg_avg_rew_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.mean(accumator[:10], axis=0)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_plot = []\n",
    "for i in range(len(rewards)):\n",
    "    if(i >= 40):\n",
    "        avg_plot.append(np.mean(rewards[i-40:i+1]))\n",
    "    else:\n",
    "        avg_plot.append(np.mean(rewards[0:i+1]))\n",
    "    # rewards[i] = avg_plot[i]\n",
    "plt.plot(avg_plot)\n",
    "plt.show()\n",
    "# var_plot = np.var(accumator[:10], axis=0)\n",
    "# avg_var_plot = []\n",
    "# for i in range(len(rewards)):\n",
    "#     if(i >= 100):\n",
    "#         avg_var_plot.append(np.mean(var_plot[i-100:i+1]))\n",
    "#     else:\n",
    "#         avg_var_plot.append(np.mean(var_plot[0:i+1]))\n",
    "# plt.plot(avg_var_plot)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "N STEP RETURNS\n",
    "----------------\n",
    "FOR N STEP RETURNS VARIATION, WE USE THE FOLLOWING CODE TO GET THE N STEP RETURN\n",
    "'''\n",
    "import torch\n",
    "def calc_nstep_returns(rewards, dones, next_v_pred, gamma, n):\n",
    "    rets = torch.zeros_like(rewards)\n",
    "    future_ret = next_v_pred\n",
    "    not_dones = 1 - dones\n",
    "    for t in reversed(range(n)):\n",
    "        rets[t] = future_ret = rewards[t] + gamma * future_ret * not_dones[t]\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DQN ALGORITHM\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "Please refer to the first tutorial for more details on the specifics of environments\n",
    "We've only added important commands you might find useful for experiments.\n",
    "'''\n",
    "\n",
    "'''\n",
    "List of example environments\n",
    "(Source - https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "'Acrobot-v1'\n",
    "'CartPole-v0'\n",
    "'MountainCar-v0'\n",
    "'''\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "print(env.action_space.sample())\n",
    "print(\"----\")\n",
    "\n",
    "'''\n",
    "# Understanding State, Action, Reward Dynamics\n",
    "\n",
    "The agent decides an action to take depending on the state.\n",
    "\n",
    "The Environment keeps a variable specifically for the current state.\n",
    "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
    "- It returns the new current state and reward for the agent to take the next action\n",
    "\n",
    "'''\n",
    "\n",
    "state = env.reset()   \n",
    "''' This returns the initial state (when environment is reset) '''\n",
    "\n",
    "print(state)\n",
    "print(\"----\")\n",
    "\n",
    "action = env.action_space.sample()  \n",
    "''' We take a random action now '''\n",
    "\n",
    "print(action)\n",
    "print(\"----\")\n",
    "\n",
    "next_state, reward, done, info = env.step(action) \n",
    "''' env.step is used to calculate new state and obtain reward based on old state and action taken  ''' \n",
    "\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(\"----\")\n",
    "\n",
    "'''\n",
    "### Q Network & Some 'hyperparameters'\n",
    "\n",
    "QNetwork1:\n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "QNetwork2: Feel free to experiment more\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "'''\n",
    "Bunch of Hyper parameters (Which you might have to tune later **wink wink**)\n",
    "'''\n",
    "BUFFER_SIZE = int(5e4)  \n",
    "'''\n",
    "replay buffer size \n",
    "'''\n",
    "BATCH_SIZE = 256         \n",
    "''' \n",
    "minibatch size \n",
    "'''\n",
    "GAMMA = 0.99            \n",
    "''' \n",
    "discount factor \n",
    "'''\n",
    "LR = 6e-4              \n",
    "''' \n",
    "learning rate \n",
    "'''\n",
    "UPDATE_EVERY = 250       \n",
    "''' \n",
    "how often to update the network (When Q target is present) \n",
    "'''\n",
    "\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64,fc3_units=32):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork1, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.tanh(self.fc1(state))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "### Replay Buffer:\n",
    "\n",
    "This is a 'deque' that helps us store experiences. Recall why we use such a technique.\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "## Truncation:\n",
    "We add a line (optionally) in the code to truncate the gradient in hopes that it would help with the stability of the learning process.\n",
    "\n",
    "## Tutorial Agent Code:\n",
    "\n",
    "class TutorialAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed,128,128).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed,128,128).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "        self.rg = np.random.RandomState(seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "#         probabs = softmax(action_values.cpu().data.numpy()[0])\n",
    "#         probabs /= probabs.sum()\n",
    "#         return self.rg.choice(np.arange(self.action_size), p = probabs)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ''' Get max predicted Q values (for next states) from target model'''\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ''' Compute Q targets for current states '''\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        ''' Get expected Q values from local model '''\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ''' Compute loss '''\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        ''' Minimize the loss '''\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        ''' Gradiant Clipping '''\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        self.optimizer.step()\n",
    "### Here, we present the DQN algorithm code.\n",
    "solved_episodes = []\n",
    "rewards = []\n",
    "''' Defining DQN Algorithm '''\n",
    "for i in range(10) :\n",
    "    state_shape = env.observation_space.shape[0]\n",
    "    action_shape = env.action_space.n\n",
    "    reward_list = []\n",
    "    def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.99):\n",
    "\n",
    "        scores = []                 \n",
    "        ''' list containing scores from each episode '''\n",
    "\n",
    "        scores_window_printing = deque(maxlen=10) \n",
    "        ''' For printing in the graph '''\n",
    "\n",
    "        scores_window= deque(maxlen=100)  \n",
    "        ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "        eps = eps_start                    \n",
    "        ''' initialize epsilon '''\n",
    "\n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            for t in range(max_t):\n",
    "                action = agent.act(state, eps)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break \n",
    "\n",
    "            reward_list.append(score)\n",
    "            scores_window.append(score)       \n",
    "            scores_window_printing.append(score)   \n",
    "            ''' save most recent score '''           \n",
    "\n",
    "            eps = max(eps_end, eps_decay*eps) \n",
    "            ''' decrease epsilon '''\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "            if i_episode % 10 == 0: \n",
    "                scores.append(np.mean(scores_window_printing))        \n",
    "            if i_episode % 100 == 0: \n",
    "               print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window)>=-80.0:\n",
    "               solved_episodes.append(i_episode)\n",
    "               print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "               break\n",
    "        return [np.array(scores),i_episode-100]\n",
    "\n",
    "    ''' Trial run to check if algorithm runs and saves the data '''\n",
    "\n",
    "    begin_time = datetime.datetime.now()\n",
    "    agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "\n",
    "\n",
    "    dqn()\n",
    "\n",
    "\n",
    "    time_taken = datetime.datetime.now() - begin_time\n",
    "\n",
    "    print(time_taken)\n",
    "    rewards.append(reward_list)\n",
    "rewards1=rewards\n",
    "average_reward_list = []\n",
    "for i in range(10000) :\n",
    "        average_reward_list.append(0)\n",
    "for j in range(10) :\n",
    "    for i in range(len(rewards[j])) :\n",
    "        average_reward_list[i]+=rewards[j][i]\n",
    "    for k in range(i,10000) :\n",
    "        average_reward_list[k]+=rewards[j][i]\n",
    "for i in range(len(average_reward_list)) :\n",
    "    average_reward_list[i]/=10\n",
    "plt.plot(average_reward_list)\n",
    "average_reward_list1 = []\n",
    "for i in range(100,len(average_reward_list)) :\n",
    "  average_reward_list1.append(np.average(average_reward_list[i-100:i]))\n",
    "plt.plot(average_reward_list1)\n",
    "print(np.mean(solved_episodes))\n",
    "print(solved_episodes)\n",
    "print(np.mean(solved_episodes))\n",
    "print(rewards)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
